{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f1a4e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "import os\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import InMemoryDataset, download_url\n",
    "\n",
    "from IPython.display import Javascript\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv,SAGEConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ef9fcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_patients = set(['F117', 'F130', 'F128', 'F133', 'F135', 'F138', 'F139', 'F136',\n",
    "                 'F131', 'F344', 'F504', 'F510', 'M512', 'F341', 'F337', 'F324',\n",
    "                 'F308', 'F317'])\n",
    "elecs = ['Fp1', 'Fp2', 'Fpz', 'F3', 'F4', 'Fz', 'C3', 'C4',\n",
    "         'Cz', 'P3', 'P4', 'Pz', '01', '02', '0z', 'F7', \n",
    "         'F8', 'T3', 'T4','T5', 'T6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab7229e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdmasny\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/notebooks/gnn_eeg/wandb/run-20221027_200512-tidpwbpz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/dmasny/neuroimaging_gnn_eeg_final_project/runs/tidpwbpz\" target=\"_blank\">gallant-firebrand-33</a></strong> to <a href=\"https://wandb.ai/dmasny/neuroimaging_gnn_eeg_final_project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/dmasny/neuroimaging_gnn_eeg_final_project/runs/tidpwbpz?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fcf46f71700>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project = \"neuroimaging_gnn_eeg_final_project\", entity = \"dmasny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6869485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNDataset(InMemoryDataset):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 root, \n",
    "                 data_dict, \n",
    "                 idx, \n",
    "                 feature_names, \n",
    "                 allow_loops, \n",
    "                 weighted, \n",
    "                 threshold = 0.65,\n",
    "                 transform = None, \n",
    "                 pre_transform = None, \n",
    "                 pre_filter = None):\n",
    "        \n",
    "        self.data = np.load(data_dict, allow_pickle = True).item()\n",
    "        self.stage = idx # idx 0 - train, idx 1 - test\n",
    "        self.feature_names = feature_names\n",
    "        self.allow_loops = allow_loops\n",
    "        self.weighted = weighted\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[self.stage])\n",
    "        \n",
    "    def vectorize_adj_mat_coo(self, matrix, allow_loops = True):\n",
    "        source_nodes = []\n",
    "        target_nodes = []\n",
    "        if allow_loops:\n",
    "            for i in range(matrix.shape[0]):\n",
    "                for j in range(i, matrix.shape[1]):\n",
    "                    source_nodes.append(i)\n",
    "                    target_nodes.append(j)\n",
    "        else:\n",
    "            for i in range(matrix.shape[0]):\n",
    "                for j in range(i + 1, matrix.shape[1]):\n",
    "                    source_nodes.append(i)\n",
    "                    target_nodes.append(j)\n",
    "        return source_nodes, target_nodes\n",
    "\n",
    "    def vectorize_adj_mat_weights(self, matrix):\n",
    "        edge_weights = []\n",
    "        if self.weighted == 'weighted':\n",
    "            if self.allow_loops:\n",
    "                for i in range(matrix.shape[0]):\n",
    "                    for j in range(i, matrix.shape[1]):\n",
    "                        edge_weights.append(matrix[i][j])\n",
    "            else:\n",
    "                for i in range(matrix.shape[0]):\n",
    "                    for j in range(i + 1, matrix.shape[1]):\n",
    "                        edge_weights.append(matrix[i][j])\n",
    "        else:\n",
    "            threshold = np.min(np.max(matrix, axis = 1)) if self.weighted == 'unweighted_dynamic_threshold' else self.threshold\n",
    "            mask = np.array((matrix > threshold), dtype = np.uint8)\n",
    "            if allow_loops:\n",
    "                for i in range(mask.shape[0]):\n",
    "                    for j in range(i, mask.shape[1]):\n",
    "                        edge_weights.append(mask[i][j])\n",
    "            else:\n",
    "                for i in range(mask.shape[0]):\n",
    "                    for j in range(i + 1, mask.shape[1]):\n",
    "                        edge_weights.append(mask[i][j])\n",
    "        return edge_weights\n",
    "\n",
    "    def upload_data(self, patch_name):\n",
    "        '''\n",
    "        input:\n",
    "            path_to_data: path to precomputed node representations\n",
    "            path_to_adj_matr: path to precomputed adj matrices\n",
    "            \n",
    "        returns:\n",
    "            Pygeometric Data object (see PyG docs)\n",
    "        '''\n",
    "        X, target, adj_matrix = self.data[patch_name] # triplet in format [X, target, A]\n",
    "        edge_index = np.array(self.vectorize_adj_mat_coo(adj_matrix))\n",
    "        edge_features = self.vectorize_adj_mat_weights(adj_matrix)\n",
    "        return Data(x = torch.tensor(X), \n",
    "                    edge_index = torch.tensor(edge_index),\n",
    "                    edge_attrs = edge_features, \n",
    "                    y = torch.tensor([target]))  \n",
    "    \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [f'gnn_dataset_train_{self.feature_names}.pt',\n",
    "                f'gnn_dataset_test_{self.feature_names}.pt']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "        \n",
    "    def process(self):\n",
    "        data_list = []\n",
    "        for elem in self.data.keys():\n",
    "            data_list.append(self.upload_data(elem))\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[self.stage])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a83e8b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels, output_dim = 2):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(2022)\n",
    "        self.conv1 = SAGEConv(num_node_features, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.linear = Linear(hidden_channels, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        \n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        \n",
    "        x = self.conv4(x ,edge_index)\n",
    "        x = x.relu()\n",
    "        \n",
    "\n",
    "        # Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        # Classifier\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "480efa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = GNNDataset('./', 'gnn_prepared_data/power_and_entropy_gnn_train.npy', \n",
    "#                            0, \n",
    "#                            'all_features',\n",
    "#                            allow_loops = True,\n",
    "#                            weighted = 'weighted') # idx = 0 - train\n",
    "# test_dataset = GNNDataset('./', 'gnn_prepared_data/power_and_entropy_gnn_test.npy', \n",
    "#                           1, \n",
    "#                           'all_features',\n",
    "#                           allow_loops = True,\n",
    "#                           weighted = 'weighted') # idx = 1 - train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a844cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c775fa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = GNNDataset('./', 'gnn_prepared_data/power_and_entropy_gnn_train.npy', 0, 'all_features',  weighted = False) # idx = 0 - train\n",
    "# test_dataset = GNNDataset('./', 'gnn_prepared_data/power_and_entropy_gnn_test.npy', 1, 'all_features', weighted = False) # idx = 1 - train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d0c845",
   "metadata": {},
   "source": [
    "1) adj(2) + loops(2) + weighted(2) + data(3) = 2*2*2*3 = 12 exp\n",
    "2) batch size, num_layers, num_epoches, lr, scheduler\n",
    "3)\n",
    "\n",
    "object : [X, target, A]\n",
    "X - only use local info about the node\n",
    "1) acc entropy <\n",
    "2) acc power <\n",
    "3) acc power + entropy < gnn(power + entropy + adj matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2778a16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, dataloader):\n",
    "#     model.train()\n",
    "\n",
    "#     for data in dataloader:  # Iterate in batches over the training dataset.\n",
    "#         out = model(data.x.type(dtype=torch.float), data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "#         loss = criterion(out, data.y)  # Compute the loss.\n",
    "#         loss.backward()  # Derive gradients.\n",
    "#         optimizer.step()  # Update parameters based on gradients.\n",
    "#         optimizer.zero_grad()  # Clear gradients.\n",
    "#         wandb.log()\n",
    "        \n",
    "# def test(model, dataloader):\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     for data in dataloader:  # Iterate in batches over the training/test dataset.\n",
    "#         out = model(data.x.type(dtype=torch.float), data.edge_index, data.batch)  \n",
    "#         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "#         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "#     return correct / len(loader.dataset)  # Derive ratio of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d11ae84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, \n",
    "          epoches, \n",
    "          criterion, \n",
    "          train_dataloader, \n",
    "          test_dataloader, \n",
    "          expirement_name, \n",
    "          path_to_save_weights = 'model_weights'):\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    for i in range(epoches):\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_loss = 0\n",
    "        for data in train_dataloader:\n",
    "            out = model(data.x.type(dtype = torch.float), \n",
    "                        data.edge_index, \n",
    "                        data.batch)  \n",
    "            loss = criterion(out, data.y) \n",
    "            train_loss += loss\n",
    "            pred = out.argmax(dim = 1)\n",
    "            train_correct += int((pred == data.y).sum())\n",
    "            loss.backward() \n",
    "            optimizer.step()  \n",
    "            optimizer.zero_grad() \n",
    "        train_loss = train_loss / len(train_dataloader.dataset)\n",
    "        train_accuracy = train_correct / len(train_dataloader.dataset)\n",
    "        \n",
    "        wandb.log({'train_loss': train_loss})\n",
    "        wandb.log({'train_accuracy': train_accuracy})\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():  \n",
    "            test_correct = 0\n",
    "            for data in test_dataloader:  \n",
    "                out = model(data.x.type(dtype = torch.float), data.edge_index, data.batch)  \n",
    "                pred = out.argmax(dim = 1)  \n",
    "                test_correct += int((pred == data.y).sum())\n",
    "            test_accuracy = test_correct / len(test_dataloader.dataset)\n",
    "            \n",
    "            if test_accuracy > best_accuracy:\n",
    "                best_accuracy = test_accuracy\n",
    "                torch.save(model.state_dict(), f'{path_to_save_weights}/{expirement_name}.pth')\n",
    "                \n",
    "            print(f'Epoch:{i} Train acc:{train_accuracy} Test acc:{test_accuracy} Train loss:{train_loss}')\n",
    "            \n",
    "            wandb.log({'test_accuracy': test_accuracy})\n",
    "            wandb.log({'best_test_accuracy': best_accuracy})\n",
    "            \n",
    "    \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9879f6ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:tidpwbpz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b462504cdf2d4c528b857cf7963f46d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.005 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.143308…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">gallant-firebrand-33</strong>: <a href=\"https://wandb.ai/dmasny/neuroimaging_gnn_eeg_final_project/runs/tidpwbpz\" target=\"_blank\">https://wandb.ai/dmasny/neuroimaging_gnn_eeg_final_project/runs/tidpwbpz</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221027_200512-tidpwbpz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:tidpwbpz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84eaae34a69244ebbcaebea06711d00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668508807197214, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/notebooks/gnn_eeg/wandb/run-20221027_200520-31x2xb1h</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/dmasny/neuroimaging_gnn_eeg_final_project/runs/31x2xb1h\" target=\"_blank\">self_loops=True_weighted=weighted_data=power_and_entropy</a></strong> to <a href=\"https://wandb.ai/dmasny/neuroimaging_gnn_eeg_final_project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Train acc:0.49415204678362573 Test acc:0.4988479262672811 Train loss:0.014838864095509052\n",
      "Epoch:1 Train acc:0.5112085769980507 Test acc:0.5011520737327189 Train loss:0.014280869625508785\n",
      "Epoch:2 Train acc:0.5082846003898636 Test acc:0.4988479262672811 Train loss:0.014163868501782417\n",
      "Epoch:3 Train acc:0.49171539961013644 Test acc:0.4988479262672811 Train loss:0.014291990548372269\n",
      "Epoch:4 Train acc:0.5160818713450293 Test acc:0.5011520737327189 Train loss:0.01417999155819416\n",
      "Epoch:5 Train acc:0.5384990253411306 Test acc:0.5011520737327189 Train loss:0.014096962288022041\n",
      "Epoch:6 Train acc:0.5014619883040936 Test acc:0.5011520737327189 Train loss:0.014203084632754326\n",
      "Epoch:7 Train acc:0.5307017543859649 Test acc:0.5011520737327189 Train loss:0.014203093945980072\n",
      "Epoch:8 Train acc:0.5389863547758285 Test acc:0.5023041474654378 Train loss:0.014072886668145657\n",
      "Epoch:9 Train acc:0.5477582846003899 Test acc:0.5702764976958525 Train loss:0.013991250656545162\n",
      "Epoch:10 Train acc:0.5477582846003899 Test acc:0.5806451612903226 Train loss:0.013967608101665974\n",
      "Epoch:11 Train acc:0.5584795321637427 Test acc:0.533410138248848 Train loss:0.013934594579041004\n",
      "Epoch:12 Train acc:0.5623781676413255 Test acc:0.5207373271889401 Train loss:0.013935107737779617\n",
      "Epoch:13 Train acc:0.5609161793372319 Test acc:0.5794930875576036 Train loss:0.013956851325929165\n",
      "Epoch:14 Train acc:0.5735867446393762 Test acc:0.521889400921659 Train loss:0.013858837075531483\n",
      "Epoch:15 Train acc:0.5462962962962963 Test acc:0.4827188940092166 Train loss:0.014168458990752697\n",
      "Epoch:16 Train acc:0.5092592592592593 Test acc:0.511520737327189 Train loss:0.01450622733682394\n",
      "Epoch:17 Train acc:0.5667641325536062 Test acc:0.511520737327189 Train loss:0.013809554278850555\n",
      "Epoch:18 Train acc:0.5570175438596491 Test acc:0.5149769585253456 Train loss:0.014066047966480255\n",
      "Epoch:19 Train acc:0.5716374269005848 Test acc:0.5633640552995391 Train loss:0.013790042139589787\n",
      "Epoch:20 Train acc:0.5799220272904484 Test acc:0.5691244239631337 Train loss:0.013630262576043606\n",
      "Epoch:21 Train acc:0.5735867446393762 Test acc:0.5610599078341014 Train loss:0.013618316501379013\n",
      "Epoch:22 Train acc:0.5935672514619883 Test acc:0.5391705069124424 Train loss:0.01339927688241005\n",
      "Epoch:23 Train acc:0.5960038986354775 Test acc:0.5702764976958525 Train loss:0.013700583018362522\n",
      "Epoch:24 Train acc:0.526803118908382 Test acc:0.5748847926267281 Train loss:0.014009484089910984\n",
      "Epoch:25 Train acc:0.5994152046783626 Test acc:0.5737327188940092 Train loss:0.013736508786678314\n",
      "Epoch:26 Train acc:0.5311890838206628 Test acc:0.4988479262672811 Train loss:0.013985405676066875\n",
      "Epoch:27 Train acc:0.5847953216374269 Test acc:0.5564516129032258 Train loss:0.013559376820921898\n",
      "Epoch:28 Train acc:0.5999025341130604 Test acc:0.5299539170506913 Train loss:0.013312580063939095\n",
      "Epoch:29 Train acc:0.5501949317738791 Test acc:0.5714285714285714 Train loss:0.013961254619061947\n",
      "Epoch:30 Train acc:0.5833333333333334 Test acc:0.5011520737327189 Train loss:0.013411936350166798\n",
      "Epoch:31 Train acc:0.5555555555555556 Test acc:0.5771889400921659 Train loss:0.01373861450701952\n",
      "Epoch:32 Train acc:0.5672514619883041 Test acc:0.533410138248848 Train loss:0.013560964725911617\n",
      "Epoch:33 Train acc:0.5706627680311891 Test acc:0.5552995391705069 Train loss:0.013449734076857567\n",
      "Epoch:34 Train acc:0.580896686159844 Test acc:0.5702764976958525 Train loss:0.013698258437216282\n",
      "Epoch:35 Train acc:0.5994152046783626 Test acc:0.5806451612903226 Train loss:0.013175169937312603\n",
      "Epoch:36 Train acc:0.5925925925925926 Test acc:0.576036866359447 Train loss:0.013376723974943161\n",
      "Epoch:37 Train acc:0.5838206627680312 Test acc:0.5241935483870968 Train loss:0.013694386929273605\n",
      "Epoch:38 Train acc:0.5745614035087719 Test acc:0.5276497695852534 Train loss:0.013621865771710873\n",
      "Epoch:39 Train acc:0.5969785575048733 Test acc:0.533410138248848 Train loss:0.013083180412650108\n",
      "Epoch:40 Train acc:0.5550682261208577 Test acc:0.5679723502304147 Train loss:0.013837887905538082\n",
      "Epoch:41 Train acc:0.5935672514619883 Test acc:0.565668202764977 Train loss:0.013366015627980232\n",
      "Epoch:42 Train acc:0.5896686159844055 Test acc:0.5633640552995391 Train loss:0.013314439915120602\n",
      "Epoch:43 Train acc:0.5994152046783626 Test acc:0.5794930875576036 Train loss:0.013134430162608624\n",
      "Epoch:44 Train acc:0.5341130604288499 Test acc:0.597926267281106 Train loss:0.014087291434407234\n",
      "Epoch:45 Train acc:0.5492202729044834 Test acc:0.5046082949308756 Train loss:0.013769542798399925\n",
      "Epoch:46 Train acc:0.5277777777777778 Test acc:0.586405529953917 Train loss:0.014107359573245049\n",
      "Epoch:47 Train acc:0.597953216374269 Test acc:0.5921658986175116 Train loss:0.013803829438984394\n",
      "Epoch:48 Train acc:0.6052631578947368 Test acc:0.5645161290322581 Train loss:0.013607800006866455\n",
      "Epoch:49 Train acc:0.5813840155945419 Test acc:0.5990783410138248 Train loss:0.01333179697394371\n",
      "Epoch:50 Train acc:0.5862573099415205 Test acc:0.5829493087557603 Train loss:0.013309641741216183\n",
      "Epoch:51 Train acc:0.6111111111111112 Test acc:0.5737327188940092 Train loss:0.012791370041668415\n",
      "Epoch:52 Train acc:0.6062378167641326 Test acc:0.5599078341013825 Train loss:0.013053925707936287\n",
      "Epoch:53 Train acc:0.594541910331384 Test acc:0.5806451612903226 Train loss:0.012988860718905926\n",
      "Epoch:54 Train acc:0.6081871345029239 Test acc:0.6048387096774194 Train loss:0.012816048227250576\n",
      "Epoch:55 Train acc:0.5974658869395711 Test acc:0.5633640552995391 Train loss:0.012900295667350292\n",
      "Epoch:56 Train acc:0.6120857699805068 Test acc:0.6059907834101382 Train loss:0.01280475128442049\n",
      "Epoch:57 Train acc:0.6028265107212476 Test acc:0.5518433179723502 Train loss:0.012814657762646675\n",
      "Epoch:58 Train acc:0.5935672514619883 Test acc:0.5737327188940092 Train loss:0.012832723557949066\n",
      "Epoch:59 Train acc:0.6145224171539961 Test acc:0.6255760368663594 Train loss:0.012793290428817272\n",
      "Epoch:60 Train acc:0.5984405458089669 Test acc:0.6175115207373272 Train loss:0.01288539357483387\n",
      "Epoch:61 Train acc:0.6072124756335283 Test acc:0.5748847926267281 Train loss:0.012674927711486816\n",
      "Epoch:62 Train acc:0.6008771929824561 Test acc:0.6129032258064516 Train loss:0.012729139998555183\n",
      "Epoch:63 Train acc:0.604775828460039 Test acc:0.6255760368663594 Train loss:0.012797792442142963\n",
      "Epoch:64 Train acc:0.604775828460039 Test acc:0.619815668202765 Train loss:0.012729928828775883\n",
      "Epoch:65 Train acc:0.5799220272904484 Test acc:0.5748847926267281 Train loss:0.013470432721078396\n",
      "Epoch:66 Train acc:0.5969785575048733 Test acc:0.6278801843317973 Train loss:0.01274630893021822\n",
      "Epoch:67 Train acc:0.6120857699805068 Test acc:0.5921658986175116 Train loss:0.01293821819126606\n",
      "Epoch:68 Train acc:0.5974658869395711 Test acc:0.6048387096774194 Train loss:0.012728777714073658\n",
      "Epoch:69 Train acc:0.5916179337231969 Test acc:0.6071428571428571 Train loss:0.0127581050619483\n",
      "Epoch:70 Train acc:0.618421052631579 Test acc:0.6082949308755761 Train loss:0.012665021233260632\n",
      "Epoch:71 Train acc:0.6052631578947368 Test acc:0.6175115207373272 Train loss:0.012620815075933933\n",
      "Epoch:72 Train acc:0.6130604288499025 Test acc:0.6082949308755761 Train loss:0.01264779269695282\n",
      "Epoch:73 Train acc:0.6003898635477583 Test acc:0.6025345622119815 Train loss:0.012814746238291264\n",
      "Epoch:74 Train acc:0.6164717348927875 Test acc:0.6002304147465438 Train loss:0.012459338642656803\n",
      "Epoch:75 Train acc:0.6052631578947368 Test acc:0.5841013824884793 Train loss:0.012321076355874538\n",
      "Epoch:76 Train acc:0.6125730994152047 Test acc:0.5875576036866359 Train loss:0.012861993163824081\n",
      "Epoch:77 Train acc:0.6038011695906432 Test acc:0.6105990783410138 Train loss:0.012689488008618355\n",
      "Epoch:78 Train acc:0.5716374269005848 Test acc:0.5529953917050692 Train loss:0.01347789540886879\n",
      "Epoch:79 Train acc:0.6106237816764133 Test acc:0.6071428571428571 Train loss:0.01260553952306509\n",
      "Epoch:80 Train acc:0.6154970760233918 Test acc:0.6278801843317973 Train loss:0.012520620599389076\n",
      "Epoch:81 Train acc:0.6003898635477583 Test acc:0.6071428571428571 Train loss:0.012780995108187199\n",
      "Epoch:82 Train acc:0.604775828460039 Test acc:0.5806451612903226 Train loss:0.012499567121267319\n",
      "Epoch:83 Train acc:0.601364522417154 Test acc:0.6278801843317973 Train loss:0.012791907414793968\n",
      "Epoch:84 Train acc:0.5838206627680312 Test acc:0.5599078341013825 Train loss:0.013122213073074818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:85 Train acc:0.6159844054580896 Test acc:0.6059907834101382 Train loss:0.01263479981571436\n",
      "Epoch:86 Train acc:0.6179337231968811 Test acc:0.6140552995391705 Train loss:0.012479064054787159\n",
      "Epoch:87 Train acc:0.6208576998050682 Test acc:0.6082949308755761 Train loss:0.01242883875966072\n",
      "Epoch:88 Train acc:0.6033138401559455 Test acc:0.6105990783410138 Train loss:0.012819865718483925\n",
      "Epoch:89 Train acc:0.6067251461988304 Test acc:0.6071428571428571 Train loss:0.012421331368386745\n",
      "Epoch:90 Train acc:0.6154970760233918 Test acc:0.6036866359447005 Train loss:0.012563303112983704\n",
      "Epoch:91 Train acc:0.6169590643274854 Test acc:0.6278801843317973 Train loss:0.012347953394055367\n",
      "Epoch:92 Train acc:0.6072124756335283 Test acc:0.5898617511520737 Train loss:0.01241233292967081\n",
      "Epoch:93 Train acc:0.6062378167641326 Test acc:0.6129032258064516 Train loss:0.013049634173512459\n",
      "Epoch:94 Train acc:0.618421052631579 Test acc:0.6117511520737328 Train loss:0.012483673170208931\n",
      "Epoch:95 Train acc:0.6247563352826511 Test acc:0.6048387096774194 Train loss:0.01243114285171032\n",
      "Epoch:96 Train acc:0.6076998050682261 Test acc:0.5887096774193549 Train loss:0.012406953610479832\n",
      "Epoch:97 Train acc:0.6208576998050682 Test acc:0.6267281105990783 Train loss:0.012495235539972782\n",
      "Epoch:98 Train acc:0.621832358674464 Test acc:0.6094470046082949 Train loss:0.012290311977267265\n",
      "Epoch:99 Train acc:0.6189083820662769 Test acc:0.5783410138248848 Train loss:0.012076363898813725\n",
      "Epoch:100 Train acc:0.6038011695906432 Test acc:0.6129032258064516 Train loss:0.012449989095330238\n",
      "Epoch:101 Train acc:0.6208576998050682 Test acc:0.5633640552995391 Train loss:0.012476056814193726\n",
      "Epoch:102 Train acc:0.5891812865497076 Test acc:0.5702764976958525 Train loss:0.01300762128084898\n",
      "Epoch:103 Train acc:0.6159844054580896 Test acc:0.6267281105990783 Train loss:0.012452667579054832\n",
      "Epoch:104 Train acc:0.6189083820662769 Test acc:0.6140552995391705 Train loss:0.012263127602636814\n",
      "Epoch:105 Train acc:0.618421052631579 Test acc:0.5944700460829493 Train loss:0.012546096928417683\n",
      "Epoch:106 Train acc:0.6081871345029239 Test acc:0.5933179723502304 Train loss:0.012923890724778175\n",
      "Epoch:107 Train acc:0.6242690058479532 Test acc:0.6048387096774194 Train loss:0.012342728674411774\n",
      "Epoch:108 Train acc:0.6174463937621832 Test acc:0.6347926267281107 Train loss:0.012750105001032352\n",
      "Epoch:109 Train acc:0.5618908382066277 Test acc:0.5737327188940092 Train loss:0.013517978601157665\n",
      "Epoch:110 Train acc:0.5950292397660819 Test acc:0.5599078341013825 Train loss:0.012576078064739704\n",
      "Epoch:111 Train acc:0.6062378167641326 Test acc:0.5414746543778802 Train loss:0.012430532835423946\n",
      "Epoch:112 Train acc:0.601364522417154 Test acc:0.6105990783410138 Train loss:0.012427058070898056\n",
      "Epoch:113 Train acc:0.6213450292397661 Test acc:0.533410138248848 Train loss:0.012738889083266258\n",
      "Epoch:114 Train acc:0.5935672514619883 Test acc:0.619815668202765 Train loss:0.012940768152475357\n",
      "Epoch:115 Train acc:0.6145224171539961 Test acc:0.6290322580645161 Train loss:0.01256069727241993\n",
      "Epoch:116 Train acc:0.6018518518518519 Test acc:0.5852534562211982 Train loss:0.013395760208368301\n",
      "Epoch:117 Train acc:0.5974658869395711 Test acc:0.6290322580645161 Train loss:0.01254878006875515\n",
      "Epoch:118 Train acc:0.6003898635477583 Test acc:0.6336405529953917 Train loss:0.012521618977189064\n",
      "Epoch:119 Train acc:0.6252436647173489 Test acc:0.6278801843317973 Train loss:0.01257441844791174\n",
      "Epoch:120 Train acc:0.6213450292397661 Test acc:0.6209677419354839 Train loss:0.0124452980235219\n",
      "Epoch:121 Train acc:0.6179337231968811 Test acc:0.6221198156682027 Train loss:0.012233037501573563\n",
      "Epoch:122 Train acc:0.6203703703703703 Test acc:0.630184331797235 Train loss:0.012547613121569157\n",
      "Epoch:123 Train acc:0.6203703703703703 Test acc:0.6324884792626728 Train loss:0.012290549464523792\n",
      "Epoch:124 Train acc:0.618421052631579 Test acc:0.6140552995391705 Train loss:0.012295190244913101\n",
      "Epoch:125 Train acc:0.6242690058479532 Test acc:0.6290322580645161 Train loss:0.012308325618505478\n",
      "Epoch:126 Train acc:0.6101364522417154 Test acc:0.5691244239631337 Train loss:0.012672711163759232\n",
      "Epoch:127 Train acc:0.6096491228070176 Test acc:0.6336405529953917 Train loss:0.01257364172488451\n",
      "Epoch:128 Train acc:0.6164717348927875 Test acc:0.6278801843317973 Train loss:0.012274676002562046\n",
      "Epoch:129 Train acc:0.6271929824561403 Test acc:0.6278801843317973 Train loss:0.012251711450517178\n",
      "Epoch:130 Train acc:0.6291423001949318 Test acc:0.5817972350230415 Train loss:0.012286856770515442\n",
      "Epoch:131 Train acc:0.6145224171539961 Test acc:0.6221198156682027 Train loss:0.012534674257040024\n",
      "Epoch:132 Train acc:0.6203703703703703 Test acc:0.6290322580645161 Train loss:0.012297313660383224\n",
      "Epoch:133 Train acc:0.6179337231968811 Test acc:0.630184331797235 Train loss:0.012344646267592907\n",
      "Epoch:134 Train acc:0.6203703703703703 Test acc:0.6267281105990783 Train loss:0.012392688542604446\n",
      "Epoch:135 Train acc:0.6111111111111112 Test acc:0.5956221198156681 Train loss:0.012187520042061806\n",
      "Epoch:136 Train acc:0.5974658869395711 Test acc:0.6382488479262672 Train loss:0.012595069594681263\n",
      "Epoch:137 Train acc:0.6125730994152047 Test acc:0.5817972350230415 Train loss:0.012517598457634449\n",
      "Epoch:138 Train acc:0.6159844054580896 Test acc:0.6428571428571429 Train loss:0.012340276502072811\n",
      "Epoch:139 Train acc:0.631578947368421 Test acc:0.6232718894009217 Train loss:0.012348530814051628\n",
      "Epoch:140 Train acc:0.6091617933723197 Test acc:0.5852534562211982 Train loss:0.01256195455789566\n",
      "Epoch:141 Train acc:0.6271929824561403 Test acc:0.5887096774193549 Train loss:0.012243459932506084\n",
      "Epoch:142 Train acc:0.6208576998050682 Test acc:0.6175115207373272 Train loss:0.012330712750554085\n",
      "Epoch:143 Train acc:0.6276803118908382 Test acc:0.6382488479262672 Train loss:0.012358148582279682\n",
      "Epoch:144 Train acc:0.6306042884990254 Test acc:0.6255760368663594 Train loss:0.012205799110233784\n",
      "Epoch:145 Train acc:0.6228070175438597 Test acc:0.6405529953917051 Train loss:0.012406007386744022\n",
      "Epoch:146 Train acc:0.615009746588694 Test acc:0.6359447004608295 Train loss:0.01262911781668663\n",
      "Epoch:147 Train acc:0.6228070175438597 Test acc:0.6359447004608295 Train loss:0.012449335306882858\n",
      "Epoch:148 Train acc:0.6140350877192983 Test acc:0.5898617511520737 Train loss:0.012532787397503853\n",
      "Epoch:149 Train acc:0.6174463937621832 Test acc:0.6382488479262672 Train loss:0.012455441989004612\n",
      "Epoch:150 Train acc:0.6164717348927875 Test acc:0.6278801843317973 Train loss:0.012000001035630703\n",
      "Epoch:151 Train acc:0.6193957115009746 Test acc:0.6094470046082949 Train loss:0.012357627041637897\n",
      "Epoch:152 Train acc:0.6145224171539961 Test acc:0.6267281105990783 Train loss:0.01249934546649456\n",
      "Epoch:153 Train acc:0.6189083820662769 Test acc:0.6324884792626728 Train loss:0.01247027050703764\n",
      "Epoch:154 Train acc:0.6174463937621832 Test acc:0.6336405529953917 Train loss:0.01237276941537857\n",
      "Epoch:155 Train acc:0.6198830409356725 Test acc:0.6382488479262672 Train loss:0.012298619374632835\n",
      "Epoch:156 Train acc:0.6115984405458089 Test acc:0.6417050691244239 Train loss:0.012329545803368092\n",
      "Epoch:157 Train acc:0.6291423001949318 Test acc:0.631336405529954 Train loss:0.012249419465661049\n",
      "Epoch:158 Train acc:0.6345029239766082 Test acc:0.6094470046082949 Train loss:0.012227124534547329\n",
      "Epoch:159 Train acc:0.6111111111111112 Test acc:0.6417050691244239 Train loss:0.012537435628473759\n",
      "Epoch:160 Train acc:0.6033138401559455 Test acc:0.5023041474654378 Train loss:0.012811311520636082\n",
      "Epoch:161 Train acc:0.5545808966861598 Test acc:0.6071428571428571 Train loss:0.013672648929059505\n",
      "Epoch:162 Train acc:0.621832358674464 Test acc:0.4988479262672811 Train loss:0.013118714094161987\n",
      "Epoch:163 Train acc:0.5536062378167641 Test acc:0.6059907834101382 Train loss:0.013518271967768669\n",
      "Epoch:164 Train acc:0.6237816764132553 Test acc:0.619815668202765 Train loss:0.012325048446655273\n",
      "Epoch:165 Train acc:0.6179337231968811 Test acc:0.6370967741935484 Train loss:0.012201515957713127\n",
      "Epoch:166 Train acc:0.621832358674464 Test acc:0.6082949308755761 Train loss:0.012298380956053734\n",
      "Epoch:167 Train acc:0.6154970760233918 Test acc:0.6232718894009217 Train loss:0.012434923090040684\n",
      "Epoch:168 Train acc:0.6193957115009746 Test acc:0.6382488479262672 Train loss:0.012475431896746159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:169 Train acc:0.6033138401559455 Test acc:0.6267281105990783 Train loss:0.01231867540627718\n",
      "Epoch:170 Train acc:0.6169590643274854 Test acc:0.5944700460829493 Train loss:0.012439655140042305\n",
      "Epoch:171 Train acc:0.6276803118908382 Test acc:0.5748847926267281 Train loss:0.012395798228681087\n",
      "Epoch:172 Train acc:0.6228070175438597 Test acc:0.631336405529954 Train loss:0.01267055980861187\n",
      "Epoch:173 Train acc:0.6242690058479532 Test acc:0.6140552995391705 Train loss:0.012327016331255436\n",
      "Epoch:174 Train acc:0.6154970760233918 Test acc:0.6370967741935484 Train loss:0.012280918657779694\n",
      "Epoch:175 Train acc:0.6164717348927875 Test acc:0.6382488479262672 Train loss:0.012414082884788513\n",
      "Epoch:176 Train acc:0.6213450292397661 Test acc:0.6152073732718893 Train loss:0.012228062376379967\n",
      "Epoch:177 Train acc:0.6081871345029239 Test acc:0.6405529953917051 Train loss:0.012546336278319359\n",
      "Epoch:178 Train acc:0.6237816764132553 Test acc:0.6175115207373272 Train loss:0.012655978091061115\n",
      "Epoch:179 Train acc:0.6320662768031189 Test acc:0.6324884792626728 Train loss:0.012354075908660889\n",
      "Epoch:180 Train acc:0.6203703703703703 Test acc:0.6382488479262672 Train loss:0.012204182334244251\n",
      "Epoch:181 Train acc:0.6111111111111112 Test acc:0.6278801843317973 Train loss:0.012290788814425468\n",
      "Epoch:182 Train acc:0.6203703703703703 Test acc:0.6324884792626728 Train loss:0.01228452567011118\n",
      "Epoch:183 Train acc:0.6189083820662769 Test acc:0.618663594470046 Train loss:0.012263891287147999\n",
      "Epoch:184 Train acc:0.6169590643274854 Test acc:0.6370967741935484 Train loss:0.012459171004593372\n",
      "Epoch:185 Train acc:0.6198830409356725 Test acc:0.6324884792626728 Train loss:0.012255584821105003\n",
      "Epoch:186 Train acc:0.6120857699805068 Test acc:0.6336405529953917 Train loss:0.01259535737335682\n",
      "Epoch:187 Train acc:0.615009746588694 Test acc:0.6002304147465438 Train loss:0.012896970845758915\n",
      "Epoch:188 Train acc:0.6086744639376218 Test acc:0.6394009216589862 Train loss:0.012376820668578148\n",
      "Epoch:189 Train acc:0.6106237816764133 Test acc:0.6382488479262672 Train loss:0.012440663762390614\n",
      "Epoch:190 Train acc:0.6169590643274854 Test acc:0.6370967741935484 Train loss:0.012635827995836735\n",
      "Epoch:191 Train acc:0.6213450292397661 Test acc:0.630184331797235 Train loss:0.012444551102817059\n",
      "Epoch:192 Train acc:0.6072124756335283 Test acc:0.6405529953917051 Train loss:0.01281235832720995\n",
      "Epoch:193 Train acc:0.6198830409356725 Test acc:0.6163594470046083 Train loss:0.01257539913058281\n",
      "Epoch:194 Train acc:0.6057504873294347 Test acc:0.6382488479262672 Train loss:0.012594851665198803\n",
      "Epoch:195 Train acc:0.6306042884990254 Test acc:0.6359447004608295 Train loss:0.012591497041285038\n",
      "Epoch:196 Train acc:0.6193957115009746 Test acc:0.6324884792626728 Train loss:0.012542998418211937\n",
      "Epoch:197 Train acc:0.6179337231968811 Test acc:0.6267281105990783 Train loss:0.012370788492262363\n",
      "Epoch:198 Train acc:0.621832358674464 Test acc:0.5967741935483871 Train loss:0.012510614469647408\n",
      "Epoch:199 Train acc:0.6198830409356725 Test acc:0.6324884792626728 Train loss:0.012412733398377895\n",
      "Epoch:200 Train acc:0.6135477582846004 Test acc:0.6036866359447005 Train loss:0.012567998841404915\n",
      "Epoch:201 Train acc:0.6379142300194932 Test acc:0.6244239631336406 Train loss:0.012299159541726112\n",
      "Epoch:202 Train acc:0.6193957115009746 Test acc:0.6370967741935484 Train loss:0.012260304763913155\n",
      "Epoch:203 Train acc:0.6291423001949318 Test acc:0.6255760368663594 Train loss:0.012212585657835007\n",
      "Epoch:204 Train acc:0.6154970760233918 Test acc:0.631336405529954 Train loss:0.012547202408313751\n",
      "Epoch:205 Train acc:0.6306042884990254 Test acc:0.6324884792626728 Train loss:0.01253633014857769\n",
      "Epoch:206 Train acc:0.6179337231968811 Test acc:0.6486175115207373 Train loss:0.0124855637550354\n",
      "Epoch:207 Train acc:0.6038011695906432 Test acc:0.6543778801843319 Train loss:0.012419329956173897\n",
      "Epoch:208 Train acc:0.6208576998050682 Test acc:0.6417050691244239 Train loss:0.012691555544734001\n",
      "Epoch:209 Train acc:0.6228070175438597 Test acc:0.6394009216589862 Train loss:0.012546858750283718\n",
      "Epoch:210 Train acc:0.6271929824561403 Test acc:0.6175115207373272 Train loss:0.012119869701564312\n",
      "Epoch:211 Train acc:0.6257309941520468 Test acc:0.6474654377880185 Train loss:0.012387681752443314\n",
      "Epoch:212 Train acc:0.6262183235867447 Test acc:0.6324884792626728 Train loss:0.012157948687672615\n",
      "Epoch:213 Train acc:0.6067251461988304 Test acc:0.619815668202765 Train loss:0.012862441129982471\n",
      "Epoch:214 Train acc:0.6179337231968811 Test acc:0.6382488479262672 Train loss:0.012307311408221722\n",
      "Epoch:215 Train acc:0.601364522417154 Test acc:0.6129032258064516 Train loss:0.012554535642266273\n",
      "Epoch:216 Train acc:0.6267056530214425 Test acc:0.6025345622119815 Train loss:0.0123353386297822\n",
      "Epoch:217 Train acc:0.6203703703703703 Test acc:0.6370967741935484 Train loss:0.012460807338356972\n",
      "Epoch:218 Train acc:0.6286549707602339 Test acc:0.6370967741935484 Train loss:0.012258915230631828\n",
      "Epoch:219 Train acc:0.6310916179337231 Test acc:0.6382488479262672 Train loss:0.012396594509482384\n",
      "Epoch:220 Train acc:0.6174463937621832 Test acc:0.6382488479262672 Train loss:0.012511953711509705\n",
      "Epoch:221 Train acc:0.6179337231968811 Test acc:0.6394009216589862 Train loss:0.01211921963840723\n",
      "Epoch:222 Train acc:0.6174463937621832 Test acc:0.6232718894009217 Train loss:0.012281573377549648\n",
      "Epoch:223 Train acc:0.6228070175438597 Test acc:0.6209677419354839 Train loss:0.01239794958382845\n",
      "Epoch:224 Train acc:0.6296296296296297 Test acc:0.6347926267281107 Train loss:0.012363514862954617\n",
      "Epoch:225 Train acc:0.628167641325536 Test acc:0.6175115207373272 Train loss:0.012459141202270985\n",
      "Epoch:226 Train acc:0.5248538011695907 Test acc:0.5357142857142857 Train loss:0.013753997161984444\n",
      "Epoch:227 Train acc:0.5618908382066277 Test acc:0.6209677419354839 Train loss:0.013380811549723148\n",
      "Epoch:228 Train acc:0.6213450292397661 Test acc:0.6670506912442397 Train loss:0.012285654433071613\n",
      "Epoch:229 Train acc:0.6091617933723197 Test acc:0.6255760368663594 Train loss:0.012600250542163849\n",
      "Epoch:230 Train acc:0.6296296296296297 Test acc:0.6267281105990783 Train loss:0.012438355013728142\n",
      "Epoch:231 Train acc:0.6267056530214425 Test acc:0.5921658986175116 Train loss:0.0121995247900486\n",
      "Epoch:232 Train acc:0.6286549707602339 Test acc:0.6555299539170507 Train loss:0.012422298081219196\n",
      "Epoch:233 Train acc:0.6223196881091618 Test acc:0.6336405529953917 Train loss:0.012246148660779\n",
      "Epoch:234 Train acc:0.6276803118908382 Test acc:0.6255760368663594 Train loss:0.012050395831465721\n",
      "Epoch:235 Train acc:0.6310916179337231 Test acc:0.6232718894009217 Train loss:0.01232702936977148\n",
      "Epoch:236 Train acc:0.6232943469785575 Test acc:0.6601382488479263 Train loss:0.012405173853039742\n",
      "Epoch:237 Train acc:0.6096491228070176 Test acc:0.5910138248847926 Train loss:0.012420718558132648\n",
      "Epoch:238 Train acc:0.6310916179337231 Test acc:0.6278801843317973 Train loss:0.012161762453615665\n",
      "Epoch:239 Train acc:0.6189083820662769 Test acc:0.6405529953917051 Train loss:0.012344462797045708\n",
      "Epoch:240 Train acc:0.618421052631579 Test acc:0.618663594470046 Train loss:0.012450532987713814\n",
      "Epoch:241 Train acc:0.631578947368421 Test acc:0.6509216589861752 Train loss:0.012225750833749771\n",
      "Epoch:242 Train acc:0.6276803118908382 Test acc:0.6589861751152074 Train loss:0.012025581672787666\n",
      "Epoch:243 Train acc:0.6345029239766082 Test acc:0.6405529953917051 Train loss:0.012123324908316135\n",
      "Epoch:244 Train acc:0.6320662768031189 Test acc:0.6601382488479263 Train loss:0.012221328914165497\n",
      "Epoch:245 Train acc:0.6330409356725146 Test acc:0.6370967741935484 Train loss:0.012249208055436611\n",
      "Epoch:246 Train acc:0.6310916179337231 Test acc:0.6117511520737328 Train loss:0.011980398558080196\n",
      "Epoch:247 Train acc:0.6461988304093568 Test acc:0.6140552995391705 Train loss:0.012117262929677963\n",
      "Epoch:248 Train acc:0.6427875243664717 Test acc:0.6417050691244239 Train loss:0.012374954298138618\n",
      "Epoch:249 Train acc:0.5847953216374269 Test acc:0.6417050691244239 Train loss:0.013301973231136799\n",
      "Epoch:250 Train acc:0.6271929824561403 Test acc:0.6232718894009217 Train loss:0.01239440031349659\n",
      "Epoch:251 Train acc:0.6364522417153996 Test acc:0.6152073732718893 Train loss:0.012009119614958763\n",
      "Epoch:252 Train acc:0.6384015594541911 Test acc:0.6336405529953917 Train loss:0.01213771291077137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:253 Train acc:0.6369395711500975 Test acc:0.6082949308755761 Train loss:0.011919207870960236\n",
      "Epoch:254 Train acc:0.6423001949317739 Test acc:0.6036866359447005 Train loss:0.01202597189694643\n",
      "Epoch:255 Train acc:0.6403508771929824 Test acc:0.6486175115207373 Train loss:0.011973198503255844\n",
      "Epoch:256 Train acc:0.648635477582846 Test acc:0.6405529953917051 Train loss:0.011811149306595325\n",
      "Epoch:257 Train acc:0.6447368421052632 Test acc:0.5944700460829493 Train loss:0.011656787246465683\n",
      "Epoch:258 Train acc:0.6471734892787524 Test acc:0.6267281105990783 Train loss:0.011858430691063404\n",
      "Epoch:259 Train acc:0.5326510721247564 Test acc:0.586405529953917 Train loss:0.014502390287816525\n",
      "Epoch:260 Train acc:0.5779727095516569 Test acc:0.543778801843318 Train loss:0.01378484908491373\n",
      "Epoch:261 Train acc:0.6232943469785575 Test acc:0.6428571428571429 Train loss:0.012760789133608341\n",
      "Epoch:262 Train acc:0.6301169590643275 Test acc:0.5518433179723502 Train loss:0.012575024738907814\n",
      "Epoch:263 Train acc:0.6125730994152047 Test acc:0.6002304147465438 Train loss:0.012820751406252384\n",
      "Epoch:264 Train acc:0.6335282651072125 Test acc:0.630184331797235 Train loss:0.012283858843147755\n",
      "Epoch:265 Train acc:0.5891812865497076 Test acc:0.5587557603686636 Train loss:0.013203474693000317\n",
      "Epoch:266 Train acc:0.6359649122807017 Test acc:0.619815668202765 Train loss:0.012408108450472355\n",
      "Epoch:267 Train acc:0.6203703703703703 Test acc:0.6267281105990783 Train loss:0.012391824275255203\n",
      "Epoch:268 Train acc:0.6223196881091618 Test acc:0.5875576036866359 Train loss:0.01257561519742012\n",
      "Epoch:269 Train acc:0.6291423001949318 Test acc:0.6105990783410138 Train loss:0.012378917075693607\n",
      "Epoch:270 Train acc:0.6262183235867447 Test acc:0.6347926267281107 Train loss:0.01240544207394123\n",
      "Epoch:271 Train acc:0.6325536062378168 Test acc:0.6267281105990783 Train loss:0.012344298884272575\n",
      "Epoch:272 Train acc:0.6252436647173489 Test acc:0.6382488479262672 Train loss:0.012265977449715137\n",
      "Epoch:273 Train acc:0.6164717348927875 Test acc:0.631336405529954 Train loss:0.012785675935447216\n",
      "Epoch:274 Train acc:0.6247563352826511 Test acc:0.6370967741935484 Train loss:0.012298413552343845\n",
      "Epoch:275 Train acc:0.6213450292397661 Test acc:0.6278801843317973 Train loss:0.012564349919557571\n",
      "Epoch:276 Train acc:0.6335282651072125 Test acc:0.6417050691244239 Train loss:0.012500511482357979\n",
      "Epoch:277 Train acc:0.6418128654970761 Test acc:0.6278801843317973 Train loss:0.01214482169598341\n",
      "Epoch:278 Train acc:0.6384015594541911 Test acc:0.6129032258064516 Train loss:0.011854060925543308\n",
      "Epoch:279 Train acc:0.6432748538011696 Test acc:0.6140552995391705 Train loss:0.01194256916642189\n",
      "Epoch:280 Train acc:0.6369395711500975 Test acc:0.6370967741935484 Train loss:0.012359857559204102\n",
      "Epoch:281 Train acc:0.5579922027290448 Test acc:0.6255760368663594 Train loss:0.014002188108861446\n",
      "Epoch:282 Train acc:0.621832358674464 Test acc:0.586405529953917 Train loss:0.013000729493796825\n",
      "Epoch:283 Train acc:0.631578947368421 Test acc:0.586405529953917 Train loss:0.012433513067662716\n",
      "Epoch:284 Train acc:0.6330409356725146 Test acc:0.5875576036866359 Train loss:0.012220051139593124\n",
      "Epoch:285 Train acc:0.6500974658869396 Test acc:0.619815668202765 Train loss:0.01203303411602974\n",
      "Epoch:286 Train acc:0.6369395711500975 Test acc:0.5829493087557603 Train loss:0.011769282631576061\n",
      "Epoch:287 Train acc:0.6393762183235867 Test acc:0.5910138248847926 Train loss:0.011996380984783173\n",
      "Epoch:288 Train acc:0.6393762183235867 Test acc:0.6244239631336406 Train loss:0.011577424593269825\n",
      "Epoch:289 Train acc:0.6457115009746589 Test acc:0.5852534562211982 Train loss:0.012028258293867111\n",
      "Epoch:290 Train acc:0.6115984405458089 Test acc:0.5944700460829493 Train loss:0.012641237117350101\n",
      "Epoch:291 Train acc:0.6086744639376218 Test acc:0.5564516129032258 Train loss:0.012814614921808243\n",
      "Epoch:292 Train acc:0.6198830409356725 Test acc:0.5829493087557603 Train loss:0.012588986195623875\n",
      "Epoch:293 Train acc:0.6018518518518519 Test acc:0.5829493087557603 Train loss:0.012556655332446098\n",
      "Epoch:294 Train acc:0.6091617933723197 Test acc:0.5990783410138248 Train loss:0.012403787113726139\n",
      "Epoch:295 Train acc:0.6179337231968811 Test acc:0.6002304147465438 Train loss:0.012343674898147583\n",
      "Epoch:296 Train acc:0.6135477582846004 Test acc:0.6117511520737328 Train loss:0.012815285474061966\n",
      "Epoch:297 Train acc:0.5341130604288499 Test acc:0.5910138248847926 Train loss:0.014218244701623917\n",
      "Epoch:298 Train acc:0.6091617933723197 Test acc:0.5599078341013825 Train loss:0.013698744587600231\n",
      "Epoch:299 Train acc:0.6067251461988304 Test acc:0.5599078341013825 Train loss:0.013459916226565838\n",
      "Epoch:300 Train acc:0.6111111111111112 Test acc:0.5610599078341014 Train loss:0.013158036395907402\n",
      "Epoch:301 Train acc:0.6140350877192983 Test acc:0.5910138248847926 Train loss:0.01310629490762949\n",
      "Epoch:302 Train acc:0.5989278752436648 Test acc:0.5829493087557603 Train loss:0.01297182310372591\n",
      "Epoch:303 Train acc:0.6067251461988304 Test acc:0.5921658986175116 Train loss:0.012837303802371025\n",
      "Epoch:304 Train acc:0.6081871345029239 Test acc:0.5748847926267281 Train loss:0.012997745536267757\n",
      "Epoch:305 Train acc:0.6125730994152047 Test acc:0.5771889400921659 Train loss:0.0125199556350708\n",
      "Epoch:306 Train acc:0.6179337231968811 Test acc:0.586405529953917 Train loss:0.012757939286530018\n",
      "Epoch:307 Train acc:0.6042884990253411 Test acc:0.5725806451612904 Train loss:0.012900173664093018\n",
      "Epoch:308 Train acc:0.6115984405458089 Test acc:0.5841013824884793 Train loss:0.01261143572628498\n",
      "Epoch:309 Train acc:0.6072124756335283 Test acc:0.5794930875576036 Train loss:0.012510323897004128\n",
      "Epoch:310 Train acc:0.6038011695906432 Test acc:0.5852534562211982 Train loss:0.012871365994215012\n",
      "Epoch:311 Train acc:0.6125730994152047 Test acc:0.6002304147465438 Train loss:0.012466629035770893\n",
      "Epoch:312 Train acc:0.6086744639376218 Test acc:0.5771889400921659 Train loss:0.012437209486961365\n",
      "Epoch:313 Train acc:0.6076998050682261 Test acc:0.6013824884792627 Train loss:0.012689825147390366\n",
      "Epoch:314 Train acc:0.6169590643274854 Test acc:0.5921658986175116 Train loss:0.012334812432527542\n",
      "Epoch:315 Train acc:0.6203703703703703 Test acc:0.5990783410138248 Train loss:0.012367316521704197\n",
      "Epoch:316 Train acc:0.6213450292397661 Test acc:0.5898617511520737 Train loss:0.012273404747247696\n",
      "Epoch:317 Train acc:0.5911306042884991 Test acc:0.5875576036866359 Train loss:0.01279610488563776\n",
      "Epoch:318 Train acc:0.6159844054580896 Test acc:0.5956221198156681 Train loss:0.01232274528592825\n",
      "Epoch:319 Train acc:0.6189083820662769 Test acc:0.619815668202765 Train loss:0.012360502034425735\n",
      "Epoch:320 Train acc:0.6179337231968811 Test acc:0.5944700460829493 Train loss:0.012229994870722294\n",
      "Epoch:321 Train acc:0.6086744639376218 Test acc:0.5933179723502304 Train loss:0.01267358660697937\n",
      "Epoch:322 Train acc:0.6203703703703703 Test acc:0.5967741935483871 Train loss:0.012349633499979973\n",
      "Epoch:323 Train acc:0.6106237816764133 Test acc:0.6002304147465438 Train loss:0.012411094270646572\n",
      "Epoch:324 Train acc:0.6057504873294347 Test acc:0.5933179723502304 Train loss:0.012779326178133488\n",
      "Epoch:325 Train acc:0.6223196881091618 Test acc:0.5990783410138248 Train loss:0.012390370480716228\n",
      "Epoch:326 Train acc:0.6203703703703703 Test acc:0.6244239631336406 Train loss:0.012464229948818684\n",
      "Epoch:327 Train acc:0.6203703703703703 Test acc:0.6244239631336406 Train loss:0.012240499258041382\n",
      "Epoch:328 Train acc:0.6101364522417154 Test acc:0.5944700460829493 Train loss:0.012521617114543915\n",
      "Epoch:329 Train acc:0.6267056530214425 Test acc:0.5956221198156681 Train loss:0.012462016195058823\n",
      "Epoch:330 Train acc:0.6271929824561403 Test acc:0.6025345622119815 Train loss:0.012290691025555134\n",
      "Epoch:331 Train acc:0.6140350877192983 Test acc:0.5990783410138248 Train loss:0.012702405452728271\n",
      "Epoch:332 Train acc:0.6179337231968811 Test acc:0.597926267281106 Train loss:0.012388555333018303\n",
      "Epoch:333 Train acc:0.6237816764132553 Test acc:0.630184331797235 Train loss:0.012159685604274273\n",
      "Epoch:334 Train acc:0.6033138401559455 Test acc:0.6163594470046083 Train loss:0.012639446184039116\n",
      "Epoch:335 Train acc:0.6354775828460039 Test acc:0.6290322580645161 Train loss:0.012257574126124382\n",
      "Epoch:336 Train acc:0.6242690058479532 Test acc:0.6163594470046083 Train loss:0.012336540035903454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:337 Train acc:0.6276803118908382 Test acc:0.6290322580645161 Train loss:0.012141630053520203\n",
      "Epoch:338 Train acc:0.6203703703703703 Test acc:0.6278801843317973 Train loss:0.01244312059134245\n",
      "Epoch:339 Train acc:0.6081871345029239 Test acc:0.6267281105990783 Train loss:0.012447596527636051\n",
      "Epoch:340 Train acc:0.6062378167641326 Test acc:0.6336405529953917 Train loss:0.012477311305701733\n",
      "Epoch:341 Train acc:0.6203703703703703 Test acc:0.6025345622119815 Train loss:0.01223254669457674\n",
      "Epoch:342 Train acc:0.615009746588694 Test acc:0.6071428571428571 Train loss:0.012281017377972603\n",
      "Epoch:343 Train acc:0.6276803118908382 Test acc:0.6002304147465438 Train loss:0.012333325110375881\n",
      "Epoch:344 Train acc:0.6198830409356725 Test acc:0.6209677419354839 Train loss:0.012385060079395771\n",
      "Epoch:345 Train acc:0.6140350877192983 Test acc:0.6267281105990783 Train loss:0.012315180152654648\n",
      "Epoch:346 Train acc:0.6169590643274854 Test acc:0.6278801843317973 Train loss:0.012237979099154472\n",
      "Epoch:347 Train acc:0.6223196881091618 Test acc:0.619815668202765 Train loss:0.012328402139246464\n",
      "Epoch:348 Train acc:0.6228070175438597 Test acc:0.6244239631336406 Train loss:0.012318586930632591\n",
      "Epoch:349 Train acc:0.6169590643274854 Test acc:0.6013824884792627 Train loss:0.012254391796886921\n",
      "Epoch:350 Train acc:0.6213450292397661 Test acc:0.6221198156682027 Train loss:0.012370764277875423\n",
      "Epoch:351 Train acc:0.6252436647173489 Test acc:0.6255760368663594 Train loss:0.012290413491427898\n",
      "Epoch:352 Train acc:0.6291423001949318 Test acc:0.6175115207373272 Train loss:0.012251356616616249\n",
      "Epoch:353 Train acc:0.6223196881091618 Test acc:0.618663594470046 Train loss:0.012303361669182777\n",
      "Epoch:354 Train acc:0.615009746588694 Test acc:0.6324884792626728 Train loss:0.01225703489035368\n",
      "Epoch:355 Train acc:0.6213450292397661 Test acc:0.6290322580645161 Train loss:0.01233406737446785\n",
      "Epoch:356 Train acc:0.634990253411306 Test acc:0.6244239631336406 Train loss:0.012350008822977543\n",
      "Epoch:357 Train acc:0.6427875243664717 Test acc:0.6347926267281107 Train loss:0.012054631486535072\n",
      "Epoch:358 Train acc:0.6140350877192983 Test acc:0.618663594470046 Train loss:0.012559037655591965\n",
      "Epoch:359 Train acc:0.6413255360623782 Test acc:0.5967741935483871 Train loss:0.011734189465641975\n",
      "Epoch:360 Train acc:0.6384015594541911 Test acc:0.631336405529954 Train loss:0.011920500546693802\n",
      "Epoch:361 Train acc:0.6369395711500975 Test acc:0.6036866359447005 Train loss:0.012219322845339775\n",
      "Epoch:362 Train acc:0.6228070175438597 Test acc:0.6105990783410138 Train loss:0.012077715247869492\n",
      "Epoch:363 Train acc:0.6413255360623782 Test acc:0.6082949308755761 Train loss:0.01214001514017582\n",
      "Epoch:364 Train acc:0.6296296296296297 Test acc:0.5944700460829493 Train loss:0.012051843106746674\n",
      "Epoch:365 Train acc:0.6437621832358674 Test acc:0.5794930875576036 Train loss:0.011656975373625755\n",
      "Epoch:366 Train acc:0.6393762183235867 Test acc:0.6025345622119815 Train loss:0.011591843329370022\n",
      "Epoch:367 Train acc:0.6403508771929824 Test acc:0.6048387096774194 Train loss:0.011716756038367748\n",
      "Epoch:368 Train acc:0.6345029239766082 Test acc:0.6244239631336406 Train loss:0.01199460867792368\n",
      "Epoch:369 Train acc:0.6408382066276803 Test acc:0.6013824884792627 Train loss:0.011791427619755268\n",
      "Epoch:370 Train acc:0.6530214424951267 Test acc:0.6209677419354839 Train loss:0.011584063991904259\n",
      "Epoch:371 Train acc:0.6544834307992202 Test acc:0.5806451612903226 Train loss:0.01161891594529152\n",
      "Epoch:372 Train acc:0.6588693957115009 Test acc:0.6082949308755761 Train loss:0.011594564653933048\n",
      "Epoch:373 Train acc:0.6603313840155945 Test acc:0.6071428571428571 Train loss:0.011670481413602829\n",
      "Epoch:374 Train acc:0.6491228070175439 Test acc:0.5887096774193549 Train loss:0.012007806450128555\n",
      "Epoch:375 Train acc:0.6559454191033138 Test acc:0.5967741935483871 Train loss:0.011625182814896107\n",
      "Epoch:376 Train acc:0.6535087719298246 Test acc:0.5967741935483871 Train loss:0.01164484303444624\n",
      "Epoch:377 Train acc:0.6159844054580896 Test acc:0.597926267281106 Train loss:0.012864727526903152\n",
      "Epoch:378 Train acc:0.6457115009746589 Test acc:0.5829493087557603 Train loss:0.0120371263474226\n",
      "Epoch:379 Train acc:0.6466861598440545 Test acc:0.5817972350230415 Train loss:0.01172578614205122\n",
      "Epoch:380 Train acc:0.6491228070175439 Test acc:0.5852534562211982 Train loss:0.011509749107062817\n",
      "Epoch:381 Train acc:0.655458089668616 Test acc:0.5875576036866359 Train loss:0.011567028239369392\n",
      "Epoch:382 Train acc:0.6549707602339181 Test acc:0.6175115207373272 Train loss:0.011285435408353806\n",
      "Epoch:383 Train acc:0.6500974658869396 Test acc:0.5783410138248848 Train loss:0.011586897075176239\n",
      "Epoch:384 Train acc:0.665692007797271 Test acc:0.5875576036866359 Train loss:0.011453835293650627\n",
      "Epoch:385 Train acc:0.652046783625731 Test acc:0.6244239631336406 Train loss:0.011669071391224861\n",
      "Epoch:386 Train acc:0.6549707602339181 Test acc:0.5956221198156681 Train loss:0.011551180854439735\n",
      "Epoch:387 Train acc:0.6471734892787524 Test acc:0.576036866359447 Train loss:0.011860428377985954\n",
      "Epoch:388 Train acc:0.6461988304093568 Test acc:0.5967741935483871 Train loss:0.011985015124082565\n",
      "Epoch:389 Train acc:0.6130604288499025 Test acc:0.5910138248847926 Train loss:0.012461269274353981\n",
      "Epoch:390 Train acc:0.6466861598440545 Test acc:0.5633640552995391 Train loss:0.01164479274302721\n",
      "Epoch:391 Train acc:0.6432748538011696 Test acc:0.5748847926267281 Train loss:0.011551824398338795\n",
      "Epoch:392 Train acc:0.6539961013645225 Test acc:0.5357142857142857 Train loss:0.011609893292188644\n",
      "Epoch:393 Train acc:0.6398635477582846 Test acc:0.6002304147465438 Train loss:0.011576935648918152\n",
      "Epoch:394 Train acc:0.6510721247563352 Test acc:0.5841013824884793 Train loss:0.011249041184782982\n",
      "Epoch:395 Train acc:0.6608187134502924 Test acc:0.5967741935483871 Train loss:0.011549416929483414\n",
      "Epoch:396 Train acc:0.6715399610136452 Test acc:0.6059907834101382 Train loss:0.011468714103102684\n",
      "Epoch:397 Train acc:0.6544834307992202 Test acc:0.6267281105990783 Train loss:0.01150933001190424\n",
      "Epoch:398 Train acc:0.6510721247563352 Test acc:0.6140552995391705 Train loss:0.01162745337933302\n",
      "Epoch:399 Train acc:0.6598440545808967 Test acc:0.5841013824884793 Train loss:0.011484112590551376\n",
      "Epoch:400 Train acc:0.6613060428849903 Test acc:0.6129032258064516 Train loss:0.01173508632928133\n",
      "Epoch:401 Train acc:0.6681286549707602 Test acc:0.597926267281106 Train loss:0.011036770418286324\n",
      "Epoch:402 Train acc:0.6695906432748538 Test acc:0.5771889400921659 Train loss:0.011375992558896542\n",
      "Epoch:403 Train acc:0.672514619883041 Test acc:0.6278801843317973 Train loss:0.011390967294573784\n",
      "Epoch:404 Train acc:0.6730019493177388 Test acc:0.6082949308755761 Train loss:0.011595471762120724\n",
      "Epoch:405 Train acc:0.601364522417154 Test acc:0.5875576036866359 Train loss:0.012916294857859612\n",
      "Epoch:406 Train acc:0.6432748538011696 Test acc:0.5898617511520737 Train loss:0.01221754215657711\n",
      "Epoch:407 Train acc:0.6276803118908382 Test acc:0.6036866359447005 Train loss:0.012417689897119999\n",
      "Epoch:408 Train acc:0.6574074074074074 Test acc:0.5587557603686636 Train loss:0.011440858244895935\n",
      "Epoch:409 Train acc:0.648635477582846 Test acc:0.5748847926267281 Train loss:0.01155366562306881\n",
      "Epoch:410 Train acc:0.6681286549707602 Test acc:0.5841013824884793 Train loss:0.011610825546085835\n",
      "Epoch:411 Train acc:0.6705653021442495 Test acc:0.5806451612903226 Train loss:0.01121967937797308\n",
      "Epoch:412 Train acc:0.6613060428849903 Test acc:0.5933179723502304 Train loss:0.01142123807221651\n",
      "Epoch:413 Train acc:0.6671539961013645 Test acc:0.6209677419354839 Train loss:0.011196519248187542\n",
      "Epoch:414 Train acc:0.6666666666666666 Test acc:0.5610599078341014 Train loss:0.011503474786877632\n",
      "Epoch:415 Train acc:0.6686159844054581 Test acc:0.5990783410138248 Train loss:0.011362024582922459\n",
      "Epoch:416 Train acc:0.665692007797271 Test acc:0.5841013824884793 Train loss:0.011656069196760654\n",
      "Epoch:417 Train acc:0.6588693957115009 Test acc:0.5910138248847926 Train loss:0.011761143803596497\n",
      "Epoch:418 Train acc:0.6652046783625731 Test acc:0.586405529953917 Train loss:0.011431559920310974\n",
      "Epoch:419 Train acc:0.669103313840156 Test acc:0.5783410138248848 Train loss:0.011296060867607594\n",
      "Epoch:420 Train acc:0.6793372319688109 Test acc:0.6013824884792627 Train loss:0.01092639658600092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:421 Train acc:0.6861598440545809 Test acc:0.586405529953917 Train loss:0.011162707582116127\n",
      "Epoch:422 Train acc:0.6720272904483431 Test acc:0.5679723502304147 Train loss:0.011347039602696896\n",
      "Epoch:423 Train acc:0.5964912280701754 Test acc:0.5783410138248848 Train loss:0.013111625798046589\n",
      "Epoch:424 Train acc:0.6442495126705653 Test acc:0.586405529953917 Train loss:0.01190549973398447\n",
      "Epoch:425 Train acc:0.6544834307992202 Test acc:0.5806451612903226 Train loss:0.011546480469405651\n",
      "Epoch:426 Train acc:0.669103313840156 Test acc:0.5910138248847926 Train loss:0.011389590799808502\n",
      "Epoch:427 Train acc:0.6734892787524367 Test acc:0.6117511520737328 Train loss:0.01112227514386177\n",
      "Epoch:428 Train acc:0.6569200779727096 Test acc:0.6013824884792627 Train loss:0.011604533530771732\n",
      "Epoch:429 Train acc:0.6559454191033138 Test acc:0.5933179723502304 Train loss:0.011696749366819859\n",
      "Epoch:430 Train acc:0.6622807017543859 Test acc:0.5771889400921659 Train loss:0.011414702981710434\n",
      "Epoch:431 Train acc:0.6632553606237817 Test acc:0.5967741935483871 Train loss:0.011506909504532814\n",
      "Epoch:432 Train acc:0.6715399610136452 Test acc:0.5783410138248848 Train loss:0.011266798712313175\n",
      "Epoch:433 Train acc:0.6754385964912281 Test acc:0.5841013824884793 Train loss:0.011263427324593067\n",
      "Epoch:434 Train acc:0.685672514619883 Test acc:0.586405529953917 Train loss:0.011092908680438995\n",
      "Epoch:435 Train acc:0.6666666666666666 Test acc:0.5783410138248848 Train loss:0.011080868542194366\n",
      "Epoch:436 Train acc:0.6851851851851852 Test acc:0.6002304147465438 Train loss:0.01105312630534172\n",
      "Epoch:437 Train acc:0.6739766081871345 Test acc:0.6013824884792627 Train loss:0.011119954288005829\n",
      "Epoch:438 Train acc:0.6910331384015594 Test acc:0.5806451612903226 Train loss:0.010920720174908638\n",
      "Epoch:439 Train acc:0.6837231968810916 Test acc:0.5794930875576036 Train loss:0.011217395775020123\n",
      "Epoch:440 Train acc:0.6929824561403509 Test acc:0.6117511520737328 Train loss:0.010877639055252075\n",
      "Epoch:441 Train acc:0.6364522417153996 Test acc:0.6152073732718893 Train loss:0.012391760013997555\n",
      "Epoch:442 Train acc:0.6764132553606238 Test acc:0.5714285714285714 Train loss:0.011405033059418201\n",
      "Epoch:443 Train acc:0.6861598440545809 Test acc:0.6025345622119815 Train loss:0.010998787358403206\n",
      "Epoch:444 Train acc:0.6734892787524367 Test acc:0.5702764976958525 Train loss:0.0110979238525033\n",
      "Epoch:445 Train acc:0.6866471734892787 Test acc:0.597926267281106 Train loss:0.010997054167091846\n",
      "Epoch:446 Train acc:0.6949317738791423 Test acc:0.619815668202765 Train loss:0.011110703460872173\n",
      "Epoch:447 Train acc:0.699317738791423 Test acc:0.5990783410138248 Train loss:0.010819223709404469\n",
      "Epoch:448 Train acc:0.6905458089668616 Test acc:0.5702764976958525 Train loss:0.010848110541701317\n",
      "Epoch:449 Train acc:0.6900584795321637 Test acc:0.5898617511520737 Train loss:0.01103533711284399\n",
      "Epoch:450 Train acc:0.6803118908382066 Test acc:0.5956221198156681 Train loss:0.011143158189952374\n",
      "Epoch:451 Train acc:0.6939571150097466 Test acc:0.5737327188940092 Train loss:0.010869637131690979\n",
      "Epoch:452 Train acc:0.6949317738791423 Test acc:0.6013824884792627 Train loss:0.01081009954214096\n",
      "Epoch:453 Train acc:0.6949317738791423 Test acc:0.5587557603686636 Train loss:0.010937257669866085\n",
      "Epoch:454 Train acc:0.6973684210526315 Test acc:0.5910138248847926 Train loss:0.010963674634695053\n",
      "Epoch:455 Train acc:0.706140350877193 Test acc:0.576036866359447 Train loss:0.010760416276752949\n",
      "Epoch:456 Train acc:0.7071150097465887 Test acc:0.6048387096774194 Train loss:0.010749162174761295\n",
      "Epoch:457 Train acc:0.7002923976608187 Test acc:0.6025345622119815 Train loss:0.010597323067486286\n",
      "Epoch:458 Train acc:0.7032163742690059 Test acc:0.5794930875576036 Train loss:0.010756346397101879\n",
      "Epoch:459 Train acc:0.7017543859649122 Test acc:0.5587557603686636 Train loss:0.010528967715799809\n",
      "Epoch:460 Train acc:0.6978557504873294 Test acc:0.5506912442396313 Train loss:0.010926988907158375\n",
      "Epoch:461 Train acc:0.6973684210526315 Test acc:0.5622119815668203 Train loss:0.010997514240443707\n",
      "Epoch:462 Train acc:0.7212475633528265 Test acc:0.619815668202765 Train loss:0.010482440702617168\n",
      "Epoch:463 Train acc:0.6949317738791423 Test acc:0.597926267281106 Train loss:0.010869544930756092\n",
      "Epoch:464 Train acc:0.7115009746588694 Test acc:0.5622119815668203 Train loss:0.010640785098075867\n",
      "Epoch:465 Train acc:0.6783625730994152 Test acc:0.5622119815668203 Train loss:0.011230751872062683\n",
      "Epoch:466 Train acc:0.7017543859649122 Test acc:0.5921658986175116 Train loss:0.010872965678572655\n",
      "Epoch:467 Train acc:0.7041910331384016 Test acc:0.5564516129032258 Train loss:0.010515336878597736\n",
      "Epoch:468 Train acc:0.7149122807017544 Test acc:0.5771889400921659 Train loss:0.010514872148633003\n",
      "Epoch:469 Train acc:0.7037037037037037 Test acc:0.5691244239631337 Train loss:0.010728721506893635\n",
      "Epoch:470 Train acc:0.7110136452241715 Test acc:0.5725806451612904 Train loss:0.010945291258394718\n",
      "Epoch:471 Train acc:0.6442495126705653 Test acc:0.6002304147465438 Train loss:0.012655147351324558\n",
      "Epoch:472 Train acc:0.6764132553606238 Test acc:0.6117511520737328 Train loss:0.01165565475821495\n",
      "Epoch:473 Train acc:0.6866471734892787 Test acc:0.597926267281106 Train loss:0.011489585041999817\n",
      "Epoch:474 Train acc:0.6832358674463938 Test acc:0.5771889400921659 Train loss:0.011413313448429108\n",
      "Epoch:475 Train acc:0.6939571150097466 Test acc:0.6059907834101382 Train loss:0.011149858124554157\n",
      "Epoch:476 Train acc:0.6754385964912281 Test acc:0.5910138248847926 Train loss:0.011313989758491516\n",
      "Epoch:477 Train acc:0.678849902534113 Test acc:0.5622119815668203 Train loss:0.011004401370882988\n",
      "Epoch:478 Train acc:0.6866471734892787 Test acc:0.5817972350230415 Train loss:0.01119509432464838\n",
      "Epoch:479 Train acc:0.6895711500974658 Test acc:0.5576036866359447 Train loss:0.011298282071948051\n",
      "Epoch:480 Train acc:0.6890838206627681 Test acc:0.5725806451612904 Train loss:0.011087439954280853\n",
      "Epoch:481 Train acc:0.6905458089668616 Test acc:0.5898617511520737 Train loss:0.01104183029383421\n",
      "Epoch:482 Train acc:0.7100389863547758 Test acc:0.5806451612903226 Train loss:0.010967600159347057\n",
      "Epoch:483 Train acc:0.6237816764132553 Test acc:0.5714285714285714 Train loss:0.0128891970962286\n",
      "Epoch:484 Train acc:0.6920077972709552 Test acc:0.5852534562211982 Train loss:0.011414438486099243\n",
      "Epoch:485 Train acc:0.6837231968810916 Test acc:0.5898617511520737 Train loss:0.011079737916588783\n",
      "Epoch:486 Train acc:0.6915204678362573 Test acc:0.5725806451612904 Train loss:0.011172093451023102\n",
      "Epoch:487 Train acc:0.6988304093567251 Test acc:0.5783410138248848 Train loss:0.01107861939817667\n",
      "Epoch:488 Train acc:0.6998050682261209 Test acc:0.5852534562211982 Train loss:0.011088144965469837\n",
      "Epoch:489 Train acc:0.6827485380116959 Test acc:0.5829493087557603 Train loss:0.011036043055355549\n",
      "Epoch:490 Train acc:0.7007797270955166 Test acc:0.5852534562211982 Train loss:0.010841263458132744\n",
      "Epoch:491 Train acc:0.6895711500974658 Test acc:0.5956221198156681 Train loss:0.011278658173978329\n",
      "Epoch:492 Train acc:0.7134502923976608 Test acc:0.5702764976958525 Train loss:0.011135327629745007\n",
      "Epoch:493 Train acc:0.6915204678362573 Test acc:0.6059907834101382 Train loss:0.011175048537552357\n",
      "Epoch:494 Train acc:0.699317738791423 Test acc:0.5933179723502304 Train loss:0.010839955881237984\n",
      "Epoch:495 Train acc:0.7090643274853801 Test acc:0.5668202764976958 Train loss:0.010554283857345581\n",
      "Epoch:496 Train acc:0.7090643274853801 Test acc:0.6105990783410138 Train loss:0.010601463727653027\n",
      "Epoch:497 Train acc:0.7115009746588694 Test acc:0.5944700460829493 Train loss:0.01064323540776968\n",
      "Epoch:498 Train acc:0.7056530214424951 Test acc:0.5725806451612904 Train loss:0.010562814772129059\n",
      "Epoch:499 Train acc:0.7188109161793372 Test acc:0.5829493087557603 Train loss:0.010613532736897469\n",
      "Epoch:500 Train acc:0.7095516569200779 Test acc:0.586405529953917 Train loss:0.010657484643161297\n",
      "Epoch:501 Train acc:0.7251461988304093 Test acc:0.5771889400921659 Train loss:0.010578702203929424\n",
      "Epoch:502 Train acc:0.7119883040935673 Test acc:0.5841013824884793 Train loss:0.010764104314148426\n",
      "Epoch:503 Train acc:0.7022417153996101 Test acc:0.5702764976958525 Train loss:0.010753747075796127\n",
      "Epoch:504 Train acc:0.7192982456140351 Test acc:0.5967741935483871 Train loss:0.010544543154537678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:505 Train acc:0.7275828460038987 Test acc:0.6163594470046083 Train loss:0.010636156424880028\n",
      "Epoch:506 Train acc:0.7115009746588694 Test acc:0.6336405529953917 Train loss:0.010743031278252602\n",
      "Epoch:507 Train acc:0.7041910331384016 Test acc:0.5829493087557603 Train loss:0.01071868184953928\n",
      "Epoch:508 Train acc:0.7105263157894737 Test acc:0.5610599078341014 Train loss:0.010565179400146008\n",
      "Epoch:509 Train acc:0.7202729044834308 Test acc:0.5875576036866359 Train loss:0.010568118654191494\n",
      "Epoch:510 Train acc:0.7183235867446394 Test acc:0.6002304147465438 Train loss:0.010366274043917656\n",
      "Epoch:511 Train acc:0.7192982456140351 Test acc:0.5944700460829493 Train loss:0.010642638429999352\n",
      "Epoch:512 Train acc:0.7124756335282652 Test acc:0.5564516129032258 Train loss:0.010757911950349808\n",
      "Epoch:513 Train acc:0.7227095516569201 Test acc:0.586405529953917 Train loss:0.010642929002642632\n",
      "Epoch:514 Train acc:0.7173489278752436 Test acc:0.6082949308755761 Train loss:0.010907364077866077\n",
      "Epoch:515 Train acc:0.6632553606237817 Test acc:0.5633640552995391 Train loss:0.012071396224200726\n",
      "Epoch:516 Train acc:0.7144249512670565 Test acc:0.5737327188940092 Train loss:0.010767806321382523\n",
      "Epoch:517 Train acc:0.7134502923976608 Test acc:0.5714285714285714 Train loss:0.010561367496848106\n",
      "Epoch:518 Train acc:0.7319688109161794 Test acc:0.5748847926267281 Train loss:0.010203325189650059\n",
      "Epoch:519 Train acc:0.7270955165692008 Test acc:0.6071428571428571 Train loss:0.010316245257854462\n",
      "Epoch:520 Train acc:0.7129629629629629 Test acc:0.5702764976958525 Train loss:0.010603089816868305\n",
      "Epoch:521 Train acc:0.7178362573099415 Test acc:0.5725806451612904 Train loss:0.010346869938075542\n",
      "Epoch:522 Train acc:0.73635477582846 Test acc:0.5702764976958525 Train loss:0.010335645638406277\n",
      "Epoch:523 Train acc:0.7256335282651072 Test acc:0.5817972350230415 Train loss:0.010187500156462193\n",
      "Epoch:524 Train acc:0.7207602339181286 Test acc:0.586405529953917 Train loss:0.010303816758096218\n",
      "Epoch:525 Train acc:0.7353801169590644 Test acc:0.5668202764976958 Train loss:0.010515297763049603\n",
      "Epoch:526 Train acc:0.7358674463937622 Test acc:0.5714285714285714 Train loss:0.010154050774872303\n",
      "Epoch:527 Train acc:0.7344054580896686 Test acc:0.5679723502304147 Train loss:0.010371699929237366\n",
      "Epoch:528 Train acc:0.6807992202729045 Test acc:0.5875576036866359 Train loss:0.011855339631438255\n",
      "Epoch:529 Train acc:0.723196881091618 Test acc:0.576036866359447 Train loss:0.010548154823482037\n",
      "Epoch:530 Train acc:0.7319688109161794 Test acc:0.5852534562211982 Train loss:0.010509556159377098\n",
      "Epoch:531 Train acc:0.7241715399610137 Test acc:0.5910138248847926 Train loss:0.010321897454559803\n",
      "Epoch:532 Train acc:0.7227095516569201 Test acc:0.5817972350230415 Train loss:0.010423705913126469\n",
      "Epoch:533 Train acc:0.7129629629629629 Test acc:0.5921658986175116 Train loss:0.01071377843618393\n",
      "Epoch:534 Train acc:0.7373294346978557 Test acc:0.554147465437788 Train loss:0.010186047293245792\n",
      "Epoch:535 Train acc:0.7353801169590644 Test acc:0.5633640552995391 Train loss:0.010128235444426537\n",
      "Epoch:536 Train acc:0.7417153996101364 Test acc:0.5748847926267281 Train loss:0.010298287495970726\n",
      "Epoch:537 Train acc:0.7383040935672515 Test acc:0.5875576036866359 Train loss:0.010273214429616928\n",
      "Epoch:538 Train acc:0.7339181286549707 Test acc:0.5990783410138248 Train loss:0.010200810618698597\n",
      "Epoch:539 Train acc:0.7266081871345029 Test acc:0.5691244239631337 Train loss:0.010492226108908653\n",
      "Epoch:540 Train acc:0.73635477582846 Test acc:0.5933179723502304 Train loss:0.009957858361303806\n",
      "Epoch:541 Train acc:0.746588693957115 Test acc:0.5679723502304147 Train loss:0.010179461911320686\n",
      "Epoch:542 Train acc:0.7348927875243665 Test acc:0.5817972350230415 Train loss:0.010223429650068283\n",
      "Epoch:543 Train acc:0.7446393762183235 Test acc:0.6071428571428571 Train loss:0.009888400323688984\n",
      "Epoch:544 Train acc:0.7217348927875243 Test acc:0.5610599078341014 Train loss:0.010179494507610798\n",
      "Epoch:545 Train acc:0.7251461988304093 Test acc:0.5771889400921659 Train loss:0.01051735132932663\n",
      "Epoch:546 Train acc:0.746588693957115 Test acc:0.565668202764977 Train loss:0.009889851324260235\n",
      "Epoch:547 Train acc:0.7470760233918129 Test acc:0.5806451612903226 Train loss:0.010263525880873203\n",
      "Epoch:548 Train acc:0.7056530214424951 Test acc:0.5898617511520737 Train loss:0.010764618404209614\n",
      "Epoch:549 Train acc:0.7426900584795322 Test acc:0.5472350230414746 Train loss:0.010125127620995045\n",
      "Epoch:550 Train acc:0.7353801169590644 Test acc:0.5771889400921659 Train loss:0.010131694376468658\n",
      "Epoch:551 Train acc:0.753411306042885 Test acc:0.5668202764976958 Train loss:0.009663324803113937\n",
      "Epoch:552 Train acc:0.7475633528265108 Test acc:0.5852534562211982 Train loss:0.00985860638320446\n",
      "Epoch:553 Train acc:0.7485380116959064 Test acc:0.5841013824884793 Train loss:0.009704381227493286\n",
      "Epoch:554 Train acc:0.7319688109161794 Test acc:0.5921658986175116 Train loss:0.010312411934137344\n",
      "Epoch:555 Train acc:0.732943469785575 Test acc:0.5887096774193549 Train loss:0.01060563512146473\n",
      "Epoch:556 Train acc:0.7495126705653021 Test acc:0.6105990783410138 Train loss:0.009864725172519684\n",
      "Epoch:557 Train acc:0.7504873294346979 Test acc:0.5944700460829493 Train loss:0.010261631570756435\n",
      "Epoch:558 Train acc:0.7144249512670565 Test acc:0.5967741935483871 Train loss:0.010667027905583382\n",
      "Epoch:559 Train acc:0.7504873294346979 Test acc:0.6013824884792627 Train loss:0.009832202456891537\n",
      "Epoch:560 Train acc:0.7490253411306043 Test acc:0.5921658986175116 Train loss:0.010026191361248493\n",
      "Epoch:561 Train acc:0.7436647173489279 Test acc:0.6140552995391705 Train loss:0.009850316680967808\n",
      "Epoch:562 Train acc:0.7378167641325536 Test acc:0.5794930875576036 Train loss:0.010561100207269192\n",
      "Epoch:563 Train acc:0.7383040935672515 Test acc:0.6082949308755761 Train loss:0.01017315499484539\n",
      "Epoch:564 Train acc:0.7422027290448343 Test acc:0.5956221198156681 Train loss:0.009831860661506653\n",
      "Epoch:565 Train acc:0.74317738791423 Test acc:0.5910138248847926 Train loss:0.009696089662611485\n",
      "Epoch:566 Train acc:0.7577972709551657 Test acc:0.6048387096774194 Train loss:0.00953663419932127\n",
      "Epoch:567 Train acc:0.7422027290448343 Test acc:0.5587557603686636 Train loss:0.009704774245619774\n",
      "Epoch:568 Train acc:0.7587719298245614 Test acc:0.5748847926267281 Train loss:0.009902331978082657\n",
      "Epoch:569 Train acc:0.7436647173489279 Test acc:0.5691244239631337 Train loss:0.00984550453722477\n",
      "Epoch:570 Train acc:0.7602339181286549 Test acc:0.5495391705069125 Train loss:0.009709064848721027\n",
      "Epoch:571 Train acc:0.74317738791423 Test acc:0.5622119815668203 Train loss:0.010297655127942562\n",
      "Epoch:572 Train acc:0.7387914230019493 Test acc:0.5230414746543779 Train loss:0.010247749276459217\n",
      "Epoch:573 Train acc:0.7158869395711501 Test acc:0.6232718894009217 Train loss:0.010458694770932198\n",
      "Epoch:574 Train acc:0.75 Test acc:0.6025345622119815 Train loss:0.009807850234210491\n",
      "Epoch:575 Train acc:0.7553606237816765 Test acc:0.586405529953917 Train loss:0.010016107931733131\n",
      "Epoch:576 Train acc:0.7348927875243665 Test acc:0.5898617511520737 Train loss:0.010380246676504612\n",
      "Epoch:577 Train acc:0.7480506822612085 Test acc:0.5702764976958525 Train loss:0.01009136438369751\n",
      "Epoch:578 Train acc:0.7495126705653021 Test acc:0.5622119815668203 Train loss:0.010129737667739391\n",
      "Epoch:579 Train acc:0.7436647173489279 Test acc:0.5933179723502304 Train loss:0.010020186193287373\n",
      "Epoch:580 Train acc:0.7543859649122807 Test acc:0.5748847926267281 Train loss:0.010127270594239235\n",
      "Epoch:581 Train acc:0.7573099415204678 Test acc:0.5414746543778802 Train loss:0.009878977201879025\n",
      "Epoch:582 Train acc:0.7592592592592593 Test acc:0.597926267281106 Train loss:0.010045267641544342\n",
      "Epoch:583 Train acc:0.6754385964912281 Test acc:0.618663594470046 Train loss:0.012077226303517818\n",
      "Epoch:584 Train acc:0.7397660818713451 Test acc:0.5645161290322581 Train loss:0.010212277062237263\n",
      "Epoch:585 Train acc:0.7509746588693957 Test acc:0.5910138248847926 Train loss:0.01002239529043436\n",
      "Epoch:586 Train acc:0.7607212475633528 Test acc:0.586405529953917 Train loss:0.010063904337584972\n",
      "Epoch:587 Train acc:0.7548732943469786 Test acc:0.565668202764977 Train loss:0.009667566046118736\n",
      "Epoch:588 Train acc:0.7538986354775828 Test acc:0.5933179723502304 Train loss:0.009689289145171642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:589 Train acc:0.7582846003898636 Test acc:0.586405529953917 Train loss:0.009689581580460072\n",
      "Epoch:590 Train acc:0.7665692007797271 Test acc:0.5910138248847926 Train loss:0.009498275816440582\n",
      "Epoch:591 Train acc:0.75682261208577 Test acc:0.5622119815668203 Train loss:0.009653441607952118\n",
      "Epoch:592 Train acc:0.7514619883040936 Test acc:0.5817972350230415 Train loss:0.00990760326385498\n",
      "Epoch:593 Train acc:0.75682261208577 Test acc:0.6002304147465438 Train loss:0.00973423384130001\n",
      "Epoch:594 Train acc:0.7597465886939572 Test acc:0.5852534562211982 Train loss:0.009565385058522224\n",
      "Epoch:595 Train acc:0.7577972709551657 Test acc:0.5887096774193549 Train loss:0.009562327526509762\n",
      "Epoch:596 Train acc:0.7655945419103314 Test acc:0.5829493087557603 Train loss:0.009396321140229702\n",
      "Epoch:597 Train acc:0.7660818713450293 Test acc:0.5956221198156681 Train loss:0.00939332228153944\n",
      "Epoch:598 Train acc:0.7709551656920078 Test acc:0.5691244239631337 Train loss:0.009755349718034267\n",
      "Epoch:599 Train acc:0.7616959064327485 Test acc:0.4861751152073733 Train loss:0.009822259657084942\n",
      "Epoch:600 Train acc:0.7051656920077972 Test acc:0.6013824884792627 Train loss:0.011609555222094059\n",
      "Epoch:601 Train acc:0.7514619883040936 Test acc:0.5725806451612904 Train loss:0.009742068126797676\n",
      "Epoch:602 Train acc:0.7607212475633528 Test acc:0.6059907834101382 Train loss:0.00996171124279499\n",
      "Epoch:603 Train acc:0.767056530214425 Test acc:0.5771889400921659 Train loss:0.009545641951262951\n",
      "Epoch:604 Train acc:0.7699805068226121 Test acc:0.5829493087557603 Train loss:0.009517344646155834\n",
      "Epoch:605 Train acc:0.7714424951267057 Test acc:0.6025345622119815 Train loss:0.009504341520369053\n",
      "Epoch:606 Train acc:0.75682261208577 Test acc:0.6129032258064516 Train loss:0.009794451296329498\n",
      "Epoch:607 Train acc:0.7733918128654971 Test acc:0.5622119815668203 Train loss:0.009472465142607689\n",
      "Epoch:608 Train acc:0.7602339181286549 Test acc:0.6163594470046083 Train loss:0.009617110714316368\n",
      "Epoch:609 Train acc:0.7222222222222222 Test acc:0.6025345622119815 Train loss:0.01088058203458786\n",
      "Epoch:610 Train acc:0.7646198830409356 Test acc:0.5944700460829493 Train loss:0.009637502022087574\n",
      "Epoch:611 Train acc:0.7641325536062378 Test acc:0.5967741935483871 Train loss:0.009553704410791397\n",
      "Epoch:612 Train acc:0.7646198830409356 Test acc:0.619815668202765 Train loss:0.009935962967574596\n",
      "Epoch:613 Train acc:0.7407407407407407 Test acc:0.6394009216589862 Train loss:0.010258548893034458\n",
      "Epoch:614 Train acc:0.75682261208577 Test acc:0.5852534562211982 Train loss:0.009681458584964275\n",
      "Epoch:615 Train acc:0.7587719298245614 Test acc:0.5806451612903226 Train loss:0.009734699502587318\n",
      "Epoch:616 Train acc:0.7719298245614035 Test acc:0.6071428571428571 Train loss:0.009650797583162785\n",
      "Epoch:617 Train acc:0.7709551656920078 Test acc:0.5829493087557603 Train loss:0.009531003423035145\n",
      "Epoch:618 Train acc:0.7558479532163743 Test acc:0.6036866359447005 Train loss:0.009878664277493954\n",
      "Epoch:619 Train acc:0.7275828460038987 Test acc:0.6002304147465438 Train loss:0.010929711163043976\n",
      "Epoch:620 Train acc:0.767056530214425 Test acc:0.6221198156682027 Train loss:0.009602184407413006\n",
      "Epoch:621 Train acc:0.7733918128654971 Test acc:0.6267281105990783 Train loss:0.009709803387522697\n",
      "Epoch:622 Train acc:0.7509746588693957 Test acc:0.6278801843317973 Train loss:0.009877742268145084\n",
      "Epoch:623 Train acc:0.7660818713450293 Test acc:0.6382488479262672 Train loss:0.009237111546099186\n",
      "Epoch:624 Train acc:0.7729044834307992 Test acc:0.5990783410138248 Train loss:0.009445293806493282\n",
      "Epoch:625 Train acc:0.783625730994152 Test acc:0.5691244239631337 Train loss:0.009566430002450943\n",
      "Epoch:626 Train acc:0.7456140350877193 Test acc:0.6071428571428571 Train loss:0.009805400855839252\n",
      "Epoch:627 Train acc:0.7733918128654971 Test acc:0.6002304147465438 Train loss:0.009125561453402042\n",
      "Epoch:628 Train acc:0.780214424951267 Test acc:0.5552995391705069 Train loss:0.009484565816819668\n",
      "Epoch:629 Train acc:0.7753411306042886 Test acc:0.6025345622119815 Train loss:0.008923741988837719\n",
      "Epoch:630 Train acc:0.7855750487329435 Test acc:0.6002304147465438 Train loss:0.008873861283063889\n",
      "Epoch:631 Train acc:0.7846003898635477 Test acc:0.5933179723502304 Train loss:0.009007221087813377\n",
      "Epoch:632 Train acc:0.7787524366471735 Test acc:0.6221198156682027 Train loss:0.009175200015306473\n",
      "Epoch:633 Train acc:0.7748538011695907 Test acc:0.5829493087557603 Train loss:0.008939106948673725\n",
      "Epoch:634 Train acc:0.7753411306042886 Test acc:0.5841013824884793 Train loss:0.009065045043826103\n",
      "Epoch:635 Train acc:0.7909356725146199 Test acc:0.5944700460829493 Train loss:0.008914776146411896\n",
      "Epoch:636 Train acc:0.7811890838206628 Test acc:0.5771889400921659 Train loss:0.00918711256235838\n",
      "Epoch:637 Train acc:0.7577972709551657 Test acc:0.6232718894009217 Train loss:0.009444382041692734\n",
      "Epoch:638 Train acc:0.7753411306042886 Test acc:0.5668202764976958 Train loss:0.00903579592704773\n",
      "Epoch:639 Train acc:0.7870370370370371 Test acc:0.5956221198156681 Train loss:0.009102905169129372\n",
      "Epoch:640 Train acc:0.7758284600389863 Test acc:0.6013824884792627 Train loss:0.009152865037322044\n",
      "Epoch:641 Train acc:0.7880116959064327 Test acc:0.5875576036866359 Train loss:0.008762357756495476\n",
      "Epoch:642 Train acc:0.7831384015594542 Test acc:0.597926267281106 Train loss:0.00934536475688219\n",
      "Epoch:643 Train acc:0.7889863547758285 Test acc:0.5806451612903226 Train loss:0.009084774181246758\n",
      "Epoch:644 Train acc:0.7889863547758285 Test acc:0.5633640552995391 Train loss:0.009007103741168976\n",
      "Epoch:645 Train acc:0.7577972709551657 Test acc:0.586405529953917 Train loss:0.00961737148463726\n",
      "Epoch:646 Train acc:0.7811890838206628 Test acc:0.6025345622119815 Train loss:0.009307836182415485\n",
      "Epoch:647 Train acc:0.7597465886939572 Test acc:0.5806451612903226 Train loss:0.009597115218639374\n",
      "Epoch:648 Train acc:0.6793372319688109 Test acc:0.5472350230414746 Train loss:0.011710763908922672\n",
      "Epoch:649 Train acc:0.6895711500974658 Test acc:0.5817972350230415 Train loss:0.011557013727724552\n",
      "Epoch:650 Train acc:0.6783625730994152 Test acc:0.5691244239631337 Train loss:0.011328133754432201\n",
      "Epoch:651 Train acc:0.7149122807017544 Test acc:0.5668202764976958 Train loss:0.01125385332852602\n",
      "Epoch:652 Train acc:0.6798245614035088 Test acc:0.5875576036866359 Train loss:0.012562461197376251\n",
      "Epoch:653 Train acc:0.7124756335282652 Test acc:0.6463133640552995 Train loss:0.010835308581590652\n",
      "Epoch:654 Train acc:0.7227095516569201 Test acc:0.6117511520737328 Train loss:0.010804973542690277\n",
      "Epoch:655 Train acc:0.7217348927875243 Test acc:0.6232718894009217 Train loss:0.010446696542203426\n",
      "Epoch:656 Train acc:0.7402534113060428 Test acc:0.6244239631336406 Train loss:0.010225810110569\n",
      "Epoch:657 Train acc:0.7451267056530214 Test acc:0.6152073732718893 Train loss:0.010292931459844112\n",
      "Epoch:658 Train acc:0.7305068226120858 Test acc:0.6370967741935484 Train loss:0.010215772315859795\n",
      "Epoch:659 Train acc:0.7422027290448343 Test acc:0.597926267281106 Train loss:0.010094796307384968\n",
      "Epoch:660 Train acc:0.7485380116959064 Test acc:0.6232718894009217 Train loss:0.009842202067375183\n",
      "Epoch:661 Train acc:0.7504873294346979 Test acc:0.5944700460829493 Train loss:0.01039836835116148\n",
      "Epoch:662 Train acc:0.7543859649122807 Test acc:0.6209677419354839 Train loss:0.009921170771121979\n",
      "Epoch:663 Train acc:0.7621832358674464 Test acc:0.6221198156682027 Train loss:0.009817525744438171\n",
      "Epoch:664 Train acc:0.7451267056530214 Test acc:0.6048387096774194 Train loss:0.009979325346648693\n",
      "Epoch:665 Train acc:0.7490253411306043 Test acc:0.6255760368663594 Train loss:0.010257355868816376\n",
      "Epoch:666 Train acc:0.7270955165692008 Test acc:0.6209677419354839 Train loss:0.010153887793421745\n",
      "Epoch:667 Train acc:0.7524366471734892 Test acc:0.618663594470046 Train loss:0.009681086987257004\n",
      "Epoch:668 Train acc:0.7631578947368421 Test acc:0.618663594470046 Train loss:0.009659313596785069\n",
      "Epoch:669 Train acc:0.7485380116959064 Test acc:0.6221198156682027 Train loss:0.009914546273648739\n",
      "Epoch:670 Train acc:0.7631578947368421 Test acc:0.5852534562211982 Train loss:0.009626515209674835\n",
      "Epoch:671 Train acc:0.7665692007797271 Test acc:0.6117511520737328 Train loss:0.009585429914295673\n",
      "Epoch:672 Train acc:0.7475633528265108 Test acc:0.6071428571428571 Train loss:0.009868795983493328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:673 Train acc:0.7646198830409356 Test acc:0.619815668202765 Train loss:0.009758650325238705\n",
      "Epoch:674 Train acc:0.76364522417154 Test acc:0.6359447004608295 Train loss:0.009733544662594795\n",
      "Epoch:675 Train acc:0.7675438596491229 Test acc:0.6071428571428571 Train loss:0.009737919084727764\n",
      "Epoch:676 Train acc:0.7743664717348928 Test acc:0.6244239631336406 Train loss:0.0093598747625947\n",
      "Epoch:677 Train acc:0.7714424951267057 Test acc:0.5910138248847926 Train loss:0.009660947136580944\n",
      "Epoch:678 Train acc:0.7719298245614035 Test acc:0.6082949308755761 Train loss:0.009848389774560928\n",
      "Epoch:679 Train acc:0.7548732943469786 Test acc:0.6163594470046083 Train loss:0.009655477479100227\n",
      "Epoch:680 Train acc:0.7675438596491229 Test acc:0.6129032258064516 Train loss:0.00936034508049488\n",
      "Epoch:681 Train acc:0.767056530214425 Test acc:0.5990783410138248 Train loss:0.009433437138795853\n",
      "Epoch:682 Train acc:0.7665692007797271 Test acc:0.6221198156682027 Train loss:0.00968716200441122\n",
      "Epoch:683 Train acc:0.76364522417154 Test acc:0.6117511520737328 Train loss:0.009603751823306084\n",
      "Epoch:684 Train acc:0.767056530214425 Test acc:0.6036866359447005 Train loss:0.009411866776645184\n",
      "Epoch:685 Train acc:0.7563352826510721 Test acc:0.6866359447004609 Train loss:0.009830889292061329\n",
      "Epoch:686 Train acc:0.7743664717348928 Test acc:0.6278801843317973 Train loss:0.009538110345602036\n",
      "Epoch:687 Train acc:0.7787524366471735 Test acc:0.6002304147465438 Train loss:0.009423986077308655\n",
      "Epoch:688 Train acc:0.7792397660818714 Test acc:0.6094470046082949 Train loss:0.009380596689879894\n",
      "Epoch:689 Train acc:0.7743664717348928 Test acc:0.5887096774193549 Train loss:0.009434404782950878\n",
      "Epoch:690 Train acc:0.7797270955165692 Test acc:0.6129032258064516 Train loss:0.009490571916103363\n",
      "Epoch:691 Train acc:0.7748538011695907 Test acc:0.6071428571428571 Train loss:0.009207213297486305\n",
      "Epoch:692 Train acc:0.7772904483430799 Test acc:0.6405529953917051 Train loss:0.009183229878544807\n",
      "Epoch:693 Train acc:0.7841130604288499 Test acc:0.6359447004608295 Train loss:0.009535809978842735\n",
      "Epoch:694 Train acc:0.7626705653021443 Test acc:0.6129032258064516 Train loss:0.009417069144546986\n",
      "Epoch:695 Train acc:0.7860623781676414 Test acc:0.6336405529953917 Train loss:0.009206355549395084\n",
      "Epoch:696 Train acc:0.7699805068226121 Test acc:0.6059907834101382 Train loss:0.009434333071112633\n",
      "Epoch:697 Train acc:0.7724171539961013 Test acc:0.5944700460829493 Train loss:0.00942761916667223\n",
      "Epoch:698 Train acc:0.7889863547758285 Test acc:0.6117511520737328 Train loss:0.008891776204109192\n",
      "Epoch:699 Train acc:0.7933723196881092 Test acc:0.6117511520737328 Train loss:0.008832587860524654\n",
      "Epoch:700 Train acc:0.7894736842105263 Test acc:0.6267281105990783 Train loss:0.009019490331411362\n",
      "Epoch:701 Train acc:0.7821637426900585 Test acc:0.6048387096774194 Train loss:0.008827561512589455\n",
      "Epoch:702 Train acc:0.7904483430799221 Test acc:0.6244239631336406 Train loss:0.008875361643731594\n",
      "Epoch:703 Train acc:0.7884990253411306 Test acc:0.597926267281106 Train loss:0.009203710593283176\n",
      "Epoch:704 Train acc:0.7894736842105263 Test acc:0.6013824884792627 Train loss:0.00875938218086958\n",
      "Epoch:705 Train acc:0.7860623781676414 Test acc:0.6117511520737328 Train loss:0.00899118185043335\n",
      "Epoch:706 Train acc:0.7967836257309941 Test acc:0.6129032258064516 Train loss:0.008837860077619553\n",
      "Epoch:707 Train acc:0.7680311890838206 Test acc:0.6094470046082949 Train loss:0.009618177078664303\n",
      "Epoch:708 Train acc:0.7811890838206628 Test acc:0.6232718894009217 Train loss:0.009097580797970295\n",
      "Epoch:709 Train acc:0.7943469785575049 Test acc:0.618663594470046 Train loss:0.008834784850478172\n",
      "Epoch:710 Train acc:0.7904483430799221 Test acc:0.5933179723502304 Train loss:0.008928082883358002\n",
      "Epoch:711 Train acc:0.7987329434697856 Test acc:0.5806451612903226 Train loss:0.009086228907108307\n",
      "Epoch:712 Train acc:0.7914230019493177 Test acc:0.6129032258064516 Train loss:0.008881653659045696\n",
      "Epoch:713 Train acc:0.7894736842105263 Test acc:0.6486175115207373 Train loss:0.00892785843461752\n",
      "Epoch:714 Train acc:0.7967836257309941 Test acc:0.6036866359447005 Train loss:0.009042743593454361\n",
      "Epoch:715 Train acc:0.8031189083820662 Test acc:0.5887096774193549 Train loss:0.008610466495156288\n",
      "Epoch:716 Train acc:0.7997076023391813 Test acc:0.5967741935483871 Train loss:0.008468926884233952\n",
      "Epoch:717 Train acc:0.8031189083820662 Test acc:0.5933179723502304 Train loss:0.008571960031986237\n",
      "Epoch:718 Train acc:0.7982456140350878 Test acc:0.5967741935483871 Train loss:0.008897471241652966\n",
      "Epoch:719 Train acc:0.7870370370370371 Test acc:0.6071428571428571 Train loss:0.008880643174052238\n",
      "Epoch:720 Train acc:0.8016569200779727 Test acc:0.5910138248847926 Train loss:0.008787591010332108\n",
      "Epoch:721 Train acc:0.783625730994152 Test acc:0.6105990783410138 Train loss:0.009222771972417831\n",
      "Epoch:722 Train acc:0.8011695906432749 Test acc:0.6105990783410138 Train loss:0.008907114155590534\n",
      "Epoch:723 Train acc:0.7714424951267057 Test acc:0.6290322580645161 Train loss:0.009421631693840027\n",
      "Epoch:724 Train acc:0.8026315789473685 Test acc:0.5921658986175116 Train loss:0.008639563806355\n",
      "Epoch:725 Train acc:0.7958089668615984 Test acc:0.6428571428571429 Train loss:0.0087999626994133\n",
      "Epoch:726 Train acc:0.783625730994152 Test acc:0.5956221198156681 Train loss:0.009110305458307266\n",
      "Epoch:727 Train acc:0.7958089668615984 Test acc:0.619815668202765 Train loss:0.009116888046264648\n",
      "Epoch:728 Train acc:0.7953216374269005 Test acc:0.6048387096774194 Train loss:0.00885386299341917\n",
      "Epoch:729 Train acc:0.8099415204678363 Test acc:0.5967741935483871 Train loss:0.00861943420022726\n",
      "Epoch:730 Train acc:0.8104288499025342 Test acc:0.618663594470046 Train loss:0.008694109506905079\n",
      "Epoch:731 Train acc:0.8016569200779727 Test acc:0.6140552995391705 Train loss:0.008521954528987408\n",
      "Epoch:732 Train acc:0.8065302144249513 Test acc:0.6244239631336406 Train loss:0.008769704960286617\n",
      "Epoch:733 Train acc:0.8021442495126706 Test acc:0.5702764976958525 Train loss:0.008648565970361233\n",
      "Epoch:734 Train acc:0.804093567251462 Test acc:0.6048387096774194 Train loss:0.00863774586468935\n",
      "Epoch:735 Train acc:0.8021442495126706 Test acc:0.6094470046082949 Train loss:0.008654349483549595\n",
      "Epoch:736 Train acc:0.800682261208577 Test acc:0.5806451612903226 Train loss:0.008439704775810242\n",
      "Epoch:737 Train acc:0.7626705653021443 Test acc:0.6082949308755761 Train loss:0.009616571478545666\n",
      "Epoch:738 Train acc:0.7987329434697856 Test acc:0.6082949308755761 Train loss:0.008396727964282036\n",
      "Epoch:739 Train acc:0.8157894736842105 Test acc:0.6117511520737328 Train loss:0.008496420457959175\n",
      "Epoch:740 Train acc:0.7982456140350878 Test acc:0.6071428571428571 Train loss:0.009084716439247131\n",
      "Epoch:741 Train acc:0.783625730994152 Test acc:0.6163594470046083 Train loss:0.009252050891518593\n",
      "Epoch:742 Train acc:0.8060428849902534 Test acc:0.6555299539170507 Train loss:0.008455594070255756\n",
      "Epoch:743 Train acc:0.8114035087719298 Test acc:0.5956221198156681 Train loss:0.008207214996218681\n",
      "Epoch:744 Train acc:0.8094541910331384 Test acc:0.6336405529953917 Train loss:0.008770554326474667\n",
      "Epoch:745 Train acc:0.7733918128654971 Test acc:0.6025345622119815 Train loss:0.009021319448947906\n",
      "Epoch:746 Train acc:0.8201754385964912 Test acc:0.6082949308755761 Train loss:0.008145352825522423\n",
      "Epoch:747 Train acc:0.8021442495126706 Test acc:0.5898617511520737 Train loss:0.008592342957854271\n",
      "Epoch:748 Train acc:0.804093567251462 Test acc:0.5944700460829493 Train loss:0.008418158628046513\n",
      "Epoch:749 Train acc:0.8133528265107213 Test acc:0.5956221198156681 Train loss:0.008181284181773663\n",
      "Epoch:750 Train acc:0.8148148148148148 Test acc:0.597926267281106 Train loss:0.008128641173243523\n",
      "Epoch:751 Train acc:0.8201754385964912 Test acc:0.6290322580645161 Train loss:0.007868943735957146\n",
      "Epoch:752 Train acc:0.8153021442495126 Test acc:0.5875576036866359 Train loss:0.008357241749763489\n",
      "Epoch:753 Train acc:0.8153021442495126 Test acc:0.6117511520737328 Train loss:0.008200275711715221\n",
      "Epoch:754 Train acc:0.8221247563352827 Test acc:0.618663594470046 Train loss:0.008061018772423267\n",
      "Epoch:755 Train acc:0.8157894736842105 Test acc:0.5956221198156681 Train loss:0.00820646621286869\n",
      "Epoch:756 Train acc:0.8192007797270955 Test acc:0.6059907834101382 Train loss:0.008136278949677944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:757 Train acc:0.8216374269005848 Test acc:0.5852534562211982 Train loss:0.00782538577914238\n",
      "Epoch:758 Train acc:0.8162768031189084 Test acc:0.5806451612903226 Train loss:0.008114311844110489\n",
      "Epoch:759 Train acc:0.8148148148148148 Test acc:0.6036866359447005 Train loss:0.008120150305330753\n",
      "Epoch:760 Train acc:0.8216374269005848 Test acc:0.6048387096774194 Train loss:0.007944107055664062\n",
      "Epoch:761 Train acc:0.8148148148148148 Test acc:0.5967741935483871 Train loss:0.007787667214870453\n",
      "Epoch:762 Train acc:0.8089668615984406 Test acc:0.6071428571428571 Train loss:0.008083398453891277\n",
      "Epoch:763 Train acc:0.7412280701754386 Test acc:0.5841013824884793 Train loss:0.011201050132513046\n",
      "Epoch:764 Train acc:0.7914230019493177 Test acc:0.6255760368663594 Train loss:0.009202494286000729\n",
      "Epoch:765 Train acc:0.7987329434697856 Test acc:0.6232718894009217 Train loss:0.008525479584932327\n",
      "Epoch:766 Train acc:0.8206627680311891 Test acc:0.5921658986175116 Train loss:0.007932876236736774\n",
      "Epoch:767 Train acc:0.8196881091617934 Test acc:0.6082949308755761 Train loss:0.007851961068809032\n",
      "Epoch:768 Train acc:0.8196881091617934 Test acc:0.5921658986175116 Train loss:0.00792276207357645\n",
      "Epoch:769 Train acc:0.8162768031189084 Test acc:0.5921658986175116 Train loss:0.007814733311533928\n",
      "Epoch:770 Train acc:0.8221247563352827 Test acc:0.5967741935483871 Train loss:0.007828916423022747\n",
      "Epoch:771 Train acc:0.8304093567251462 Test acc:0.5910138248847926 Train loss:0.0076966676861047745\n",
      "Epoch:772 Train acc:0.8196881091617934 Test acc:0.652073732718894 Train loss:0.00798430759459734\n",
      "Epoch:773 Train acc:0.8206627680311891 Test acc:0.6278801843317973 Train loss:0.007656924892216921\n",
      "Epoch:774 Train acc:0.8157894736842105 Test acc:0.618663594470046 Train loss:0.007882488891482353\n",
      "Epoch:775 Train acc:0.8230994152046783 Test acc:0.586405529953917 Train loss:0.007955745793879032\n",
      "Epoch:776 Train acc:0.8216374269005848 Test acc:0.5817972350230415 Train loss:0.007648947648704052\n",
      "Epoch:777 Train acc:0.8138401559454191 Test acc:0.5748847926267281 Train loss:0.008335663937032223\n",
      "Epoch:778 Train acc:0.8148148148148148 Test acc:0.5967741935483871 Train loss:0.007896353490650654\n",
      "Epoch:779 Train acc:0.8216374269005848 Test acc:0.5990783410138248 Train loss:0.007917515933513641\n",
      "Epoch:780 Train acc:0.8216374269005848 Test acc:0.5841013824884793 Train loss:0.0079320864751935\n",
      "Epoch:781 Train acc:0.8206627680311891 Test acc:0.5829493087557603 Train loss:0.00810249987989664\n",
      "Epoch:782 Train acc:0.7811890838206628 Test acc:0.5887096774193549 Train loss:0.009071356616914272\n",
      "Epoch:783 Train acc:0.8226120857699805 Test acc:0.6163594470046083 Train loss:0.007968555204570293\n",
      "Epoch:784 Train acc:0.8099415204678363 Test acc:0.5852534562211982 Train loss:0.008343259803950787\n",
      "Epoch:785 Train acc:0.7846003898635477 Test acc:0.618663594470046 Train loss:0.009640447795391083\n",
      "Epoch:786 Train acc:0.8138401559454191 Test acc:0.5783410138248848 Train loss:0.00829359795898199\n",
      "Epoch:787 Train acc:0.8245614035087719 Test acc:0.6129032258064516 Train loss:0.00787528045475483\n",
      "Epoch:788 Train acc:0.8235867446393762 Test acc:0.5841013824884793 Train loss:0.00772860087454319\n",
      "Epoch:789 Train acc:0.8260233918128655 Test acc:0.5771889400921659 Train loss:0.007618570700287819\n",
      "Epoch:790 Train acc:0.8196881091617934 Test acc:0.6048387096774194 Train loss:0.007787272334098816\n",
      "Epoch:791 Train acc:0.8118908382066277 Test acc:0.6071428571428571 Train loss:0.00830492377281189\n",
      "Epoch:792 Train acc:0.8323586744639376 Test acc:0.5783410138248848 Train loss:0.007613387890160084\n",
      "Epoch:793 Train acc:0.8245614035087719 Test acc:0.5852534562211982 Train loss:0.007740173023194075\n",
      "Epoch:794 Train acc:0.8260233918128655 Test acc:0.6278801843317973 Train loss:0.007460669614374638\n",
      "Epoch:795 Train acc:0.8026315789473685 Test acc:0.5817972350230415 Train loss:0.00836036168038845\n",
      "Epoch:796 Train acc:0.7948343079922028 Test acc:0.5910138248847926 Train loss:0.00840346235781908\n",
      "Epoch:797 Train acc:0.8338206627680312 Test acc:0.6082949308755761 Train loss:0.007646069396287203\n",
      "Epoch:798 Train acc:0.8279727095516569 Test acc:0.6405529953917051 Train loss:0.007840961217880249\n",
      "Epoch:799 Train acc:0.8133528265107213 Test acc:0.6278801843317973 Train loss:0.007925695739686489\n",
      "Epoch:800 Train acc:0.8357699805068226 Test acc:0.5714285714285714 Train loss:0.007481919601559639\n",
      "Epoch:801 Train acc:0.8284600389863548 Test acc:0.5967741935483871 Train loss:0.007668650709092617\n",
      "Epoch:802 Train acc:0.8221247563352827 Test acc:0.5990783410138248 Train loss:0.0076935808174312115\n",
      "Epoch:803 Train acc:0.8367446393762183 Test acc:0.5714285714285714 Train loss:0.007343763951212168\n",
      "Epoch:804 Train acc:0.8328460038986355 Test acc:0.597926267281106 Train loss:0.0075812796130776405\n",
      "Epoch:805 Train acc:0.8318713450292398 Test acc:0.6175115207373272 Train loss:0.007665869314223528\n",
      "Epoch:806 Train acc:0.8304093567251462 Test acc:0.5956221198156681 Train loss:0.00750730512663722\n",
      "Epoch:807 Train acc:0.8304093567251462 Test acc:0.6048387096774194 Train loss:0.008020193316042423\n",
      "Epoch:808 Train acc:0.8079922027290448 Test acc:0.6152073732718893 Train loss:0.008105691522359848\n",
      "Epoch:809 Train acc:0.8333333333333334 Test acc:0.5944700460829493 Train loss:0.0071863168850541115\n",
      "Epoch:810 Train acc:0.8372319688109162 Test acc:0.586405529953917 Train loss:0.007537203375250101\n",
      "Epoch:811 Train acc:0.8328460038986355 Test acc:0.5921658986175116 Train loss:0.007189100608229637\n",
      "Epoch:812 Train acc:0.8182261208576999 Test acc:0.5806451612903226 Train loss:0.009006536565721035\n",
      "Epoch:813 Train acc:0.6812865497076024 Test acc:0.5506912442396313 Train loss:0.011374812573194504\n",
      "Epoch:814 Train acc:0.6885964912280702 Test acc:0.630184331797235 Train loss:0.010889263823628426\n",
      "Epoch:815 Train acc:0.6978557504873294 Test acc:0.6336405529953917 Train loss:0.010627198033034801\n",
      "Epoch:816 Train acc:0.6861598440545809 Test acc:0.5829493087557603 Train loss:0.010723352432250977\n",
      "Epoch:817 Train acc:0.7183235867446394 Test acc:0.5887096774193549 Train loss:0.010626054368913174\n",
      "Epoch:818 Train acc:0.7246588693957114 Test acc:0.6463133640552995 Train loss:0.010273340158164501\n",
      "Epoch:819 Train acc:0.7076023391812866 Test acc:0.618663594470046 Train loss:0.010482674464583397\n",
      "Epoch:820 Train acc:0.7412280701754386 Test acc:0.631336405529954 Train loss:0.009947038255631924\n",
      "Epoch:821 Train acc:0.73635477582846 Test acc:0.6555299539170507 Train loss:0.010277987457811832\n",
      "Epoch:822 Train acc:0.699317738791423 Test acc:0.6866359447004609 Train loss:0.010601794347167015\n",
      "Epoch:823 Train acc:0.7461013645224172 Test acc:0.6578341013824884 Train loss:0.009890098124742508\n",
      "Epoch:824 Train acc:0.7490253411306043 Test acc:0.6221198156682027 Train loss:0.009558111429214478\n",
      "Epoch:825 Train acc:0.7592592592592593 Test acc:0.6497695852534562 Train loss:0.009715120308101177\n",
      "Epoch:826 Train acc:0.7451267056530214 Test acc:0.6370967741935484 Train loss:0.009826630353927612\n",
      "Epoch:827 Train acc:0.7426900584795322 Test acc:0.6336405529953917 Train loss:0.010062790475785732\n",
      "Epoch:828 Train acc:0.7504873294346979 Test acc:0.6440092165898618 Train loss:0.009656917303800583\n",
      "Epoch:829 Train acc:0.7597465886939572 Test acc:0.6532258064516129 Train loss:0.009681357070803642\n",
      "Epoch:830 Train acc:0.7621832358674464 Test acc:0.6428571428571429 Train loss:0.009340252727270126\n",
      "Epoch:831 Train acc:0.7889863547758285 Test acc:0.631336405529954 Train loss:0.008754007518291473\n",
      "Epoch:832 Train acc:0.7928849902534113 Test acc:0.6059907834101382 Train loss:0.008640225976705551\n",
      "Epoch:833 Train acc:0.8084795321637427 Test acc:0.5887096774193549 Train loss:0.008827636018395424\n",
      "Epoch:834 Train acc:0.8001949317738791 Test acc:0.6048387096774194 Train loss:0.00852261669933796\n",
      "Epoch:835 Train acc:0.8118908382066277 Test acc:0.6175115207373272 Train loss:0.008284099400043488\n",
      "Epoch:836 Train acc:0.8235867446393762 Test acc:0.6002304147465438 Train loss:0.007766511291265488\n",
      "Epoch:837 Train acc:0.814327485380117 Test acc:0.6013824884792627 Train loss:0.007623595185577869\n",
      "Epoch:838 Train acc:0.8318713450292398 Test acc:0.5967741935483871 Train loss:0.007372166030108929\n",
      "Epoch:839 Train acc:0.8265107212475633 Test acc:0.5771889400921659 Train loss:0.007573001552373171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:840 Train acc:0.8333333333333334 Test acc:0.5817972350230415 Train loss:0.007601766847074032\n",
      "Epoch:841 Train acc:0.8128654970760234 Test acc:0.5898617511520737 Train loss:0.008083470165729523\n",
      "Epoch:842 Train acc:0.8172514619883041 Test acc:0.5645161290322581 Train loss:0.008553844876587391\n",
      "Epoch:843 Train acc:0.7923976608187134 Test acc:0.5967741935483871 Train loss:0.008625512011349201\n",
      "Epoch:844 Train acc:0.8352826510721247 Test acc:0.6059907834101382 Train loss:0.007452386897057295\n",
      "Epoch:845 Train acc:0.8357699805068226 Test acc:0.5875576036866359 Train loss:0.007301968522369862\n",
      "Epoch:846 Train acc:0.8391812865497076 Test acc:0.5956221198156681 Train loss:0.007441292982548475\n",
      "Epoch:847 Train acc:0.8289473684210527 Test acc:0.5956221198156681 Train loss:0.008005109615623951\n",
      "Epoch:848 Train acc:0.8172514619883041 Test acc:0.5875576036866359 Train loss:0.007730839308351278\n",
      "Epoch:849 Train acc:0.8372319688109162 Test acc:0.5967741935483871 Train loss:0.0077124545350670815\n",
      "Epoch:850 Train acc:0.8206627680311891 Test acc:0.597926267281106 Train loss:0.008027837611734867\n",
      "Epoch:851 Train acc:0.8386939571150097 Test acc:0.5852534562211982 Train loss:0.007574256509542465\n",
      "Epoch:852 Train acc:0.8377192982456141 Test acc:0.5737327188940092 Train loss:0.007157135754823685\n",
      "Epoch:853 Train acc:0.8382066276803118 Test acc:0.5817972350230415 Train loss:0.007364570628851652\n",
      "Epoch:854 Train acc:0.8406432748538012 Test acc:0.5748847926267281 Train loss:0.007063377648591995\n",
      "Epoch:855 Train acc:0.8416179337231969 Test acc:0.5875576036866359 Train loss:0.006867104209959507\n",
      "Epoch:856 Train acc:0.8396686159844055 Test acc:0.5817972350230415 Train loss:0.007110040634870529\n",
      "Epoch:857 Train acc:0.8464912280701754 Test acc:0.5829493087557603 Train loss:0.006950834766030312\n",
      "Epoch:858 Train acc:0.8347953216374269 Test acc:0.5944700460829493 Train loss:0.0073246038518846035\n",
      "Epoch:859 Train acc:0.8440545808966862 Test acc:0.5910138248847926 Train loss:0.006792608182877302\n",
      "Epoch:860 Train acc:0.844541910331384 Test acc:0.5990783410138248 Train loss:0.007025804370641708\n",
      "Epoch:861 Train acc:0.8450292397660819 Test acc:0.6048387096774194 Train loss:0.007301617879420519\n",
      "Epoch:862 Train acc:0.8406432748538012 Test acc:0.5933179723502304 Train loss:0.0069240424782037735\n",
      "Epoch:863 Train acc:0.8318713450292398 Test acc:0.597926267281106 Train loss:0.007190059870481491\n",
      "Epoch:864 Train acc:0.7958089668615984 Test acc:0.5921658986175116 Train loss:0.009107211604714394\n",
      "Epoch:865 Train acc:0.8250487329434698 Test acc:0.5633640552995391 Train loss:0.007316699251532555\n",
      "Epoch:866 Train acc:0.8421052631578947 Test acc:0.5841013824884793 Train loss:0.007118931971490383\n",
      "Epoch:867 Train acc:0.8382066276803118 Test acc:0.6082949308755761 Train loss:0.007471015211194754\n",
      "Epoch:868 Train acc:0.8494152046783626 Test acc:0.6117511520737328 Train loss:0.007201373111456633\n",
      "Epoch:869 Train acc:0.827485380116959 Test acc:0.5771889400921659 Train loss:0.0073685781098902225\n",
      "Epoch:870 Train acc:0.8455165692007798 Test acc:0.6013824884792627 Train loss:0.006785225588828325\n",
      "Epoch:871 Train acc:0.8425925925925926 Test acc:0.5933179723502304 Train loss:0.0068120649084448814\n",
      "Epoch:872 Train acc:0.8523391812865497 Test acc:0.5829493087557603 Train loss:0.006555086933076382\n",
      "Epoch:873 Train acc:0.8503898635477583 Test acc:0.586405529953917 Train loss:0.006779362913221121\n",
      "Epoch:874 Train acc:0.8333333333333334 Test acc:0.5714285714285714 Train loss:0.007158767431974411\n",
      "Epoch:875 Train acc:0.8187134502923976 Test acc:0.597926267281106 Train loss:0.008324502035975456\n",
      "Epoch:876 Train acc:0.8104288499025342 Test acc:0.5967741935483871 Train loss:0.008663574233651161\n",
      "Epoch:877 Train acc:0.8377192982456141 Test acc:0.5714285714285714 Train loss:0.007269477937370539\n",
      "Epoch:878 Train acc:0.834307992202729 Test acc:0.5967741935483871 Train loss:0.007040943950414658\n",
      "Epoch:879 Train acc:0.851364522417154 Test acc:0.6071428571428571 Train loss:0.006796509027481079\n",
      "Epoch:880 Train acc:0.8279727095516569 Test acc:0.5898617511520737 Train loss:0.007450349163264036\n",
      "Epoch:881 Train acc:0.8557504873294347 Test acc:0.6059907834101382 Train loss:0.006881319917738438\n",
      "Epoch:882 Train acc:0.8196881091617934 Test acc:0.5898617511520737 Train loss:0.007977362722158432\n",
      "Epoch:883 Train acc:0.8455165692007798 Test acc:0.6002304147465438 Train loss:0.006837452296167612\n",
      "Epoch:884 Train acc:0.8484405458089669 Test acc:0.6163594470046083 Train loss:0.006613258272409439\n",
      "Epoch:885 Train acc:0.8474658869395711 Test acc:0.5794930875576036 Train loss:0.006668807007372379\n",
      "Epoch:886 Train acc:0.8508771929824561 Test acc:0.5887096774193549 Train loss:0.006561819929629564\n",
      "Epoch:887 Train acc:0.8518518518518519 Test acc:0.5472350230414746 Train loss:0.006592914927750826\n",
      "Epoch:888 Train acc:0.8084795321637427 Test acc:0.5725806451612904 Train loss:0.008092084899544716\n",
      "Epoch:889 Train acc:0.8435672514619883 Test acc:0.5898617511520737 Train loss:0.006802275776863098\n",
      "Epoch:890 Train acc:0.8245614035087719 Test acc:0.6048387096774194 Train loss:0.007541842758655548\n",
      "Epoch:891 Train acc:0.8328460038986355 Test acc:0.5852534562211982 Train loss:0.0072220382280647755\n",
      "Epoch:892 Train acc:0.8474658869395711 Test acc:0.5771889400921659 Train loss:0.006788363680243492\n",
      "Epoch:893 Train acc:0.8542884990253411 Test acc:0.5794930875576036 Train loss:0.006376967299729586\n",
      "Epoch:894 Train acc:0.8596491228070176 Test acc:0.6036866359447005 Train loss:0.006354027893394232\n",
      "Epoch:895 Train acc:0.8503898635477583 Test acc:0.5990783410138248 Train loss:0.006398653611540794\n",
      "Epoch:896 Train acc:0.8508771929824561 Test acc:0.5829493087557603 Train loss:0.00643646577373147\n",
      "Epoch:897 Train acc:0.8528265107212476 Test acc:0.5852534562211982 Train loss:0.006895896978676319\n",
      "Epoch:898 Train acc:0.8576998050682261 Test acc:0.5829493087557603 Train loss:0.006501139607280493\n",
      "Epoch:899 Train acc:0.8615984405458089 Test acc:0.5875576036866359 Train loss:0.006448453292250633\n",
      "Epoch:900 Train acc:0.8528265107212476 Test acc:0.5783410138248848 Train loss:0.006721098441630602\n",
      "Epoch:901 Train acc:0.8318713450292398 Test acc:0.5725806451612904 Train loss:0.0072478619404137135\n",
      "Epoch:902 Train acc:0.8489278752436648 Test acc:0.6152073732718893 Train loss:0.006512985564768314\n",
      "Epoch:903 Train acc:0.8523391812865497 Test acc:0.5956221198156681 Train loss:0.00662222970277071\n",
      "Epoch:904 Train acc:0.8226120857699805 Test acc:0.6048387096774194 Train loss:0.007524659391492605\n",
      "Epoch:905 Train acc:0.8572124756335283 Test acc:0.6013824884792627 Train loss:0.006452228873968124\n",
      "Epoch:906 Train acc:0.8562378167641326 Test acc:0.5783410138248848 Train loss:0.0064080809243023396\n",
      "Epoch:907 Train acc:0.8645224171539961 Test acc:0.5668202764976958 Train loss:0.006357734557241201\n",
      "Epoch:908 Train acc:0.854775828460039 Test acc:0.5783410138248848 Train loss:0.0065343137830495834\n",
      "Epoch:909 Train acc:0.8494152046783626 Test acc:0.5576036866359447 Train loss:0.006758199073374271\n",
      "Epoch:910 Train acc:0.854775828460039 Test acc:0.5771889400921659 Train loss:0.006651442963629961\n",
      "Epoch:911 Train acc:0.8557504873294347 Test acc:0.5829493087557603 Train loss:0.006504578050225973\n",
      "Epoch:912 Train acc:0.8572124756335283 Test acc:0.5898617511520737 Train loss:0.006379807833582163\n",
      "Epoch:913 Train acc:0.8620857699805068 Test acc:0.5679723502304147 Train loss:0.00630059652030468\n",
      "Epoch:914 Train acc:0.8523391812865497 Test acc:0.6002304147465438 Train loss:0.006474520079791546\n",
      "Epoch:915 Train acc:0.8552631578947368 Test acc:0.586405529953917 Train loss:0.0063477386720478535\n",
      "Epoch:916 Train acc:0.854775828460039 Test acc:0.5725806451612904 Train loss:0.006912031210958958\n",
      "Epoch:917 Train acc:0.8567251461988304 Test acc:0.5691244239631337 Train loss:0.006330557633191347\n",
      "Epoch:918 Train acc:0.8669590643274854 Test acc:0.5714285714285714 Train loss:0.006128579378128052\n",
      "Epoch:919 Train acc:0.8703703703703703 Test acc:0.565668202764977 Train loss:0.006199336610734463\n",
      "Epoch:920 Train acc:0.868421052631579 Test acc:0.5990783410138248 Train loss:0.006530457641929388\n",
      "Epoch:921 Train acc:0.8391812865497076 Test acc:0.5956221198156681 Train loss:0.006777535192668438\n",
      "Epoch:922 Train acc:0.8591617933723197 Test acc:0.5841013824884793 Train loss:0.006412086542695761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:923 Train acc:0.8494152046783626 Test acc:0.5956221198156681 Train loss:0.006937935948371887\n",
      "Epoch:924 Train acc:0.854775828460039 Test acc:0.6025345622119815 Train loss:0.006775742396712303\n",
      "Epoch:925 Train acc:0.8664717348927875 Test acc:0.586405529953917 Train loss:0.006096929777413607\n",
      "Epoch:926 Train acc:0.8640350877192983 Test acc:0.5668202764976958 Train loss:0.006066667381674051\n",
      "Epoch:927 Train acc:0.8645224171539961 Test acc:0.5622119815668203 Train loss:0.005918975453823805\n",
      "Epoch:928 Train acc:0.8645224171539961 Test acc:0.5875576036866359 Train loss:0.006098421290516853\n",
      "Epoch:929 Train acc:0.8713450292397661 Test acc:0.554147465437788 Train loss:0.006002767942845821\n",
      "Epoch:930 Train acc:0.8664717348927875 Test acc:0.5737327188940092 Train loss:0.006147441919893026\n",
      "Epoch:931 Train acc:0.8723196881091618 Test acc:0.5725806451612904 Train loss:0.006156805902719498\n",
      "Epoch:932 Train acc:0.8664717348927875 Test acc:0.576036866359447 Train loss:0.006098165176808834\n",
      "Epoch:933 Train acc:0.8645224171539961 Test acc:0.5841013824884793 Train loss:0.006330357398837805\n",
      "Epoch:934 Train acc:0.8689083820662769 Test acc:0.5587557603686636 Train loss:0.006300884764641523\n",
      "Epoch:935 Train acc:0.8435672514619883 Test acc:0.5253456221198156 Train loss:0.007004586048424244\n",
      "Epoch:936 Train acc:0.8698830409356725 Test acc:0.6002304147465438 Train loss:0.006176742725074291\n",
      "Epoch:937 Train acc:0.8625730994152047 Test acc:0.5702764976958525 Train loss:0.0061270566657185555\n",
      "Epoch:938 Train acc:0.8625730994152047 Test acc:0.5633640552995391 Train loss:0.006090478040277958\n",
      "Epoch:939 Train acc:0.8552631578947368 Test acc:0.5990783410138248 Train loss:0.006650153081864119\n",
      "Epoch:940 Train acc:0.8630604288499025 Test acc:0.5495391705069125 Train loss:0.006332650780677795\n",
      "Epoch:941 Train acc:0.8645224171539961 Test acc:0.5299539170506913 Train loss:0.00605918699875474\n",
      "Epoch:942 Train acc:0.8713450292397661 Test acc:0.6117511520737328 Train loss:0.005842698272317648\n",
      "Epoch:943 Train acc:0.8562378167641326 Test acc:0.576036866359447 Train loss:0.00640692887827754\n",
      "Epoch:944 Train acc:0.851364522417154 Test acc:0.5771889400921659 Train loss:0.0065767536871135235\n",
      "Epoch:945 Train acc:0.8762183235867447 Test acc:0.5933179723502304 Train loss:0.006019585765898228\n",
      "Epoch:946 Train acc:0.8742690058479532 Test acc:0.5898617511520737 Train loss:0.006001367699354887\n",
      "Epoch:947 Train acc:0.8645224171539961 Test acc:0.5898617511520737 Train loss:0.006046242546290159\n",
      "Epoch:948 Train acc:0.8659844054580896 Test acc:0.6002304147465438 Train loss:0.006059020292013884\n",
      "Epoch:949 Train acc:0.8801169590643275 Test acc:0.5622119815668203 Train loss:0.005592539440840483\n",
      "Epoch:950 Train acc:0.8747563352826511 Test acc:0.6048387096774194 Train loss:0.005687362980097532\n",
      "Epoch:951 Train acc:0.8742690058479532 Test acc:0.586405529953917 Train loss:0.005660437047481537\n",
      "Epoch:952 Train acc:0.8796296296296297 Test acc:0.5633640552995391 Train loss:0.005702665541321039\n",
      "Epoch:953 Train acc:0.8801169590643275 Test acc:0.5610599078341014 Train loss:0.005859187338501215\n",
      "Epoch:954 Train acc:0.834307992202729 Test acc:0.5668202764976958 Train loss:0.007293237838894129\n",
      "Epoch:955 Train acc:0.8572124756335283 Test acc:0.576036866359447 Train loss:0.006241586524993181\n",
      "Epoch:956 Train acc:0.8786549707602339 Test acc:0.5921658986175116 Train loss:0.005670664366334677\n",
      "Epoch:957 Train acc:0.8601364522417154 Test acc:0.5794930875576036 Train loss:0.006165544502437115\n",
      "Epoch:958 Train acc:0.8752436647173489 Test acc:0.5817972350230415 Train loss:0.00577592384070158\n",
      "Epoch:959 Train acc:0.8801169590643275 Test acc:0.5737327188940092 Train loss:0.0055722263641655445\n",
      "Epoch:960 Train acc:0.8825536062378168 Test acc:0.5679723502304147 Train loss:0.0054632555693387985\n",
      "Epoch:961 Train acc:0.8810916179337231 Test acc:0.5633640552995391 Train loss:0.005530342925339937\n",
      "Epoch:962 Train acc:0.8708576998050682 Test acc:0.576036866359447 Train loss:0.005509002134203911\n",
      "Epoch:963 Train acc:0.8810916179337231 Test acc:0.5702764976958525 Train loss:0.0055811405181884766\n",
      "Epoch:964 Train acc:0.8708576998050682 Test acc:0.5587557603686636 Train loss:0.0059375520795583725\n",
      "Epoch:965 Train acc:0.8693957115009746 Test acc:0.5944700460829493 Train loss:0.005941169336438179\n",
      "Epoch:966 Train acc:0.8854775828460039 Test acc:0.5737327188940092 Train loss:0.005736234597861767\n",
      "Epoch:967 Train acc:0.8260233918128655 Test acc:0.5645161290322581 Train loss:0.007843759842216969\n",
      "Epoch:968 Train acc:0.8187134502923976 Test acc:0.5576036866359447 Train loss:0.008060213178396225\n",
      "Epoch:969 Train acc:0.8689083820662769 Test acc:0.5483870967741935 Train loss:0.005966463126242161\n",
      "Epoch:970 Train acc:0.878167641325536 Test acc:0.597926267281106 Train loss:0.00564869400113821\n",
      "Epoch:971 Train acc:0.8747563352826511 Test acc:0.5702764976958525 Train loss:0.005567390471696854\n",
      "Epoch:972 Train acc:0.8698830409356725 Test acc:0.5668202764976958 Train loss:0.005634268280118704\n",
      "Epoch:973 Train acc:0.8776803118908382 Test acc:0.5668202764976958 Train loss:0.00559251056984067\n",
      "Epoch:974 Train acc:0.8869395711500975 Test acc:0.5691244239631337 Train loss:0.005526857916265726\n",
      "Epoch:975 Train acc:0.8810916179337231 Test acc:0.5702764976958525 Train loss:0.005786924622952938\n",
      "Epoch:976 Train acc:0.8762183235867447 Test acc:0.5771889400921659 Train loss:0.005581941921263933\n",
      "Epoch:977 Train acc:0.8806042884990254 Test acc:0.5725806451612904 Train loss:0.005358031950891018\n",
      "Epoch:978 Train acc:0.8752436647173489 Test acc:0.5668202764976958 Train loss:0.0061443704180419445\n",
      "Epoch:979 Train acc:0.8845029239766082 Test acc:0.597926267281106 Train loss:0.005335197318345308\n",
      "Epoch:980 Train acc:0.8806042884990254 Test acc:0.5806451612903226 Train loss:0.005644756834954023\n",
      "Epoch:981 Train acc:0.8435672514619883 Test acc:0.5771889400921659 Train loss:0.007555336691439152\n",
      "Epoch:982 Train acc:0.8411306042884991 Test acc:0.5841013824884793 Train loss:0.007287796586751938\n",
      "Epoch:983 Train acc:0.8732943469785575 Test acc:0.6013824884792627 Train loss:0.005650445353239775\n",
      "Epoch:984 Train acc:0.8615984405458089 Test acc:0.5472350230414746 Train loss:0.006051728501915932\n",
      "Epoch:985 Train acc:0.8898635477582846 Test acc:0.5691244239631337 Train loss:0.005482720676809549\n",
      "Epoch:986 Train acc:0.8874269005847953 Test acc:0.5576036866359447 Train loss:0.005286426283419132\n",
      "Epoch:987 Train acc:0.871832358674464 Test acc:0.5495391705069125 Train loss:0.005686728283762932\n",
      "Epoch:988 Train acc:0.8776803118908382 Test acc:0.5622119815668203 Train loss:0.00621597608551383\n",
      "Epoch:989 Train acc:0.8664717348927875 Test acc:0.5921658986175116 Train loss:0.006261944770812988\n",
      "Epoch:990 Train acc:0.8810916179337231 Test acc:0.5794930875576036 Train loss:0.005325386766344309\n",
      "Epoch:991 Train acc:0.8903508771929824 Test acc:0.6025345622119815 Train loss:0.005130774341523647\n",
      "Epoch:992 Train acc:0.8927875243664717 Test acc:0.5933179723502304 Train loss:0.004999707452952862\n",
      "Epoch:993 Train acc:0.8806042884990254 Test acc:0.6129032258064516 Train loss:0.005636685527861118\n",
      "Epoch:994 Train acc:0.881578947368421 Test acc:0.6163594470046083 Train loss:0.005260358564555645\n",
      "Epoch:995 Train acc:0.8869395711500975 Test acc:0.6071428571428571 Train loss:0.005181449931114912\n",
      "Epoch:996 Train acc:0.8864522417153996 Test acc:0.5460829493087558 Train loss:0.005216711200773716\n",
      "Epoch:997 Train acc:0.8903508771929824 Test acc:0.576036866359447 Train loss:0.0053221494890749454\n",
      "Epoch:998 Train acc:0.8996101364522417 Test acc:0.5967741935483871 Train loss:0.005032758694142103\n",
      "Epoch:999 Train acc:0.9005847953216374 Test acc:0.5852534562211982 Train loss:0.005197970662266016\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba445361c2a4782a05315a25f313586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_test_accuracy</td><td>▁▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████████</td></tr><tr><td>test_accuracy</td><td>▁▄▄▆▇▇▇▆█▅█▆▆▇▅▅▆▅▅▅▅▄▆▄▅▅▇▆▆▅▆▆▇▄▅▅▄▄▄▆</td></tr><tr><td>train_accuracy</td><td>▁▁▂▃▃▃▃▃▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▇▆▄▇▇▇█▇▇█</td></tr><tr><td>train_loss</td><td>██▇▇▇▇▇▇▇▆▆▆▇▆▆▆▇▆▅▆▅▅▅▄▅▄▅▄▄▃▃▃▅▃▂▂▂▂▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_test_accuracy</td><td>0.68664</td></tr><tr><td>test_accuracy</td><td>0.58525</td></tr><tr><td>train_accuracy</td><td>0.90058</td></tr><tr><td>train_loss</td><td>0.0052</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">self_loops=True_weighted=weighted_data=power_and_entropy</strong>: <a href=\"https://wandb.ai/dmasny/neuroimaging_gnn_eeg_final_project/runs/31x2xb1h\" target=\"_blank\">https://wandb.ai/dmasny/neuroimaging_gnn_eeg_final_project/runs/31x2xb1h</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221027_200520-31x2xb1h/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_loops=True_weighted=weighted_data=power_and_entropy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/notebooks/gnn_eeg/wandb/run-20221027_202041-214306gt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/dmasny/neuroimaging_gnn_eeg_final_project/runs/214306gt\" target=\"_blank\">self_loops=True_weighted=weighted_data=only_entropy</a></strong> to <a href=\"https://wandb.ai/dmasny/neuroimaging_gnn_eeg_final_project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Train acc:0.49853801169590645 Test acc:0.5011520737327189 Train loss:0.015604461543262005\n",
      "Epoch:1 Train acc:0.4926900584795322 Test acc:0.4988479262672811 Train loss:0.014292693696916103\n",
      "Epoch:2 Train acc:0.5214424951267057 Test acc:0.4965437788018433 Train loss:0.014207515865564346\n",
      "Epoch:3 Train acc:0.5146198830409356 Test acc:0.4988479262672811 Train loss:0.014209109358489513\n",
      "Epoch:4 Train acc:0.5380116959064327 Test acc:0.532258064516129 Train loss:0.014170452952384949\n",
      "Epoch:5 Train acc:0.554093567251462 Test acc:0.5011520737327189 Train loss:0.013952169567346573\n",
      "Epoch:6 Train acc:0.5019493177387915 Test acc:0.5 Train loss:0.014461575075984001\n",
      "Epoch:7 Train acc:0.5131578947368421 Test acc:0.5748847926267281 Train loss:0.014164166525006294\n",
      "Epoch:8 Train acc:0.5545808966861598 Test acc:0.5357142857142857 Train loss:0.01402237918227911\n",
      "Epoch:9 Train acc:0.5818713450292398 Test acc:0.5103686635944701 Train loss:0.01386258751153946\n",
      "Epoch:10 Train acc:0.5277777777777778 Test acc:0.4988479262672811 Train loss:0.014222204685211182\n",
      "Epoch:11 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01420979667454958\n",
      "Epoch:12 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014199219644069672\n",
      "Epoch:13 Train acc:0.5029239766081871 Test acc:0.5011520737327189 Train loss:0.014192271046340466\n",
      "Epoch:14 Train acc:0.4951267056530214 Test acc:0.4988479262672811 Train loss:0.014202283695340157\n",
      "Epoch:15 Train acc:0.4951267056530214 Test acc:0.5011520737327189 Train loss:0.014191742055118084\n",
      "Epoch:16 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187655411660671\n",
      "Epoch:17 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014191309921443462\n",
      "Epoch:18 Train acc:0.49902534113060426 Test acc:0.4988479262672811 Train loss:0.014188864268362522\n",
      "Epoch:19 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187929220497608\n",
      "Epoch:20 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014202741906046867\n",
      "Epoch:21 Train acc:0.5058479532163743 Test acc:0.5011520737327189 Train loss:0.014187436550855637\n",
      "Epoch:22 Train acc:0.49707602339181284 Test acc:0.4988479262672811 Train loss:0.014185769483447075\n",
      "Epoch:23 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014197682961821556\n",
      "Epoch:24 Train acc:0.4922027290448343 Test acc:0.5011520737327189 Train loss:0.014187997207045555\n",
      "Epoch:25 Train acc:0.4844054580896686 Test acc:0.4988479262672811 Train loss:0.014191508293151855\n",
      "Epoch:26 Train acc:0.4980506822612086 Test acc:0.5011520737327189 Train loss:0.014188959263265133\n",
      "Epoch:27 Train acc:0.49317738791423 Test acc:0.4988479262672811 Train loss:0.014188192784786224\n",
      "Epoch:28 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014182916842401028\n",
      "Epoch:29 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014197479002177715\n",
      "Epoch:30 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014190402813255787\n",
      "Epoch:31 Train acc:0.5077972709551657 Test acc:0.5011520737327189 Train loss:0.01418964471668005\n",
      "Epoch:32 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014201832935214043\n",
      "Epoch:33 Train acc:0.50682261208577 Test acc:0.4988479262672811 Train loss:0.01418739277869463\n",
      "Epoch:34 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01419199351221323\n",
      "Epoch:35 Train acc:0.4902534113060429 Test acc:0.5011520737327189 Train loss:0.014188835397362709\n",
      "Epoch:36 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189613051712513\n",
      "Epoch:37 Train acc:0.4873294346978557 Test acc:0.4988479262672811 Train loss:0.014187649823725224\n",
      "Epoch:38 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014184724539518356\n",
      "Epoch:39 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014186717569828033\n",
      "Epoch:40 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014199900440871716\n",
      "Epoch:41 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014191768132150173\n",
      "Epoch:42 Train acc:0.5009746588693957 Test acc:0.5011520737327189 Train loss:0.014189383015036583\n",
      "Epoch:43 Train acc:0.48635477582846004 Test acc:0.4988479262672811 Train loss:0.014190864749252796\n",
      "Epoch:44 Train acc:0.5009746588693957 Test acc:0.5011520737327189 Train loss:0.01419155579060316\n",
      "Epoch:45 Train acc:0.4980506822612086 Test acc:0.4988479262672811 Train loss:0.014188943430781364\n",
      "Epoch:46 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014186074025928974\n",
      "Epoch:47 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188330620527267\n",
      "Epoch:48 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188542030751705\n",
      "Epoch:49 Train acc:0.48830409356725146 Test acc:0.4988479262672811 Train loss:0.01419041957706213\n",
      "Epoch:50 Train acc:0.5029239766081871 Test acc:0.5011520737327189 Train loss:0.014188846573233604\n",
      "Epoch:51 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418926939368248\n",
      "Epoch:52 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189706183969975\n",
      "Epoch:53 Train acc:0.49707602339181284 Test acc:0.5011520737327189 Train loss:0.014187570661306381\n",
      "Epoch:54 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014186405576765537\n",
      "Epoch:55 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014185795560479164\n",
      "Epoch:56 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014193923212587833\n",
      "Epoch:57 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189403504133224\n",
      "Epoch:58 Train acc:0.48830409356725146 Test acc:0.5011520737327189 Train loss:0.014188633300364017\n",
      "Epoch:59 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418575644493103\n",
      "Epoch:60 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014191215857863426\n",
      "Epoch:61 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188814908266068\n",
      "Epoch:62 Train acc:0.4873294346978557 Test acc:0.4988479262672811 Train loss:0.014188735745847225\n",
      "Epoch:63 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189458452165127\n",
      "Epoch:64 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188525266945362\n",
      "Epoch:65 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014186539687216282\n",
      "Epoch:66 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014193311333656311\n",
      "Epoch:67 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014186682179570198\n",
      "Epoch:68 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01419309712946415\n",
      "Epoch:69 Train acc:0.48148148148148145 Test acc:0.5011520737327189 Train loss:0.014188381843268871\n",
      "Epoch:70 Train acc:0.4922027290448343 Test acc:0.4988479262672811 Train loss:0.014187730848789215\n",
      "Epoch:71 Train acc:0.4844054580896686 Test acc:0.4988479262672811 Train loss:0.014189030043780804\n",
      "Epoch:72 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188718050718307\n",
      "Epoch:73 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188231900334358\n",
      "Epoch:74 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187971130013466\n",
      "Epoch:75 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189730398356915\n",
      "Epoch:76 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187101274728775\n",
      "Epoch:77 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014184700325131416\n",
      "Epoch:78 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014190630987286568\n",
      "Epoch:79 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01419080700725317\n",
      "Epoch:80 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189409092068672\n",
      "Epoch:81 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188647270202637\n",
      "Epoch:82 Train acc:0.4775828460038986 Test acc:0.4988479262672811 Train loss:0.014190183952450752\n",
      "Epoch:83 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189190231263638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:84 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014183861203491688\n",
      "Epoch:85 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418914832174778\n",
      "Epoch:86 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014184143394231796\n",
      "Epoch:87 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014191736467182636\n",
      "Epoch:88 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187995344400406\n",
      "Epoch:89 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188539236783981\n",
      "Epoch:90 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188427478075027\n",
      "Epoch:91 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188227243721485\n",
      "Epoch:92 Train acc:0.49415204678362573 Test acc:0.4988479262672811 Train loss:0.014188221655786037\n",
      "Epoch:93 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187270775437355\n",
      "Epoch:94 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014193395152688026\n",
      "Epoch:95 Train acc:0.5019493177387915 Test acc:0.5011520737327189 Train loss:0.014187352731823921\n",
      "Epoch:96 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188109897077084\n",
      "Epoch:97 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188237488269806\n",
      "Epoch:98 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188168570399284\n",
      "Epoch:99 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014192027971148491\n",
      "Epoch:100 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188404195010662\n",
      "Epoch:101 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01419472973793745\n",
      "Epoch:102 Train acc:0.49902534113060426 Test acc:0.4988479262672811 Train loss:0.014189109206199646\n",
      "Epoch:103 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014190467074513435\n",
      "Epoch:104 Train acc:0.4853801169590643 Test acc:0.4988479262672811 Train loss:0.01419110782444477\n",
      "Epoch:105 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189083129167557\n",
      "Epoch:106 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014190207235515118\n",
      "Epoch:107 Train acc:0.49122807017543857 Test acc:0.5011520737327189 Train loss:0.014189890585839748\n",
      "Epoch:108 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188139699399471\n",
      "Epoch:109 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188474044203758\n",
      "Epoch:110 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187540858983994\n",
      "Epoch:111 Train acc:0.4922027290448343 Test acc:0.4988479262672811 Train loss:0.014187985099852085\n",
      "Epoch:112 Train acc:0.48635477582846004 Test acc:0.4988479262672811 Train loss:0.014186745509505272\n",
      "Epoch:113 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189384877681732\n",
      "Epoch:114 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014186101965606213\n",
      "Epoch:115 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418248564004898\n",
      "Epoch:116 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014183384366333485\n",
      "Epoch:117 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014184797182679176\n",
      "Epoch:118 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014183788560330868\n",
      "Epoch:119 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014202925376594067\n",
      "Epoch:120 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014183765277266502\n",
      "Epoch:121 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014191215857863426\n",
      "Epoch:122 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01419318001717329\n",
      "Epoch:123 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187630265951157\n",
      "Epoch:124 Train acc:0.49317738791423 Test acc:0.5011520737327189 Train loss:0.01418981421738863\n",
      "Epoch:125 Train acc:0.49707602339181284 Test acc:0.4988479262672811 Train loss:0.014190146699547768\n",
      "Epoch:126 Train acc:0.5097465886939572 Test acc:0.5011520737327189 Train loss:0.014187319204211235\n",
      "Epoch:127 Train acc:0.4766081871345029 Test acc:0.5011520737327189 Train loss:0.014187474735081196\n",
      "Epoch:128 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014194362796843052\n",
      "Epoch:129 Train acc:0.4980506822612086 Test acc:0.4988479262672811 Train loss:0.01418911200016737\n",
      "Epoch:130 Train acc:0.5009746588693957 Test acc:0.5011520737327189 Train loss:0.014188201166689396\n",
      "Epoch:131 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014186694286763668\n",
      "Epoch:132 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189198613166809\n",
      "Epoch:133 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418812945485115\n",
      "Epoch:134 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189871028065681\n",
      "Epoch:135 Train acc:0.49707602339181284 Test acc:0.4988479262672811 Train loss:0.014186874963343143\n",
      "Epoch:136 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014194910414516926\n",
      "Epoch:137 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188484288752079\n",
      "Epoch:138 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418797206133604\n",
      "Epoch:139 Train acc:0.48927875243664715 Test acc:0.4988479262672811 Train loss:0.014188224449753761\n",
      "Epoch:140 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189183712005615\n",
      "Epoch:141 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188902452588081\n",
      "Epoch:142 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187751337885857\n",
      "Epoch:143 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187651686370373\n",
      "Epoch:144 Train acc:0.49415204678362573 Test acc:0.4988479262672811 Train loss:0.01418833527714014\n",
      "Epoch:145 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187817461788654\n",
      "Epoch:146 Train acc:0.4873294346978557 Test acc:0.4988479262672811 Train loss:0.014189650304615498\n",
      "Epoch:147 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014186179265379906\n",
      "Epoch:148 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418811734765768\n",
      "Epoch:149 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189550653100014\n",
      "Epoch:150 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188050292432308\n",
      "Epoch:151 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188585802912712\n",
      "Epoch:152 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189625158905983\n",
      "Epoch:153 Train acc:0.5048732943469786 Test acc:0.5011520737327189 Train loss:0.014185559935867786\n",
      "Epoch:154 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014192700386047363\n",
      "Epoch:155 Train acc:0.49610136452241715 Test acc:0.4988479262672811 Train loss:0.014187850058078766\n",
      "Epoch:156 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188304543495178\n",
      "Epoch:157 Train acc:0.49122807017543857 Test acc:0.5011520737327189 Train loss:0.014189422130584717\n",
      "Epoch:158 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187951572239399\n",
      "Epoch:159 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188414439558983\n",
      "Epoch:160 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188064262270927\n",
      "Epoch:161 Train acc:0.49317738791423 Test acc:0.5011520737327189 Train loss:0.014188394881784916\n",
      "Epoch:162 Train acc:0.48830409356725146 Test acc:0.5011520737327189 Train loss:0.014189406298100948\n",
      "Epoch:163 Train acc:0.5058479532163743 Test acc:0.4988479262672811 Train loss:0.01418722327798605\n",
      "Epoch:164 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188684523105621\n",
      "Epoch:165 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189646579325199\n",
      "Epoch:166 Train acc:0.49122807017543857 Test acc:0.5011520737327189 Train loss:0.01418749988079071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:167 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188917353749275\n",
      "Epoch:168 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187972992658615\n",
      "Epoch:169 Train acc:0.48050682261208577 Test acc:0.5011520737327189 Train loss:0.014188273809850216\n",
      "Epoch:170 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187191613018513\n",
      "Epoch:171 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188461005687714\n",
      "Epoch:172 Train acc:0.4853801169590643 Test acc:0.5011520737327189 Train loss:0.014189261943101883\n",
      "Epoch:173 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418820396065712\n",
      "Epoch:174 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189125038683414\n",
      "Epoch:175 Train acc:0.4980506822612086 Test acc:0.4988479262672811 Train loss:0.014191736467182636\n",
      "Epoch:176 Train acc:0.49610136452241715 Test acc:0.5011520737327189 Train loss:0.014191053807735443\n",
      "Epoch:177 Train acc:0.4980506822612086 Test acc:0.4988479262672811 Train loss:0.01419082935899496\n",
      "Epoch:178 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187830500304699\n",
      "Epoch:179 Train acc:0.4853801169590643 Test acc:0.4988479262672811 Train loss:0.0141914589330554\n",
      "Epoch:180 Train acc:0.4951267056530214 Test acc:0.5011520737327189 Train loss:0.014187178574502468\n",
      "Epoch:181 Train acc:0.4834307992202729 Test acc:0.5011520737327189 Train loss:0.014188275672495365\n",
      "Epoch:182 Train acc:0.47953216374269003 Test acc:0.5011520737327189 Train loss:0.014188342727720737\n",
      "Epoch:183 Train acc:0.4902534113060429 Test acc:0.4988479262672811 Train loss:0.014188287779688835\n",
      "Epoch:184 Train acc:0.4873294346978557 Test acc:0.4988479262672811 Train loss:0.014187450520694256\n",
      "Epoch:185 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014184643514454365\n",
      "Epoch:186 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014194597490131855\n",
      "Epoch:187 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189178124070168\n",
      "Epoch:188 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187922701239586\n",
      "Epoch:189 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188208617269993\n",
      "Epoch:190 Train acc:0.4834307992202729 Test acc:0.5011520737327189 Train loss:0.014189787209033966\n",
      "Epoch:191 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014185645617544651\n",
      "Epoch:192 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014194617047905922\n",
      "Epoch:193 Train acc:0.47855750487329435 Test acc:0.4988479262672811 Train loss:0.014188583940267563\n",
      "Epoch:194 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188550412654877\n",
      "Epoch:195 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418840978294611\n",
      "Epoch:196 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014186723157763481\n",
      "Epoch:197 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014190858229994774\n",
      "Epoch:198 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188328757882118\n",
      "Epoch:199 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188222587108612\n",
      "Epoch:200 Train acc:0.48927875243664715 Test acc:0.5011520737327189 Train loss:0.014188000001013279\n",
      "Epoch:201 Train acc:0.47855750487329435 Test acc:0.5011520737327189 Train loss:0.014188611879944801\n",
      "Epoch:202 Train acc:0.4980506822612086 Test acc:0.4988479262672811 Train loss:0.014187193475663662\n",
      "Epoch:203 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189271256327629\n",
      "Epoch:204 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188411645591259\n",
      "Epoch:205 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014186915010213852\n",
      "Epoch:206 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014195018447935581\n",
      "Epoch:207 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418990083038807\n",
      "Epoch:208 Train acc:0.49610136452241715 Test acc:0.5011520737327189 Train loss:0.014187576249241829\n",
      "Epoch:209 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187903143465519\n",
      "Epoch:210 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188219793140888\n",
      "Epoch:211 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188883826136589\n",
      "Epoch:212 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014185825362801552\n",
      "Epoch:213 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014196433126926422\n",
      "Epoch:214 Train acc:0.49122807017543857 Test acc:0.4988479262672811 Train loss:0.014187865890562534\n",
      "Epoch:215 Train acc:0.4727095516569201 Test acc:0.5011520737327189 Train loss:0.014189145527780056\n",
      "Epoch:216 Train acc:0.49610136452241715 Test acc:0.4988479262672811 Train loss:0.014187936671078205\n",
      "Epoch:217 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01419240701943636\n",
      "Epoch:218 Train acc:0.5029239766081871 Test acc:0.5011520737327189 Train loss:0.014184435829520226\n",
      "Epoch:219 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189549721777439\n",
      "Epoch:220 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418977603316307\n",
      "Epoch:221 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187417924404144\n",
      "Epoch:222 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188661240041256\n",
      "Epoch:223 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188324101269245\n",
      "Epoch:224 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187884517014027\n",
      "Epoch:225 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418867614120245\n",
      "Epoch:226 Train acc:0.48050682261208577 Test acc:0.5011520737327189 Train loss:0.014187856577336788\n",
      "Epoch:227 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187921769917011\n",
      "Epoch:228 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188680797815323\n",
      "Epoch:229 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187884517014027\n",
      "Epoch:230 Train acc:0.49122807017543857 Test acc:0.4988479262672811 Train loss:0.014188404195010662\n",
      "Epoch:231 Train acc:0.49707602339181284 Test acc:0.5011520737327189 Train loss:0.014188818633556366\n",
      "Epoch:232 Train acc:0.49707602339181284 Test acc:0.4988479262672811 Train loss:0.01418515294790268\n",
      "Epoch:233 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014195303432643414\n",
      "Epoch:234 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188366942107677\n",
      "Epoch:235 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189817011356354\n",
      "Epoch:236 Train acc:0.5029239766081871 Test acc:0.5011520737327189 Train loss:0.014185632579028606\n",
      "Epoch:237 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01419081725180149\n",
      "Epoch:238 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188391156494617\n",
      "Epoch:239 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188253320753574\n",
      "Epoch:240 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189619570970535\n",
      "Epoch:241 Train acc:0.49610136452241715 Test acc:0.4988479262672811 Train loss:0.014187431894242764\n",
      "Epoch:242 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014186147600412369\n",
      "Epoch:243 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014197533950209618\n",
      "Epoch:244 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187033288180828\n",
      "Epoch:245 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014192101545631886\n",
      "Epoch:246 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418562512844801\n",
      "Epoch:247 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014190583489835262\n",
      "Epoch:248 Train acc:0.49122807017543857 Test acc:0.4988479262672811 Train loss:0.014187518507242203\n",
      "Epoch:249 Train acc:0.47076023391812866 Test acc:0.4988479262672811 Train loss:0.01418799627572298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:250 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187886379659176\n",
      "Epoch:251 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188303612172604\n",
      "Epoch:252 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189546927809715\n",
      "Epoch:253 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418896671384573\n",
      "Epoch:254 Train acc:0.4980506822612086 Test acc:0.5011520737327189 Train loss:0.014187314547598362\n",
      "Epoch:255 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189247973263264\n",
      "Epoch:256 Train acc:0.49122807017543857 Test acc:0.4988479262672811 Train loss:0.014186946675181389\n",
      "Epoch:257 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418453361839056\n",
      "Epoch:258 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014190949499607086\n",
      "Epoch:259 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418850664049387\n",
      "Epoch:260 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188067056238651\n",
      "Epoch:261 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188702218234539\n",
      "Epoch:262 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418791338801384\n",
      "Epoch:263 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418899092823267\n",
      "Epoch:264 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187471941113472\n",
      "Epoch:265 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187614433467388\n",
      "Epoch:266 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014186580665409565\n",
      "Epoch:267 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014191405847668648\n",
      "Epoch:268 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418793573975563\n",
      "Epoch:269 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014185765758156776\n",
      "Epoch:270 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189054258167744\n",
      "Epoch:271 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014191118068993092\n",
      "Epoch:272 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418874878436327\n",
      "Epoch:273 Train acc:0.5009746588693957 Test acc:0.5011520737327189 Train loss:0.014188527129590511\n",
      "Epoch:274 Train acc:0.49902534113060426 Test acc:0.4988479262672811 Train loss:0.014187254942953587\n",
      "Epoch:275 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014191187918186188\n",
      "Epoch:276 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187143184244633\n",
      "Epoch:277 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187323860824108\n",
      "Epoch:278 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187333174049854\n",
      "Epoch:279 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01419188641011715\n",
      "Epoch:280 Train acc:0.49317738791423 Test acc:0.5011520737327189 Train loss:0.014186836779117584\n",
      "Epoch:281 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014186237938702106\n",
      "Epoch:282 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418930571526289\n",
      "Epoch:283 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014185425825417042\n",
      "Epoch:284 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014193653129041195\n",
      "Epoch:285 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189307577908039\n",
      "Epoch:286 Train acc:0.5019493177387915 Test acc:0.4988479262672811 Train loss:0.014186568558216095\n",
      "Epoch:287 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188309200108051\n",
      "Epoch:288 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188572764396667\n",
      "Epoch:289 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188448898494244\n",
      "Epoch:290 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187467284500599\n",
      "Epoch:291 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014190003275871277\n",
      "Epoch:292 Train acc:0.4873294346978557 Test acc:0.5011520737327189 Train loss:0.014187885448336601\n",
      "Epoch:293 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014185028150677681\n",
      "Epoch:294 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188521541655064\n",
      "Epoch:295 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014192180708050728\n",
      "Epoch:296 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187643304467201\n",
      "Epoch:297 Train acc:0.49610136452241715 Test acc:0.4988479262672811 Train loss:0.014188609085977077\n",
      "Epoch:298 Train acc:0.5 Test acc:0.5011520737327189 Train loss:0.014186226762831211\n",
      "Epoch:299 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014191808179020882\n",
      "Epoch:300 Train acc:0.4902534113060429 Test acc:0.4988479262672811 Train loss:0.014188450761139393\n",
      "Epoch:301 Train acc:0.48635477582846004 Test acc:0.5011520737327189 Train loss:0.014187440276145935\n",
      "Epoch:302 Train acc:0.4980506822612086 Test acc:0.4988479262672811 Train loss:0.01418788731098175\n",
      "Epoch:303 Train acc:0.49610136452241715 Test acc:0.5011520737327189 Train loss:0.014190041460096836\n",
      "Epoch:304 Train acc:0.4922027290448343 Test acc:0.4988479262672811 Train loss:0.014187301509082317\n",
      "Epoch:305 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014190813526511192\n",
      "Epoch:306 Train acc:0.4922027290448343 Test acc:0.4988479262672811 Train loss:0.014188094064593315\n",
      "Epoch:307 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418636180460453\n",
      "Epoch:308 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188027940690517\n",
      "Epoch:309 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187684282660484\n",
      "Epoch:310 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418560091406107\n",
      "Epoch:311 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014191806316375732\n",
      "Epoch:312 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188389293849468\n",
      "Epoch:313 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014190197922289371\n",
      "Epoch:314 Train acc:0.50682261208577 Test acc:0.5011520737327189 Train loss:0.01418613363057375\n",
      "Epoch:315 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418625470250845\n",
      "Epoch:316 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188738539814949\n",
      "Epoch:317 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014185305684804916\n",
      "Epoch:318 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014194302260875702\n",
      "Epoch:319 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188834466040134\n",
      "Epoch:320 Train acc:0.4951267056530214 Test acc:0.4988479262672811 Train loss:0.01418602280318737\n",
      "Epoch:321 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014192041009664536\n",
      "Epoch:322 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188837260007858\n",
      "Epoch:323 Train acc:0.5009746588693957 Test acc:0.5011520737327189 Train loss:0.014187928289175034\n",
      "Epoch:324 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418766938149929\n",
      "Epoch:325 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188236556947231\n",
      "Epoch:326 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188016764819622\n",
      "Epoch:327 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188999310135841\n",
      "Epoch:328 Train acc:0.4922027290448343 Test acc:0.4988479262672811 Train loss:0.014188291504979134\n",
      "Epoch:329 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187589287757874\n",
      "Epoch:330 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187515713274479\n",
      "Epoch:331 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014186649583280087\n",
      "Epoch:332 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01419090386480093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:333 Train acc:0.4834307992202729 Test acc:0.4988479262672811 Train loss:0.014188510365784168\n",
      "Epoch:334 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189287088811398\n",
      "Epoch:335 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014191309921443462\n",
      "Epoch:336 Train acc:0.49415204678362573 Test acc:0.5011520737327189 Train loss:0.014187662862241268\n",
      "Epoch:337 Train acc:0.48635477582846004 Test acc:0.4988479262672811 Train loss:0.014187493361532688\n",
      "Epoch:338 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187379740178585\n",
      "Epoch:339 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418773178011179\n",
      "Epoch:340 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188463799655437\n",
      "Epoch:341 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189952984452248\n",
      "Epoch:342 Train acc:0.5 Test acc:0.5011520737327189 Train loss:0.014187020249664783\n",
      "Epoch:343 Train acc:0.48050682261208577 Test acc:0.5011520737327189 Train loss:0.014188457280397415\n",
      "Epoch:344 Train acc:0.50682261208577 Test acc:0.4988479262672811 Train loss:0.014189530164003372\n",
      "Epoch:345 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014186539687216282\n",
      "Epoch:346 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188515953719616\n",
      "Epoch:347 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188235625624657\n",
      "Epoch:348 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014186955988407135\n",
      "Epoch:349 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014185783453285694\n",
      "Epoch:350 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188655652105808\n",
      "Epoch:351 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418900303542614\n",
      "Epoch:352 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014191561378538609\n",
      "Epoch:353 Train acc:0.49902534113060426 Test acc:0.4988479262672811 Train loss:0.014186096377670765\n",
      "Epoch:354 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014192327857017517\n",
      "Epoch:355 Train acc:0.49707602339181284 Test acc:0.5011520737327189 Train loss:0.014187303371727467\n",
      "Epoch:356 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.0141896428540349\n",
      "Epoch:357 Train acc:0.5 Test acc:0.4988479262672811 Train loss:0.014187556691467762\n",
      "Epoch:358 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189468696713448\n",
      "Epoch:359 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187646098434925\n",
      "Epoch:360 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187948778271675\n",
      "Epoch:361 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014185840263962746\n",
      "Epoch:362 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014192716218531132\n",
      "Epoch:363 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187353663146496\n",
      "Epoch:364 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187776483595371\n",
      "Epoch:365 Train acc:0.48148148148148145 Test acc:0.4988479262672811 Train loss:0.014189276844263077\n",
      "Epoch:366 Train acc:0.48050682261208577 Test acc:0.5011520737327189 Train loss:0.014189635403454304\n",
      "Epoch:367 Train acc:0.4951267056530214 Test acc:0.4988479262672811 Train loss:0.014187057502567768\n",
      "Epoch:368 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188126660883427\n",
      "Epoch:369 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188786037266254\n",
      "Epoch:370 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187500812113285\n",
      "Epoch:371 Train acc:0.48050682261208577 Test acc:0.4988479262672811 Train loss:0.01418923120945692\n",
      "Epoch:372 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187600463628769\n",
      "Epoch:373 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189749024808407\n",
      "Epoch:374 Train acc:0.4873294346978557 Test acc:0.5011520737327189 Train loss:0.014190088957548141\n",
      "Epoch:375 Train acc:0.4980506822612086 Test acc:0.4988479262672811 Train loss:0.01418580487370491\n",
      "Epoch:376 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01419356744736433\n",
      "Epoch:377 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187131077051163\n",
      "Epoch:378 Train acc:0.48635477582846004 Test acc:0.5011520737327189 Train loss:0.014188701286911964\n",
      "Epoch:379 Train acc:0.49122807017543857 Test acc:0.4988479262672811 Train loss:0.014187234453856945\n",
      "Epoch:380 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014185678213834763\n",
      "Epoch:381 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014191077090799809\n",
      "Epoch:382 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188469387590885\n",
      "Epoch:383 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189246110618114\n",
      "Epoch:384 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014190527610480785\n",
      "Epoch:385 Train acc:0.4873294346978557 Test acc:0.5011520737327189 Train loss:0.014187975786626339\n",
      "Epoch:386 Train acc:0.4727095516569201 Test acc:0.4988479262672811 Train loss:0.014189225621521473\n",
      "Epoch:387 Train acc:0.5048732943469786 Test acc:0.5011520737327189 Train loss:0.014187268912792206\n",
      "Epoch:388 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014192390255630016\n",
      "Epoch:389 Train acc:0.4834307992202729 Test acc:0.4988479262672811 Train loss:0.01418984867632389\n",
      "Epoch:390 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014186898246407509\n",
      "Epoch:391 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014191621914505959\n",
      "Epoch:392 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014185382053256035\n",
      "Epoch:393 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014190412126481533\n",
      "Epoch:394 Train acc:0.4775828460038986 Test acc:0.5011520737327189 Train loss:0.014188936911523342\n",
      "Epoch:395 Train acc:0.5019493177387915 Test acc:0.4988479262672811 Train loss:0.014187006279826164\n",
      "Epoch:396 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188036322593689\n",
      "Epoch:397 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014186329208314419\n",
      "Epoch:398 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188868924975395\n",
      "Epoch:399 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187896624207497\n",
      "Epoch:400 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014184858649969101\n",
      "Epoch:401 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014183721505105495\n",
      "Epoch:402 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01419250387698412\n",
      "Epoch:403 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014192985370755196\n",
      "Epoch:404 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189665205776691\n",
      "Epoch:405 Train acc:0.49610136452241715 Test acc:0.5011520737327189 Train loss:0.014189020730555058\n",
      "Epoch:406 Train acc:0.4980506822612086 Test acc:0.4988479262672811 Train loss:0.014188368804752827\n",
      "Epoch:407 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188014902174473\n",
      "Epoch:408 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189362525939941\n",
      "Epoch:409 Train acc:0.5048732943469786 Test acc:0.5011520737327189 Train loss:0.014190350659191608\n",
      "Epoch:410 Train acc:0.49317738791423 Test acc:0.4988479262672811 Train loss:0.01418757252395153\n",
      "Epoch:411 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188209548592567\n",
      "Epoch:412 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014191346243023872\n",
      "Epoch:413 Train acc:0.4678362573099415 Test acc:0.4988479262672811 Train loss:0.01418870035558939\n",
      "Epoch:414 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188024215400219\n",
      "Epoch:415 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014190654270350933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:416 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187733642756939\n",
      "Epoch:417 Train acc:0.4980506822612086 Test acc:0.5011520737327189 Train loss:0.014186733402311802\n",
      "Epoch:418 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01419141422957182\n",
      "Epoch:419 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014191930182278156\n",
      "Epoch:420 Train acc:0.47368421052631576 Test acc:0.4988479262672811 Train loss:0.014187660999596119\n",
      "Epoch:421 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189161360263824\n",
      "Epoch:422 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188168570399284\n",
      "Epoch:423 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418954599648714\n",
      "Epoch:424 Train acc:0.4834307992202729 Test acc:0.4988479262672811 Train loss:0.014188754372298717\n",
      "Epoch:425 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188956469297409\n",
      "Epoch:426 Train acc:0.48635477582846004 Test acc:0.4988479262672811 Train loss:0.01418786309659481\n",
      "Epoch:427 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187504537403584\n",
      "Epoch:428 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187317341566086\n",
      "Epoch:429 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418771967291832\n",
      "Epoch:430 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014185935258865356\n",
      "Epoch:431 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014184629544615746\n",
      "Epoch:432 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188612811267376\n",
      "Epoch:433 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.0141924312338233\n",
      "Epoch:434 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187635853886604\n",
      "Epoch:435 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188888482749462\n",
      "Epoch:436 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188827015459538\n",
      "Epoch:437 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418852899223566\n",
      "Epoch:438 Train acc:0.5029239766081871 Test acc:0.4988479262672811 Train loss:0.014186234213411808\n",
      "Epoch:439 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014183804392814636\n",
      "Epoch:440 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014194367453455925\n",
      "Epoch:441 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.0141904940828681\n",
      "Epoch:442 Train acc:0.4853801169590643 Test acc:0.5011520737327189 Train loss:0.014187698252499104\n",
      "Epoch:443 Train acc:0.4922027290448343 Test acc:0.4988479262672811 Train loss:0.014187317341566086\n",
      "Epoch:444 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014191263355314732\n",
      "Epoch:445 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187531545758247\n",
      "Epoch:446 Train acc:0.4951267056530214 Test acc:0.5011520737327189 Train loss:0.014188139699399471\n",
      "Epoch:447 Train acc:0.49317738791423 Test acc:0.4988479262672811 Train loss:0.014188116416335106\n",
      "Epoch:448 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187616296112537\n",
      "Epoch:449 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187724329531193\n",
      "Epoch:450 Train acc:0.49317738791423 Test acc:0.4988479262672811 Train loss:0.014187581837177277\n",
      "Epoch:451 Train acc:0.48927875243664715 Test acc:0.4988479262672811 Train loss:0.014187493361532688\n",
      "Epoch:452 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187620021402836\n",
      "Epoch:453 Train acc:0.48927875243664715 Test acc:0.5011520737327189 Train loss:0.014190721325576305\n",
      "Epoch:454 Train acc:0.49902534113060426 Test acc:0.4988479262672811 Train loss:0.014187409542500973\n",
      "Epoch:455 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014193049632012844\n",
      "Epoch:456 Train acc:0.4844054580896686 Test acc:0.5011520737327189 Train loss:0.014189492911100388\n",
      "Epoch:457 Train acc:0.5 Test acc:0.4988479262672811 Train loss:0.014187461696565151\n",
      "Epoch:458 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187818393111229\n",
      "Epoch:459 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188851229846478\n",
      "Epoch:460 Train acc:0.49707602339181284 Test acc:0.5011520737327189 Train loss:0.014186843298375607\n",
      "Epoch:461 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418805681169033\n",
      "Epoch:462 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188554137945175\n",
      "Epoch:463 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188152737915516\n",
      "Epoch:464 Train acc:0.4844054580896686 Test acc:0.5011520737327189 Train loss:0.014188176020979881\n",
      "Epoch:465 Train acc:0.4834307992202729 Test acc:0.4988479262672811 Train loss:0.014188104309141636\n",
      "Epoch:466 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418845634907484\n",
      "Epoch:467 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014186970889568329\n",
      "Epoch:468 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188423752784729\n",
      "Epoch:469 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014190604910254478\n",
      "Epoch:470 Train acc:0.49415204678362573 Test acc:0.4988479262672811 Train loss:0.014187948778271675\n",
      "Epoch:471 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187825843691826\n",
      "Epoch:472 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188694767653942\n",
      "Epoch:473 Train acc:0.49317738791423 Test acc:0.5011520737327189 Train loss:0.014186891727149487\n",
      "Epoch:474 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187916181981564\n",
      "Epoch:475 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014186226762831211\n",
      "Epoch:476 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188915491104126\n",
      "Epoch:477 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014186595566570759\n",
      "Epoch:478 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01419025007635355\n",
      "Epoch:479 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189093373715878\n",
      "Epoch:480 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418771781027317\n",
      "Epoch:481 Train acc:0.46588693957115007 Test acc:0.5011520737327189 Train loss:0.014188230969011784\n",
      "Epoch:482 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014190426096320152\n",
      "Epoch:483 Train acc:0.48830409356725146 Test acc:0.5011520737327189 Train loss:0.014188144356012344\n",
      "Epoch:484 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014191561378538609\n",
      "Epoch:485 Train acc:0.5029239766081871 Test acc:0.4988479262672811 Train loss:0.014186616986989975\n",
      "Epoch:486 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418972946703434\n",
      "Epoch:487 Train acc:0.47953216374269003 Test acc:0.5011520737327189 Train loss:0.0141877057030797\n",
      "Epoch:488 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418839581310749\n",
      "Epoch:489 Train acc:0.49707602339181284 Test acc:0.4988479262672811 Train loss:0.014187644235789776\n",
      "Epoch:490 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187068678438663\n",
      "Epoch:491 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01419047825038433\n",
      "Epoch:492 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187215827405453\n",
      "Epoch:493 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418718695640564\n",
      "Epoch:494 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01419102679938078\n",
      "Epoch:495 Train acc:0.4824561403508772 Test acc:0.4988479262672811 Train loss:0.01418840978294611\n",
      "Epoch:496 Train acc:0.48927875243664715 Test acc:0.4988479262672811 Train loss:0.014188319444656372\n",
      "Epoch:497 Train acc:0.4922027290448343 Test acc:0.5011520737327189 Train loss:0.014192815870046616\n",
      "Epoch:498 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187819324433804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:499 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187405817210674\n",
      "Epoch:500 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188398607075214\n",
      "Epoch:501 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418673899024725\n",
      "Epoch:502 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188569039106369\n",
      "Epoch:503 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014186061918735504\n",
      "Epoch:504 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188160188496113\n",
      "Epoch:505 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188363216817379\n",
      "Epoch:506 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01419210433959961\n",
      "Epoch:507 Train acc:0.48050682261208577 Test acc:0.4988479262672811 Train loss:0.014188351109623909\n",
      "Epoch:508 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014190820045769215\n",
      "Epoch:509 Train acc:0.49707602339181284 Test acc:0.4988479262672811 Train loss:0.014189209789037704\n",
      "Epoch:510 Train acc:0.49707602339181284 Test acc:0.5011520737327189 Train loss:0.014186490327119827\n",
      "Epoch:511 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189500361680984\n",
      "Epoch:512 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188014902174473\n",
      "Epoch:513 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418845634907484\n",
      "Epoch:514 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187793247401714\n",
      "Epoch:515 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187872409820557\n",
      "Epoch:516 Train acc:0.5038986354775828 Test acc:0.4988479262672811 Train loss:0.014186172746121883\n",
      "Epoch:517 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014193488284945488\n",
      "Epoch:518 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188382774591446\n",
      "Epoch:519 Train acc:0.5048732943469786 Test acc:0.5011520737327189 Train loss:0.014188229106366634\n",
      "Epoch:520 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418865192681551\n",
      "Epoch:521 Train acc:0.5097465886939572 Test acc:0.4988479262672811 Train loss:0.014185057021677494\n",
      "Epoch:522 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.0141850421205163\n",
      "Epoch:523 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418355479836464\n",
      "Epoch:524 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014195875264704227\n",
      "Epoch:525 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014190450310707092\n",
      "Epoch:526 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188600704073906\n",
      "Epoch:527 Train acc:0.5038986354775828 Test acc:0.5011520737327189 Train loss:0.014189036563038826\n",
      "Epoch:528 Train acc:0.4824561403508772 Test acc:0.4988479262672811 Train loss:0.014192068949341774\n",
      "Epoch:529 Train acc:0.4980506822612086 Test acc:0.5011520737327189 Train loss:0.014187678694725037\n",
      "Epoch:530 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189515262842178\n",
      "Epoch:531 Train acc:0.4766081871345029 Test acc:0.4988479262672811 Train loss:0.014189519919455051\n",
      "Epoch:532 Train acc:0.5029239766081871 Test acc:0.5011520737327189 Train loss:0.014186899177730083\n",
      "Epoch:533 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418791338801384\n",
      "Epoch:534 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189026318490505\n",
      "Epoch:535 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188163913786411\n",
      "Epoch:536 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014186520129442215\n",
      "Epoch:537 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014185624197125435\n",
      "Epoch:538 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418838370591402\n",
      "Epoch:539 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014191744849085808\n",
      "Epoch:540 Train acc:0.4824561403508772 Test acc:0.5011520737327189 Train loss:0.014187788590788841\n",
      "Epoch:541 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189655892550945\n",
      "Epoch:542 Train acc:0.49122807017543857 Test acc:0.4988479262672811 Train loss:0.014187848195433617\n",
      "Epoch:543 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014186691492795944\n",
      "Epoch:544 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418553851544857\n",
      "Epoch:545 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418881956487894\n",
      "Epoch:546 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189361594617367\n",
      "Epoch:547 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187874272465706\n",
      "Epoch:548 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014191671274602413\n",
      "Epoch:549 Train acc:0.4775828460038986 Test acc:0.4988479262672811 Train loss:0.014189301058650017\n",
      "Epoch:550 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187580905854702\n",
      "Epoch:551 Train acc:0.4834307992202729 Test acc:0.4988479262672811 Train loss:0.014187672175467014\n",
      "Epoch:552 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187347143888474\n",
      "Epoch:553 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014186415821313858\n",
      "Epoch:554 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187620021402836\n",
      "Epoch:555 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.0141924899071455\n",
      "Epoch:556 Train acc:0.47855750487329435 Test acc:0.4988479262672811 Train loss:0.014190624468028545\n",
      "Epoch:557 Train acc:0.48927875243664715 Test acc:0.4988479262672811 Train loss:0.014187635853886604\n",
      "Epoch:558 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187602326273918\n",
      "Epoch:559 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014190095476806164\n",
      "Epoch:560 Train acc:0.4922027290448343 Test acc:0.5011520737327189 Train loss:0.014187327586114407\n",
      "Epoch:561 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014186082407832146\n",
      "Epoch:562 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188282191753387\n",
      "Epoch:563 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189003966748714\n",
      "Epoch:564 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188546687364578\n",
      "Epoch:565 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418783888220787\n",
      "Epoch:566 Train acc:0.4756335282651072 Test acc:0.5011520737327189 Train loss:0.014188205823302269\n",
      "Epoch:567 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188593253493309\n",
      "Epoch:568 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188883826136589\n",
      "Epoch:569 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189532026648521\n",
      "Epoch:570 Train acc:0.49707602339181284 Test acc:0.4988479262672811 Train loss:0.01418739277869463\n",
      "Epoch:571 Train acc:0.4775828460038986 Test acc:0.4988479262672811 Train loss:0.01418889407068491\n",
      "Epoch:572 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189778827130795\n",
      "Epoch:573 Train acc:0.49707602339181284 Test acc:0.5011520737327189 Train loss:0.0141878891736269\n",
      "Epoch:574 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014190110377967358\n",
      "Epoch:575 Train acc:0.48148148148148145 Test acc:0.4988479262672811 Train loss:0.014189007692039013\n",
      "Epoch:576 Train acc:0.4980506822612086 Test acc:0.5011520737327189 Train loss:0.014187485910952091\n",
      "Epoch:577 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189607463777065\n",
      "Epoch:578 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188366942107677\n",
      "Epoch:579 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188207685947418\n",
      "Epoch:580 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418846845626831\n",
      "Epoch:581 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189670793712139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:582 Train acc:0.5107212475633528 Test acc:0.4988479262672811 Train loss:0.014188898727297783\n",
      "Epoch:583 Train acc:0.49610136452241715 Test acc:0.5011520737327189 Train loss:0.014186909422278404\n",
      "Epoch:584 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187896624207497\n",
      "Epoch:585 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014186386950314045\n",
      "Epoch:586 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188208617269993\n",
      "Epoch:587 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014186937361955643\n",
      "Epoch:588 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189448207616806\n",
      "Epoch:589 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188839122653008\n",
      "Epoch:590 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014191140420734882\n",
      "Epoch:591 Train acc:0.49902534113060426 Test acc:0.4988479262672811 Train loss:0.014187407679855824\n",
      "Epoch:592 Train acc:0.49610136452241715 Test acc:0.5011520737327189 Train loss:0.014188827015459538\n",
      "Epoch:593 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188016764819622\n",
      "Epoch:594 Train acc:0.5 Test acc:0.4988479262672811 Train loss:0.014191372320055962\n",
      "Epoch:595 Train acc:0.4834307992202729 Test acc:0.4988479262672811 Train loss:0.014187655411660671\n",
      "Epoch:596 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418742910027504\n",
      "Epoch:597 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418986078351736\n",
      "Epoch:598 Train acc:0.4951267056530214 Test acc:0.5011520737327189 Train loss:0.014188735745847225\n",
      "Epoch:599 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189701527357101\n",
      "Epoch:600 Train acc:0.5185185185185185 Test acc:0.4988479262672811 Train loss:0.014184532687067986\n",
      "Epoch:601 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014191481284797192\n",
      "Epoch:602 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418611966073513\n",
      "Epoch:603 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187358319759369\n",
      "Epoch:604 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418445073068142\n",
      "Epoch:605 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014183159917593002\n",
      "Epoch:606 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014183231629431248\n",
      "Epoch:607 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189592562615871\n",
      "Epoch:608 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01419516559690237\n",
      "Epoch:609 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187866821885109\n",
      "Epoch:610 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187920838594437\n",
      "Epoch:611 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188023284077644\n",
      "Epoch:612 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189018867909908\n",
      "Epoch:613 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187505468726158\n",
      "Epoch:614 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188048429787159\n",
      "Epoch:615 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187893830239773\n",
      "Epoch:616 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187808148562908\n",
      "Epoch:617 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187928289175034\n",
      "Epoch:618 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187553897500038\n",
      "Epoch:619 Train acc:0.4873294346978557 Test acc:0.4988479262672811 Train loss:0.01418775599449873\n",
      "Epoch:620 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188798144459724\n",
      "Epoch:621 Train acc:0.49317738791423 Test acc:0.5011520737327189 Train loss:0.014189688488841057\n",
      "Epoch:622 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418907567858696\n",
      "Epoch:623 Train acc:0.4922027290448343 Test acc:0.4988479262672811 Train loss:0.01418852899223566\n",
      "Epoch:624 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187966473400593\n",
      "Epoch:625 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187916181981564\n",
      "Epoch:626 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014186818152666092\n",
      "Epoch:627 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01419176533818245\n",
      "Epoch:628 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187517575919628\n",
      "Epoch:629 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187113381922245\n",
      "Epoch:630 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418858952820301\n",
      "Epoch:631 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188548550009727\n",
      "Epoch:632 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187860302627087\n",
      "Epoch:633 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188002794981003\n",
      "Epoch:634 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014186129905283451\n",
      "Epoch:635 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188435859978199\n",
      "Epoch:636 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188568107783794\n",
      "Epoch:637 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188853092491627\n",
      "Epoch:638 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188620261847973\n",
      "Epoch:639 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188203029334545\n",
      "Epoch:640 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014190188609063625\n",
      "Epoch:641 Train acc:0.48927875243664715 Test acc:0.5011520737327189 Train loss:0.014188740402460098\n",
      "Epoch:642 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187769964337349\n",
      "Epoch:643 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187860302627087\n",
      "Epoch:644 Train acc:0.47953216374269003 Test acc:0.5011520737327189 Train loss:0.014188244007527828\n",
      "Epoch:645 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188582077622414\n",
      "Epoch:646 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187689870595932\n",
      "Epoch:647 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418765913695097\n",
      "Epoch:648 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188486151397228\n",
      "Epoch:649 Train acc:0.49122807017543857 Test acc:0.4988479262672811 Train loss:0.014188076369464397\n",
      "Epoch:650 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187602326273918\n",
      "Epoch:651 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014185579493641853\n",
      "Epoch:652 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014186104759573936\n",
      "Epoch:653 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014183301478624344\n",
      "Epoch:654 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014196163974702358\n",
      "Epoch:655 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014183665625751019\n",
      "Epoch:656 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188380911946297\n",
      "Epoch:657 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188411645591259\n",
      "Epoch:658 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014192240312695503\n",
      "Epoch:659 Train acc:0.4951267056530214 Test acc:0.5011520737327189 Train loss:0.014187089167535305\n",
      "Epoch:660 Train acc:0.4951267056530214 Test acc:0.5011520737327189 Train loss:0.014195965602993965\n",
      "Epoch:661 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014197157695889473\n",
      "Epoch:662 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.0141869792714715\n",
      "Epoch:663 Train acc:0.49122807017543857 Test acc:0.5011520737327189 Train loss:0.014187493361532688\n",
      "Epoch:664 Train acc:0.4844054580896686 Test acc:0.4988479262672811 Train loss:0.014188360422849655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:665 Train acc:0.5 Test acc:0.5011520737327189 Train loss:0.014189384877681732\n",
      "Epoch:666 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188903383910656\n",
      "Epoch:667 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187037013471127\n",
      "Epoch:668 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188935048878193\n",
      "Epoch:669 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189809560775757\n",
      "Epoch:670 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418775599449873\n",
      "Epoch:671 Train acc:0.4853801169590643 Test acc:0.5011520737327189 Train loss:0.014187918975949287\n",
      "Epoch:672 Train acc:0.49415204678362573 Test acc:0.5011520737327189 Train loss:0.014188125729560852\n",
      "Epoch:673 Train acc:0.4873294346978557 Test acc:0.4988479262672811 Train loss:0.014189518056809902\n",
      "Epoch:674 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014190011657774448\n",
      "Epoch:675 Train acc:0.4834307992202729 Test acc:0.5011520737327189 Train loss:0.01418924517929554\n",
      "Epoch:676 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187099412083626\n",
      "Epoch:677 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01419019140303135\n",
      "Epoch:678 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187881723046303\n",
      "Epoch:679 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187511056661606\n",
      "Epoch:680 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014186936430633068\n",
      "Epoch:681 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014184712432324886\n",
      "Epoch:682 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188071712851524\n",
      "Epoch:683 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014183960855007172\n",
      "Epoch:684 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418930385261774\n",
      "Epoch:685 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014192658476531506\n",
      "Epoch:686 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188313856720924\n",
      "Epoch:687 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187997207045555\n",
      "Epoch:688 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188054949045181\n",
      "Epoch:689 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188801869750023\n",
      "Epoch:690 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418954785913229\n",
      "Epoch:691 Train acc:0.5029239766081871 Test acc:0.5011520737327189 Train loss:0.01419187244027853\n",
      "Epoch:692 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014185103587806225\n",
      "Epoch:693 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01419178768992424\n",
      "Epoch:694 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187175780534744\n",
      "Epoch:695 Train acc:0.47368421052631576 Test acc:0.4988479262672811 Train loss:0.014188065193593502\n",
      "Epoch:696 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014184987172484398\n",
      "Epoch:697 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418854296207428\n",
      "Epoch:698 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014184420928359032\n",
      "Epoch:699 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014194033108651638\n",
      "Epoch:700 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187579974532127\n",
      "Epoch:701 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188753440976143\n",
      "Epoch:702 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187859371304512\n",
      "Epoch:703 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187635853886604\n",
      "Epoch:704 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014186927117407322\n",
      "Epoch:705 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014192171394824982\n",
      "Epoch:706 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418666634708643\n",
      "Epoch:707 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014184437692165375\n",
      "Epoch:708 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188067056238651\n",
      "Epoch:709 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188045635819435\n",
      "Epoch:710 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188691042363644\n",
      "Epoch:711 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014191222377121449\n",
      "Epoch:712 Train acc:0.49707602339181284 Test acc:0.5011520737327189 Train loss:0.014187428168952465\n",
      "Epoch:713 Train acc:0.48635477582846004 Test acc:0.5011520737327189 Train loss:0.014188295230269432\n",
      "Epoch:714 Train acc:0.5029239766081871 Test acc:0.4988479262672811 Train loss:0.014187640510499477\n",
      "Epoch:715 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014185596257448196\n",
      "Epoch:716 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188564382493496\n",
      "Epoch:717 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014191791415214539\n",
      "Epoch:718 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188128523528576\n",
      "Epoch:719 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187751337885857\n",
      "Epoch:720 Train acc:0.4834307992202729 Test acc:0.4988479262672811 Train loss:0.014187907800078392\n",
      "Epoch:721 Train acc:0.49707602339181284 Test acc:0.5011520737327189 Train loss:0.014187988825142384\n",
      "Epoch:722 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418608333915472\n",
      "Epoch:723 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014191634021699429\n",
      "Epoch:724 Train acc:0.4922027290448343 Test acc:0.4988479262672811 Train loss:0.014187484979629517\n",
      "Epoch:725 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187771826982498\n",
      "Epoch:726 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188320375978947\n",
      "Epoch:727 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187580905854702\n",
      "Epoch:728 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187430962920189\n",
      "Epoch:729 Train acc:0.4844054580896686 Test acc:0.5011520737327189 Train loss:0.014187709428369999\n",
      "Epoch:730 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01419081911444664\n",
      "Epoch:731 Train acc:0.4678362573099415 Test acc:0.4988479262672811 Train loss:0.014188731089234352\n",
      "Epoch:732 Train acc:0.4824561403508772 Test acc:0.4988479262672811 Train loss:0.014187932014465332\n",
      "Epoch:733 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187968336045742\n",
      "Epoch:734 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418874878436327\n",
      "Epoch:735 Train acc:0.5029239766081871 Test acc:0.5011520737327189 Train loss:0.014188124798238277\n",
      "Epoch:736 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187580905854702\n",
      "Epoch:737 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187706634402275\n",
      "Epoch:738 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188501983880997\n",
      "Epoch:739 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187240041792393\n",
      "Epoch:740 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188566245138645\n",
      "Epoch:741 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188637025654316\n",
      "Epoch:742 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418805681169033\n",
      "Epoch:743 Train acc:0.48635477582846004 Test acc:0.4988479262672811 Train loss:0.014188702218234539\n",
      "Epoch:744 Train acc:0.48148148148148145 Test acc:0.4988479262672811 Train loss:0.014188149943947792\n",
      "Epoch:745 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014185957610607147\n",
      "Epoch:746 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014191285707056522\n",
      "Epoch:747 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188718982040882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:748 Train acc:0.4746588693957115 Test acc:0.5011520737327189 Train loss:0.014189142733812332\n",
      "Epoch:749 Train acc:0.49707602339181284 Test acc:0.4988479262672811 Train loss:0.014191919006407261\n",
      "Epoch:750 Train acc:0.48148148148148145 Test acc:0.5011520737327189 Train loss:0.01419010665267706\n",
      "Epoch:751 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187036082148552\n",
      "Epoch:752 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187511056661606\n",
      "Epoch:753 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188040979206562\n",
      "Epoch:754 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187918044626713\n",
      "Epoch:755 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187648892402649\n",
      "Epoch:756 Train acc:0.49902534113060426 Test acc:0.4988479262672811 Train loss:0.014188052155077457\n",
      "Epoch:757 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187651686370373\n",
      "Epoch:758 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187926426529884\n",
      "Epoch:759 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418940257281065\n",
      "Epoch:760 Train acc:0.48927875243664715 Test acc:0.4988479262672811 Train loss:0.01418833527714014\n",
      "Epoch:761 Train acc:0.4980506822612086 Test acc:0.5011520737327189 Train loss:0.014189542271196842\n",
      "Epoch:762 Train acc:0.4980506822612086 Test acc:0.4988479262672811 Train loss:0.01418730802834034\n",
      "Epoch:763 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188002794981003\n",
      "Epoch:764 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189251698553562\n",
      "Epoch:765 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189818874001503\n",
      "Epoch:766 Train acc:0.49415204678362573 Test acc:0.5011520737327189 Train loss:0.014189597219228745\n",
      "Epoch:767 Train acc:0.49122807017543857 Test acc:0.4988479262672811 Train loss:0.014185597188770771\n",
      "Epoch:768 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014184227213263512\n",
      "Epoch:769 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01419403962790966\n",
      "Epoch:770 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187602326273918\n",
      "Epoch:771 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187905006110668\n",
      "Epoch:772 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187674038112164\n",
      "Epoch:773 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187623746693134\n",
      "Epoch:774 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418810524046421\n",
      "Epoch:775 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187692664563656\n",
      "Epoch:776 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187594875693321\n",
      "Epoch:777 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187755063176155\n",
      "Epoch:778 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187490567564964\n",
      "Epoch:779 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187769964337349\n",
      "Epoch:780 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188590459525585\n",
      "Epoch:781 Train acc:0.5048732943469786 Test acc:0.5011520737327189 Train loss:0.014188368804752827\n",
      "Epoch:782 Train acc:0.48927875243664715 Test acc:0.5011520737327189 Train loss:0.014187978580594063\n",
      "Epoch:783 Train acc:0.47855750487329435 Test acc:0.4988479262672811 Train loss:0.014189954847097397\n",
      "Epoch:784 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014186723157763481\n",
      "Epoch:785 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418935228139162\n",
      "Epoch:786 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014190967194736004\n",
      "Epoch:787 Train acc:0.47953216374269003 Test acc:0.5011520737327189 Train loss:0.014188910834491253\n",
      "Epoch:788 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188066124916077\n",
      "Epoch:789 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188189059495926\n",
      "Epoch:790 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188618399202824\n",
      "Epoch:791 Train acc:0.5077972709551657 Test acc:0.4988479262672811 Train loss:0.01418748963624239\n",
      "Epoch:792 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014186188578605652\n",
      "Epoch:793 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01419144682586193\n",
      "Epoch:794 Train acc:0.47953216374269003 Test acc:0.4988479262672811 Train loss:0.014188097789883614\n",
      "Epoch:795 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188345521688461\n",
      "Epoch:796 Train acc:0.4922027290448343 Test acc:0.4988479262672811 Train loss:0.014187909662723541\n",
      "Epoch:797 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187498949468136\n",
      "Epoch:798 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187492430210114\n",
      "Epoch:799 Train acc:0.4873294346978557 Test acc:0.4988479262672811 Train loss:0.014188360422849655\n",
      "Epoch:800 Train acc:0.48635477582846004 Test acc:0.5011520737327189 Train loss:0.014187873341143131\n",
      "Epoch:801 Train acc:0.4873294346978557 Test acc:0.4988479262672811 Train loss:0.014188925735652447\n",
      "Epoch:802 Train acc:0.4844054580896686 Test acc:0.4988479262672811 Train loss:0.014188049361109734\n",
      "Epoch:803 Train acc:0.5019493177387915 Test acc:0.5011520737327189 Train loss:0.014188218861818314\n",
      "Epoch:804 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014190063811838627\n",
      "Epoch:805 Train acc:0.4824561403508772 Test acc:0.4988479262672811 Train loss:0.014188159257173538\n",
      "Epoch:806 Train acc:0.49415204678362573 Test acc:0.5011520737327189 Train loss:0.01418835949152708\n",
      "Epoch:807 Train acc:0.4902534113060429 Test acc:0.5011520737327189 Train loss:0.014187678694725037\n",
      "Epoch:808 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418781466782093\n",
      "Epoch:809 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188091270625591\n",
      "Epoch:810 Train acc:0.48635477582846004 Test acc:0.5011520737327189 Train loss:0.014187443070113659\n",
      "Epoch:811 Train acc:0.5 Test acc:0.4988479262672811 Train loss:0.014188009314239025\n",
      "Epoch:812 Train acc:0.5 Test acc:0.5011520737327189 Train loss:0.014187002554535866\n",
      "Epoch:813 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189377427101135\n",
      "Epoch:814 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188200235366821\n",
      "Epoch:815 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188408851623535\n",
      "Epoch:816 Train acc:0.4844054580896686 Test acc:0.5011520737327189 Train loss:0.014188249595463276\n",
      "Epoch:817 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188647270202637\n",
      "Epoch:818 Train acc:0.4853801169590643 Test acc:0.4988479262672811 Train loss:0.014187898486852646\n",
      "Epoch:819 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014186874963343143\n",
      "Epoch:820 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014192108064889908\n",
      "Epoch:821 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014191684313118458\n",
      "Epoch:822 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418765913695097\n",
      "Epoch:823 Train acc:0.5029239766081871 Test acc:0.5011520737327189 Train loss:0.01418733224272728\n",
      "Epoch:824 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187706634402275\n",
      "Epoch:825 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014185967855155468\n",
      "Epoch:826 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418992504477501\n",
      "Epoch:827 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188691042363644\n",
      "Epoch:828 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014185911044478416\n",
      "Epoch:829 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418977975845337\n",
      "Epoch:830 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188426546752453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:831 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189931564033031\n",
      "Epoch:832 Train acc:0.5 Test acc:0.4988479262672811 Train loss:0.014187580905854702\n",
      "Epoch:833 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187166467308998\n",
      "Epoch:834 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187159016728401\n",
      "Epoch:835 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187718741595745\n",
      "Epoch:836 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188786037266254\n",
      "Epoch:837 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188092201948166\n",
      "Epoch:838 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014185527339577675\n",
      "Epoch:839 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188610948622227\n",
      "Epoch:840 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188013970851898\n",
      "Epoch:841 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188006520271301\n",
      "Epoch:842 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014186677522957325\n",
      "Epoch:843 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188073575496674\n",
      "Epoch:844 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014191935770213604\n",
      "Epoch:845 Train acc:0.48830409356725146 Test acc:0.5011520737327189 Train loss:0.014188800007104874\n",
      "Epoch:846 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187753200531006\n",
      "Epoch:847 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014190325513482094\n",
      "Epoch:848 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187217690050602\n",
      "Epoch:849 Train acc:0.4951267056530214 Test acc:0.5011520737327189 Train loss:0.014188583008944988\n",
      "Epoch:850 Train acc:0.4980506822612086 Test acc:0.4988479262672811 Train loss:0.014188045635819435\n",
      "Epoch:851 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188252389431\n",
      "Epoch:852 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187982305884361\n",
      "Epoch:853 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418637577444315\n",
      "Epoch:854 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014191696420311928\n",
      "Epoch:855 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188013970851898\n",
      "Epoch:856 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187985099852085\n",
      "Epoch:857 Train acc:0.48050682261208577 Test acc:0.4988479262672811 Train loss:0.014188003726303577\n",
      "Epoch:858 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187647961080074\n",
      "Epoch:859 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188315719366074\n",
      "Epoch:860 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187417924404144\n",
      "Epoch:861 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187957160174847\n",
      "Epoch:862 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187800697982311\n",
      "Epoch:863 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418998371809721\n",
      "Epoch:864 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014186793938279152\n",
      "Epoch:865 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187641441822052\n",
      "Epoch:866 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418787706643343\n",
      "Epoch:867 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187629334628582\n",
      "Epoch:868 Train acc:0.4853801169590643 Test acc:0.4988479262672811 Train loss:0.014188246801495552\n",
      "Epoch:869 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418889407068491\n",
      "Epoch:870 Train acc:0.49707602339181284 Test acc:0.5011520737327189 Train loss:0.014187994413077831\n",
      "Epoch:871 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187203720211983\n",
      "Epoch:872 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187099412083626\n",
      "Epoch:873 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187008142471313\n",
      "Epoch:874 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014195790514349937\n",
      "Epoch:875 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418828684836626\n",
      "Epoch:876 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418687216937542\n",
      "Epoch:877 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01419117022305727\n",
      "Epoch:878 Train acc:0.4902534113060429 Test acc:0.4988479262672811 Train loss:0.01418828684836626\n",
      "Epoch:879 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418760884553194\n",
      "Epoch:880 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187326654791832\n",
      "Epoch:881 Train acc:0.48148148148148145 Test acc:0.5011520737327189 Train loss:0.014188108034431934\n",
      "Epoch:882 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189382083714008\n",
      "Epoch:883 Train acc:0.49122807017543857 Test acc:0.4988479262672811 Train loss:0.01419033482670784\n",
      "Epoch:884 Train acc:0.4766081871345029 Test acc:0.4988479262672811 Train loss:0.014189614914357662\n",
      "Epoch:885 Train acc:0.4902534113060429 Test acc:0.4988479262672811 Train loss:0.014187786728143692\n",
      "Epoch:886 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418748963624239\n",
      "Epoch:887 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418854296207428\n",
      "Epoch:888 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188144356012344\n",
      "Epoch:889 Train acc:0.5 Test acc:0.5011520737327189 Train loss:0.014186294749379158\n",
      "Epoch:890 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418897695839405\n",
      "Epoch:891 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014190721325576305\n",
      "Epoch:892 Train acc:0.49707602339181284 Test acc:0.4988479262672811 Train loss:0.014186516404151917\n",
      "Epoch:893 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014191757887601852\n",
      "Epoch:894 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014186992309987545\n",
      "Epoch:895 Train acc:0.4756335282651072 Test acc:0.5011520737327189 Train loss:0.01418782863765955\n",
      "Epoch:896 Train acc:0.48148148148148145 Test acc:0.5011520737327189 Train loss:0.01418890431523323\n",
      "Epoch:897 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187836088240147\n",
      "Epoch:898 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187609776854515\n",
      "Epoch:899 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418836135417223\n",
      "Epoch:900 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418973132967949\n",
      "Epoch:901 Train acc:0.4922027290448343 Test acc:0.4988479262672811 Train loss:0.014188006520271301\n",
      "Epoch:902 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014190612360835075\n",
      "Epoch:903 Train acc:0.5 Test acc:0.5011520737327189 Train loss:0.014186957851052284\n",
      "Epoch:904 Train acc:0.4834307992202729 Test acc:0.5011520737327189 Train loss:0.014187637716531754\n",
      "Epoch:905 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188944362103939\n",
      "Epoch:906 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014186537824571133\n",
      "Epoch:907 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014185622334480286\n",
      "Epoch:908 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189950190484524\n",
      "Epoch:909 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189057983458042\n",
      "Epoch:910 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188560657203197\n",
      "Epoch:911 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188450761139393\n",
      "Epoch:912 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188870787620544\n",
      "Epoch:913 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418781466782093\n",
      "Epoch:914 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187590219080448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:915 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187565073370934\n",
      "Epoch:916 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014186038635671139\n",
      "Epoch:917 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014186449348926544\n",
      "Epoch:918 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014184260740876198\n",
      "Epoch:919 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014185955747961998\n",
      "Epoch:920 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01419126521795988\n",
      "Epoch:921 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014195378869771957\n",
      "Epoch:922 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188197441399097\n",
      "Epoch:923 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187892898917198\n",
      "Epoch:924 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187587425112724\n",
      "Epoch:925 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188270084559917\n",
      "Epoch:926 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189238660037518\n",
      "Epoch:927 Train acc:0.47855750487329435 Test acc:0.5011520737327189 Train loss:0.01418854109942913\n",
      "Epoch:928 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014188410714268684\n",
      "Epoch:929 Train acc:0.5019493177387915 Test acc:0.4988479262672811 Train loss:0.014187974855303764\n",
      "Epoch:930 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418793573975563\n",
      "Epoch:931 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189239591360092\n",
      "Epoch:932 Train acc:0.5019493177387915 Test acc:0.5011520737327189 Train loss:0.014188449829816818\n",
      "Epoch:933 Train acc:0.50682261208577 Test acc:0.4988479262672811 Train loss:0.014187189750373363\n",
      "Epoch:934 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418939046561718\n",
      "Epoch:935 Train acc:0.4853801169590643 Test acc:0.5011520737327189 Train loss:0.014188932254910469\n",
      "Epoch:936 Train acc:0.4873294346978557 Test acc:0.4988479262672811 Train loss:0.014188586734235287\n",
      "Epoch:937 Train acc:0.4980506822612086 Test acc:0.5011520737327189 Train loss:0.014187944121658802\n",
      "Epoch:938 Train acc:0.48635477582846004 Test acc:0.5011520737327189 Train loss:0.014189191162586212\n",
      "Epoch:939 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187906868755817\n",
      "Epoch:940 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014190941117703915\n",
      "Epoch:941 Train acc:0.5029239766081871 Test acc:0.4988479262672811 Train loss:0.014187133871018887\n",
      "Epoch:942 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188198372721672\n",
      "Epoch:943 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014189914800226688\n",
      "Epoch:944 Train acc:0.4873294346978557 Test acc:0.5011520737327189 Train loss:0.014192200265824795\n",
      "Epoch:945 Train acc:0.4951267056530214 Test acc:0.4988479262672811 Train loss:0.01418586727231741\n",
      "Epoch:946 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418943703174591\n",
      "Epoch:947 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188160188496113\n",
      "Epoch:948 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014185406267642975\n",
      "Epoch:949 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188327826559544\n",
      "Epoch:950 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014192868955433369\n",
      "Epoch:951 Train acc:0.4668615984405458 Test acc:0.4988479262672811 Train loss:0.014190617948770523\n",
      "Epoch:952 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187715947628021\n",
      "Epoch:953 Train acc:0.4902534113060429 Test acc:0.4988479262672811 Train loss:0.014187087304890156\n",
      "Epoch:954 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014184287749230862\n",
      "Epoch:955 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014190257526934147\n",
      "Epoch:956 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014193438924849033\n",
      "Epoch:957 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188005588948727\n",
      "Epoch:958 Train acc:0.5 Test acc:0.5011520737327189 Train loss:0.014188193716108799\n",
      "Epoch:959 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014189845882356167\n",
      "Epoch:960 Train acc:0.5 Test acc:0.4988479262672811 Train loss:0.014187520369887352\n",
      "Epoch:961 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187420718371868\n",
      "Epoch:962 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188649132847786\n",
      "Epoch:963 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187444932758808\n",
      "Epoch:964 Train acc:0.49317738791423 Test acc:0.4988479262672811 Train loss:0.014188329689204693\n",
      "Epoch:965 Train acc:0.4834307992202729 Test acc:0.5011520737327189 Train loss:0.014188931323587894\n",
      "Epoch:966 Train acc:0.4980506822612086 Test acc:0.4988479262672811 Train loss:0.014188217930495739\n",
      "Epoch:967 Train acc:0.4902534113060429 Test acc:0.4988479262672811 Train loss:0.014187977649271488\n",
      "Epoch:968 Train acc:0.48830409356725146 Test acc:0.4988479262672811 Train loss:0.014186998829245567\n",
      "Epoch:969 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188846573233604\n",
      "Epoch:970 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014190461486577988\n",
      "Epoch:971 Train acc:0.47855750487329435 Test acc:0.4988479262672811 Train loss:0.01418865006417036\n",
      "Epoch:972 Train acc:0.49122807017543857 Test acc:0.5011520737327189 Train loss:0.014187340624630451\n",
      "Epoch:973 Train acc:0.4756335282651072 Test acc:0.5011520737327189 Train loss:0.014189750887453556\n",
      "Epoch:974 Train acc:0.48830409356725146 Test acc:0.4988479262672811 Train loss:0.014189576730132103\n",
      "Epoch:975 Train acc:0.48635477582846004 Test acc:0.5011520737327189 Train loss:0.014188496395945549\n",
      "Epoch:976 Train acc:0.5 Test acc:0.4988479262672811 Train loss:0.014187539927661419\n",
      "Epoch:977 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014191878028213978\n",
      "Epoch:978 Train acc:0.4922027290448343 Test acc:0.5011520737327189 Train loss:0.014187753200531006\n",
      "Epoch:979 Train acc:0.48148148148148145 Test acc:0.4988479262672811 Train loss:0.014188006520271301\n",
      "Epoch:980 Train acc:0.4775828460038986 Test acc:0.4988479262672811 Train loss:0.014188818633556366\n",
      "Epoch:981 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187823049724102\n",
      "Epoch:982 Train acc:0.4824561403508772 Test acc:0.5011520737327189 Train loss:0.014188130386173725\n",
      "Epoch:983 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014187371358275414\n",
      "Epoch:984 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.014190712943673134\n",
      "Epoch:985 Train acc:0.4756335282651072 Test acc:0.5011520737327189 Train loss:0.01418971549719572\n",
      "Epoch:986 Train acc:0.4824561403508772 Test acc:0.4988479262672811 Train loss:0.014188486151397228\n",
      "Epoch:987 Train acc:0.5058479532163743 Test acc:0.5011520737327189 Train loss:0.014189573936164379\n",
      "Epoch:988 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188168570399284\n",
      "Epoch:989 Train acc:0.4902534113060429 Test acc:0.4988479262672811 Train loss:0.014187326654791832\n",
      "Epoch:990 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.01418845821171999\n",
      "Epoch:991 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014185004867613316\n",
      "Epoch:992 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014187917113304138\n",
      "Epoch:993 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188011176884174\n",
      "Epoch:994 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014191449619829655\n",
      "Epoch:995 Train acc:0.5009746588693957 Test acc:0.4988479262672811 Train loss:0.014188391156494617\n",
      "Epoch:996 Train acc:0.48050682261208577 Test acc:0.4988479262672811 Train loss:0.014187776483595371\n",
      "Epoch:997 Train acc:0.49415204678362573 Test acc:0.4988479262672811 Train loss:0.01418785098940134\n",
      "Epoch:998 Train acc:0.4844054580896686 Test acc:0.5011520737327189 Train loss:0.014188197441399097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:999 Train acc:0.49902534113060426 Test acc:0.5011520737327189 Train loss:0.01418784074485302\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_test_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>█▇▅▆▆▁▅▄▆▅▅▆▆▆▆▆▆▆▅▂▅▅▆▅▅▆▄▆▆▃▄▇▅▆▅▁▅▆▄▆</td></tr><tr><td>train_loss</td><td>▁▇▆██▇▇▇▇▇▇███▇▇▇▆▇▇▇▇█▇▇▇▇▇▇▇▆▇▇▇▇▇▇█▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_test_accuracy</td><td>0.57488</td></tr><tr><td>test_accuracy</td><td>0.50115</td></tr><tr><td>train_accuracy</td><td>0.49903</td></tr><tr><td>train_loss</td><td>0.01419</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">self_loops=True_weighted=weighted_data=only_entropy</strong>: <a href=\"https://wandb.ai/dmasny/neuroimaging_gnn_eeg_final_project/runs/214306gt\" target=\"_blank\">https://wandb.ai/dmasny/neuroimaging_gnn_eeg_final_project/runs/214306gt</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221027_202041-214306gt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_loops=True_weighted=weighted_data=only_entropy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/notebooks/gnn_eeg/wandb/run-20221027_203548-2vl54knt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/dmasny/neuroimaging_gnn_eeg_final_project/runs/2vl54knt\" target=\"_blank\">self_loops=True_weighted=weighted_data=only_powerbands</a></strong> to <a href=\"https://wandb.ai/dmasny/neuroimaging_gnn_eeg_final_project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Train acc:0.5428849902534113 Test acc:0.4665898617511521 Train loss:0.014184386469423771\n",
      "Epoch:1 Train acc:0.5662768031189084 Test acc:0.4251152073732719 Train loss:0.013797205872833729\n",
      "Epoch:2 Train acc:0.601364522417154 Test acc:0.4147465437788018 Train loss:0.013724185526371002\n",
      "Epoch:3 Train acc:0.584307992202729 Test acc:0.41244239631336405 Train loss:0.013682049699127674\n",
      "Epoch:4 Train acc:0.604775828460039 Test acc:0.4216589861751152 Train loss:0.013269738294184208\n",
      "Epoch:5 Train acc:0.6174463937621832 Test acc:0.48963133640552997 Train loss:0.013063423335552216\n",
      "Epoch:6 Train acc:0.5555555555555556 Test acc:0.4205069124423963 Train loss:0.01390183623880148\n",
      "Epoch:7 Train acc:0.5896686159844055 Test acc:0.4423963133640553 Train loss:0.013311393558979034\n",
      "Epoch:8 Train acc:0.628167641325536 Test acc:0.42972350230414746 Train loss:0.01276925764977932\n",
      "Epoch:9 Train acc:0.6301169590643275 Test acc:0.4412442396313364 Train loss:0.012714765034615993\n",
      "Epoch:10 Train acc:0.6247563352826511 Test acc:0.43317972350230416 Train loss:0.012730089947581291\n",
      "Epoch:11 Train acc:0.6388888888888888 Test acc:0.478110599078341 Train loss:0.012801614589989185\n",
      "Epoch:12 Train acc:0.615009746588694 Test acc:0.4642857142857143 Train loss:0.012761340476572514\n",
      "Epoch:13 Train acc:0.6252436647173489 Test acc:0.4423963133640553 Train loss:0.012799516320228577\n",
      "Epoch:14 Train acc:0.6340155945419104 Test acc:0.4804147465437788 Train loss:0.012417959980666637\n",
      "Epoch:15 Train acc:0.6232943469785575 Test acc:0.4573732718894009 Train loss:0.012902667745947838\n",
      "Epoch:16 Train acc:0.6364522417153996 Test acc:0.44930875576036866 Train loss:0.01274982187896967\n",
      "Epoch:17 Train acc:0.6247563352826511 Test acc:0.45276497695852536 Train loss:0.01269597839564085\n",
      "Epoch:18 Train acc:0.6325536062378168 Test acc:0.4447004608294931 Train loss:0.01262302603572607\n",
      "Epoch:19 Train acc:0.6442495126705653 Test acc:0.4596774193548387 Train loss:0.01266584824770689\n",
      "Epoch:20 Train acc:0.628167641325536 Test acc:0.4792626728110599 Train loss:0.012334387749433517\n",
      "Epoch:21 Train acc:0.6340155945419104 Test acc:0.46543778801843316 Train loss:0.012497362680733204\n",
      "Epoch:22 Train acc:0.6262183235867447 Test acc:0.4827188940092166 Train loss:0.012461411766707897\n",
      "Epoch:23 Train acc:0.6442495126705653 Test acc:0.4804147465437788 Train loss:0.012235961854457855\n",
      "Epoch:24 Train acc:0.6505847953216374 Test acc:0.4470046082949309 Train loss:0.012329823337495327\n",
      "Epoch:25 Train acc:0.6535087719298246 Test acc:0.45276497695852536 Train loss:0.012349221855401993\n",
      "Epoch:26 Train acc:0.6340155945419104 Test acc:0.45161290322580644 Train loss:0.012506627477705479\n",
      "Epoch:27 Train acc:0.6539961013645225 Test acc:0.44815668202764974 Train loss:0.012474630028009415\n",
      "Epoch:28 Train acc:0.6252436647173489 Test acc:0.47119815668202764 Train loss:0.012486262246966362\n",
      "Epoch:29 Train acc:0.6179337231968811 Test acc:0.4873271889400922 Train loss:0.012849975377321243\n",
      "Epoch:30 Train acc:0.6359649122807017 Test acc:0.4976958525345622 Train loss:0.01229226402938366\n",
      "Epoch:31 Train acc:0.6364522417153996 Test acc:0.543778801843318 Train loss:0.0123420599848032\n",
      "Epoch:32 Train acc:0.6481481481481481 Test acc:0.511520737327189 Train loss:0.0122680040076375\n",
      "Epoch:33 Train acc:0.6379142300194932 Test acc:0.4804147465437788 Train loss:0.012364625930786133\n",
      "Epoch:34 Train acc:0.648635477582846 Test acc:0.5241935483870968 Train loss:0.012149185873568058\n",
      "Epoch:35 Train acc:0.6544834307992202 Test acc:0.46889400921658986 Train loss:0.012338717468082905\n",
      "Epoch:36 Train acc:0.6539961013645225 Test acc:0.5299539170506913 Train loss:0.012099338695406914\n",
      "Epoch:37 Train acc:0.6549707602339181 Test acc:0.511520737327189 Train loss:0.012106199748814106\n",
      "Epoch:38 Train acc:0.6539961013645225 Test acc:0.511520737327189 Train loss:0.012120178900659084\n",
      "Epoch:39 Train acc:0.652046783625731 Test acc:0.4735023041474654 Train loss:0.01181474793702364\n",
      "Epoch:40 Train acc:0.6574074074074074 Test acc:0.5241935483870968 Train loss:0.012156653217971325\n",
      "Epoch:41 Train acc:0.6466861598440545 Test acc:0.5069124423963134 Train loss:0.012125045992434025\n",
      "Epoch:42 Train acc:0.6613060428849903 Test acc:0.5149769585253456 Train loss:0.011961880140006542\n",
      "Epoch:43 Train acc:0.6603313840155945 Test acc:0.5138248847926268 Train loss:0.011949444189667702\n",
      "Epoch:44 Train acc:0.655458089668616 Test acc:0.5172811059907834 Train loss:0.01203620433807373\n",
      "Epoch:45 Train acc:0.6559454191033138 Test acc:0.5069124423963134 Train loss:0.011762543581426144\n",
      "Epoch:46 Train acc:0.6608187134502924 Test acc:0.48502304147465436 Train loss:0.011795809492468834\n",
      "Epoch:47 Train acc:0.6569200779727096 Test acc:0.4988479262672811 Train loss:0.011895135045051575\n",
      "Epoch:48 Train acc:0.6739766081871345 Test acc:0.48963133640552997 Train loss:0.011677457951009274\n",
      "Epoch:49 Train acc:0.6710526315789473 Test acc:0.478110599078341 Train loss:0.011749768629670143\n",
      "Epoch:50 Train acc:0.6661793372319688 Test acc:0.4792626728110599 Train loss:0.01164280530065298\n",
      "Epoch:51 Train acc:0.672514619883041 Test acc:0.4735023041474654 Train loss:0.011720712296664715\n",
      "Epoch:52 Train acc:0.6549707602339181 Test acc:0.4619815668202765 Train loss:0.01190111506730318\n",
      "Epoch:53 Train acc:0.6749512670565302 Test acc:0.5103686635944701 Train loss:0.011557663790881634\n",
      "Epoch:54 Train acc:0.678849902534113 Test acc:0.47235023041474655 Train loss:0.011611360125243664\n",
      "Epoch:55 Train acc:0.6778752436647173 Test acc:0.46889400921658986 Train loss:0.011712448671460152\n",
      "Epoch:56 Train acc:0.6715399610136452 Test acc:0.49539170506912444 Train loss:0.011697805486619473\n",
      "Epoch:57 Train acc:0.6769005847953217 Test acc:0.4988479262672811 Train loss:0.01158057525753975\n",
      "Epoch:58 Train acc:0.6773879142300195 Test acc:0.5195852534562212 Train loss:0.011517890729010105\n",
      "Epoch:59 Train acc:0.6710526315789473 Test acc:0.49539170506912444 Train loss:0.01199668925255537\n",
      "Epoch:60 Train acc:0.6539961013645225 Test acc:0.4861751152073733 Train loss:0.01199781708419323\n",
      "Epoch:61 Train acc:0.6866471734892787 Test acc:0.4930875576036866 Train loss:0.011614318005740643\n",
      "Epoch:62 Train acc:0.6593567251461988 Test acc:0.5495391705069125 Train loss:0.011813496239483356\n",
      "Epoch:63 Train acc:0.6798245614035088 Test acc:0.49078341013824883 Train loss:0.011645443737506866\n",
      "Epoch:64 Train acc:0.6866471734892787 Test acc:0.4619815668202765 Train loss:0.011554433032870293\n",
      "Epoch:65 Train acc:0.6944444444444444 Test acc:0.4804147465437788 Train loss:0.011403311975300312\n",
      "Epoch:66 Train acc:0.6764132553606238 Test acc:0.5011520737327189 Train loss:0.011679641902446747\n",
      "Epoch:67 Train acc:0.6895711500974658 Test acc:0.5253456221198156 Train loss:0.011293950490653515\n",
      "Epoch:68 Train acc:0.6876218323586745 Test acc:0.4815668202764977 Train loss:0.011452991515398026\n",
      "Epoch:69 Train acc:0.685672514619883 Test acc:0.5138248847926268 Train loss:0.011401036754250526\n",
      "Epoch:70 Train acc:0.6837231968810916 Test acc:0.49539170506912444 Train loss:0.011307095177471638\n",
      "Epoch:71 Train acc:0.6954191033138402 Test acc:0.4735023041474654 Train loss:0.011236043646931648\n",
      "Epoch:72 Train acc:0.699317738791423 Test acc:0.4861751152073733 Train loss:0.011312012560665607\n",
      "Epoch:73 Train acc:0.6968810916179338 Test acc:0.4942396313364055 Train loss:0.01132435817271471\n",
      "Epoch:74 Train acc:0.6968810916179338 Test acc:0.4631336405529954 Train loss:0.011355440132319927\n",
      "Epoch:75 Train acc:0.6988304093567251 Test acc:0.4804147465437788 Train loss:0.011293550953269005\n",
      "Epoch:76 Train acc:0.6954191033138402 Test acc:0.4608294930875576 Train loss:0.011354649439454079\n",
      "Epoch:77 Train acc:0.6954191033138402 Test acc:0.45852534562211983 Train loss:0.011329669505357742\n",
      "Epoch:78 Train acc:0.7139376218323586 Test acc:0.45276497695852536 Train loss:0.011067311279475689\n",
      "Epoch:79 Train acc:0.7056530214424951 Test acc:0.4619815668202765 Train loss:0.011108220554888248\n",
      "Epoch:80 Train acc:0.7071150097465887 Test acc:0.4504608294930876 Train loss:0.011123666539788246\n",
      "Epoch:81 Train acc:0.706140350877193 Test acc:0.4539170506912442 Train loss:0.010915940627455711\n",
      "Epoch:82 Train acc:0.7090643274853801 Test acc:0.45622119815668205 Train loss:0.01124673429876566\n",
      "Epoch:83 Train acc:0.7037037037037037 Test acc:0.4769585253456221 Train loss:0.011253557167947292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:84 Train acc:0.7022417153996101 Test acc:0.4470046082949309 Train loss:0.011042105033993721\n",
      "Epoch:85 Train acc:0.7100389863547758 Test acc:0.43663594470046085 Train loss:0.01115530263632536\n",
      "Epoch:86 Train acc:0.7100389863547758 Test acc:0.4873271889400922 Train loss:0.01087440736591816\n",
      "Epoch:87 Train acc:0.7217348927875243 Test acc:0.4665898617511521 Train loss:0.010896013118326664\n",
      "Epoch:88 Train acc:0.7212475633528265 Test acc:0.45852534562211983 Train loss:0.010767488740384579\n",
      "Epoch:89 Train acc:0.7090643274853801 Test acc:0.46774193548387094 Train loss:0.010933508165180683\n",
      "Epoch:90 Train acc:0.7066276803118908 Test acc:0.44585253456221197 Train loss:0.01126119215041399\n",
      "Epoch:91 Train acc:0.7188109161793372 Test acc:0.5092165898617511 Train loss:0.010539032518863678\n",
      "Epoch:92 Train acc:0.7124756335282652 Test acc:0.43663594470046085 Train loss:0.010899461805820465\n",
      "Epoch:93 Train acc:0.7076023391812866 Test acc:0.45852534562211983 Train loss:0.010473732836544514\n",
      "Epoch:94 Train acc:0.7300194931773879 Test acc:0.48963133640552997 Train loss:0.010371891781687737\n",
      "Epoch:95 Train acc:0.7270955165692008 Test acc:0.4470046082949309 Train loss:0.010473791509866714\n",
      "Epoch:96 Train acc:0.719785575048733 Test acc:0.511520737327189 Train loss:0.010954599827528\n",
      "Epoch:97 Train acc:0.7139376218323586 Test acc:0.4930875576036866 Train loss:0.010801034048199654\n",
      "Epoch:98 Train acc:0.7300194931773879 Test acc:0.47465437788018433 Train loss:0.010479649528861046\n",
      "Epoch:99 Train acc:0.7202729044834308 Test acc:0.4700460829493088 Train loss:0.010567795485258102\n",
      "Epoch:100 Train acc:0.7314814814814815 Test acc:0.5 Train loss:0.010545497760176659\n",
      "Epoch:101 Train acc:0.7256335282651072 Test acc:0.49078341013824883 Train loss:0.010637507773935795\n",
      "Epoch:102 Train acc:0.7368421052631579 Test acc:0.4665898617511521 Train loss:0.010415632277727127\n",
      "Epoch:103 Train acc:0.7412280701754386 Test acc:0.47465437788018433 Train loss:0.010548558086156845\n",
      "Epoch:104 Train acc:0.7246588693957114 Test acc:0.45852534562211983 Train loss:0.010662946850061417\n",
      "Epoch:105 Train acc:0.7285575048732943 Test acc:0.44930875576036866 Train loss:0.010584494099020958\n",
      "Epoch:106 Train acc:0.7261208576998051 Test acc:0.478110599078341 Train loss:0.010338294319808483\n",
      "Epoch:107 Train acc:0.7270955165692008 Test acc:0.48502304147465436 Train loss:0.010197642259299755\n",
      "Epoch:108 Train acc:0.7441520467836257 Test acc:0.4735023041474654 Train loss:0.010381962172687054\n",
      "Epoch:109 Train acc:0.7149122807017544 Test acc:0.4423963133640553 Train loss:0.010496146976947784\n",
      "Epoch:110 Train acc:0.7456140350877193 Test acc:0.4539170506912442 Train loss:0.010020731948316097\n",
      "Epoch:111 Train acc:0.7470760233918129 Test acc:0.4470046082949309 Train loss:0.009942459873855114\n",
      "Epoch:112 Train acc:0.7529239766081871 Test acc:0.45852534562211983 Train loss:0.010254092514514923\n",
      "Epoch:113 Train acc:0.7153996101364523 Test acc:0.48963133640552997 Train loss:0.011040791869163513\n",
      "Epoch:114 Train acc:0.7426900584795322 Test acc:0.4504608294930876 Train loss:0.010202365927398205\n",
      "Epoch:115 Train acc:0.7397660818713451 Test acc:0.43663594470046085 Train loss:0.010373825207352638\n",
      "Epoch:116 Train acc:0.7446393762183235 Test acc:0.41359447004608296 Train loss:0.010251195169985294\n",
      "Epoch:117 Train acc:0.7446393762183235 Test acc:0.47465437788018433 Train loss:0.010216978378593922\n",
      "Epoch:118 Train acc:0.7495126705653021 Test acc:0.4400921658986175 Train loss:0.009970628656446934\n",
      "Epoch:119 Train acc:0.7470760233918129 Test acc:0.48847926267281105 Train loss:0.010161207988858223\n",
      "Epoch:120 Train acc:0.7392787524366472 Test acc:0.44930875576036866 Train loss:0.010064433328807354\n",
      "Epoch:121 Train acc:0.7504873294346979 Test acc:0.4827188940092166 Train loss:0.0098884841427207\n",
      "Epoch:122 Train acc:0.7524366471734892 Test acc:0.4642857142857143 Train loss:0.009756213054060936\n",
      "Epoch:123 Train acc:0.7602339181286549 Test acc:0.45622119815668205 Train loss:0.009525050409138203\n",
      "Epoch:124 Train acc:0.7641325536062378 Test acc:0.4631336405529954 Train loss:0.00935346633195877\n",
      "Epoch:125 Train acc:0.7690058479532164 Test acc:0.46774193548387094 Train loss:0.0093915443867445\n",
      "Epoch:126 Train acc:0.7626705653021443 Test acc:0.4642857142857143 Train loss:0.009395208209753036\n",
      "Epoch:127 Train acc:0.77046783625731 Test acc:0.4700460829493088 Train loss:0.009627589955925941\n",
      "Epoch:128 Train acc:0.75682261208577 Test acc:0.4827188940092166 Train loss:0.009830129332840443\n",
      "Epoch:129 Train acc:0.7597465886939572 Test acc:0.4700460829493088 Train loss:0.009755539707839489\n",
      "Epoch:130 Train acc:0.7597465886939572 Test acc:0.4400921658986175 Train loss:0.009881306439638138\n",
      "Epoch:131 Train acc:0.7743664717348928 Test acc:0.4470046082949309 Train loss:0.009610326960682869\n",
      "Epoch:132 Train acc:0.7538986354775828 Test acc:0.423963133640553 Train loss:0.010205050930380821\n",
      "Epoch:133 Train acc:0.7202729044834308 Test acc:0.4423963133640553 Train loss:0.010077918879687786\n",
      "Epoch:134 Train acc:0.7612085769980507 Test acc:0.47465437788018433 Train loss:0.009403010830283165\n",
      "Epoch:135 Train acc:0.7685185185185185 Test acc:0.46543778801843316 Train loss:0.009216923266649246\n",
      "Epoch:136 Train acc:0.7729044834307992 Test acc:0.42972350230414746 Train loss:0.009294101968407631\n",
      "Epoch:137 Train acc:0.7626705653021443 Test acc:0.4608294930875576 Train loss:0.00964840967208147\n",
      "Epoch:138 Train acc:0.7724171539961013 Test acc:0.45161290322580644 Train loss:0.009210072457790375\n",
      "Epoch:139 Train acc:0.7738791423001949 Test acc:0.48847926267281105 Train loss:0.009069748222827911\n",
      "Epoch:140 Train acc:0.7807017543859649 Test acc:0.43317972350230416 Train loss:0.008751099929213524\n",
      "Epoch:141 Train acc:0.7797270955165692 Test acc:0.4700460829493088 Train loss:0.008846630342304707\n",
      "Epoch:142 Train acc:0.767056530214425 Test acc:0.43894009216589863 Train loss:0.009248086251318455\n",
      "Epoch:143 Train acc:0.7792397660818714 Test acc:0.478110599078341 Train loss:0.008984209969639778\n",
      "Epoch:144 Train acc:0.7777777777777778 Test acc:0.4608294930875576 Train loss:0.008819996379315853\n",
      "Epoch:145 Train acc:0.7870370370370371 Test acc:0.4930875576036866 Train loss:0.008742175996303558\n",
      "Epoch:146 Train acc:0.7821637426900585 Test acc:0.4769585253456221 Train loss:0.009161707945168018\n",
      "Epoch:147 Train acc:0.7733918128654971 Test acc:0.49193548387096775 Train loss:0.009139454923570156\n",
      "Epoch:148 Train acc:0.7777777777777778 Test acc:0.49193548387096775 Train loss:0.009011372923851013\n",
      "Epoch:149 Train acc:0.783625730994152 Test acc:0.5 Train loss:0.008865929208695889\n",
      "Epoch:150 Train acc:0.7821637426900585 Test acc:0.4400921658986175 Train loss:0.00899274181574583\n",
      "Epoch:151 Train acc:0.7889863547758285 Test acc:0.46889400921658986 Train loss:0.008943155407905579\n",
      "Epoch:152 Train acc:0.783625730994152 Test acc:0.42972350230414746 Train loss:0.009006555192172527\n",
      "Epoch:153 Train acc:0.7977582846003899 Test acc:0.45852534562211983 Train loss:0.008434479124844074\n",
      "Epoch:154 Train acc:0.8060428849902534 Test acc:0.44815668202764974 Train loss:0.008205436170101166\n",
      "Epoch:155 Train acc:0.7904483430799221 Test acc:0.47235023041474655 Train loss:0.00933822151273489\n",
      "Epoch:156 Train acc:0.7616959064327485 Test acc:0.43894009216589863 Train loss:0.009729946032166481\n",
      "Epoch:157 Train acc:0.7997076023391813 Test acc:0.4573732718894009 Train loss:0.008708464913070202\n",
      "Epoch:158 Train acc:0.7899610136452242 Test acc:0.49078341013824883 Train loss:0.008540192618966103\n",
      "Epoch:159 Train acc:0.7933723196881092 Test acc:0.4930875576036866 Train loss:0.008418334648013115\n",
      "Epoch:160 Train acc:0.793859649122807 Test acc:0.4804147465437788 Train loss:0.009116243571043015\n",
      "Epoch:161 Train acc:0.7992202729044834 Test acc:0.47235023041474655 Train loss:0.008299852721393108\n",
      "Epoch:162 Train acc:0.800682261208577 Test acc:0.49193548387096775 Train loss:0.008127319626510143\n",
      "Epoch:163 Train acc:0.804093567251462 Test acc:0.44585253456221197 Train loss:0.008279803209006786\n",
      "Epoch:164 Train acc:0.804093567251462 Test acc:0.478110599078341 Train loss:0.008580784313380718\n",
      "Epoch:165 Train acc:0.8084795321637427 Test acc:0.4642857142857143 Train loss:0.008087895810604095\n",
      "Epoch:166 Train acc:0.7738791423001949 Test acc:0.42626728110599077 Train loss:0.00906890444457531\n",
      "Epoch:167 Train acc:0.7948343079922028 Test acc:0.4827188940092166 Train loss:0.008171948604285717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:168 Train acc:0.8089668615984406 Test acc:0.4216589861751152 Train loss:0.008005909621715546\n",
      "Epoch:169 Train acc:0.8118908382066277 Test acc:0.47235023041474655 Train loss:0.007823743857443333\n",
      "Epoch:170 Train acc:0.814327485380117 Test acc:0.4804147465437788 Train loss:0.008076589554548264\n",
      "Epoch:171 Train acc:0.804093567251462 Test acc:0.4815668202764977 Train loss:0.00836227647960186\n",
      "Epoch:172 Train acc:0.8133528265107213 Test acc:0.48502304147465436 Train loss:0.008077058009803295\n",
      "Epoch:173 Train acc:0.7846003898635477 Test acc:0.4642857142857143 Train loss:0.009167632088065147\n",
      "Epoch:174 Train acc:0.7860623781676414 Test acc:0.47465437788018433 Train loss:0.008931182324886322\n",
      "Epoch:175 Train acc:0.8128654970760234 Test acc:0.45622119815668205 Train loss:0.007892485707998276\n",
      "Epoch:176 Train acc:0.8240740740740741 Test acc:0.45622119815668205 Train loss:0.007877664640545845\n",
      "Epoch:177 Train acc:0.8089668615984406 Test acc:0.44930875576036866 Train loss:0.00826746691018343\n",
      "Epoch:178 Train acc:0.8118908382066277 Test acc:0.4182027649769585 Train loss:0.007758900988847017\n",
      "Epoch:179 Train acc:0.8318713450292398 Test acc:0.4930875576036866 Train loss:0.007444005459547043\n",
      "Epoch:180 Train acc:0.8182261208576999 Test acc:0.4735023041474654 Train loss:0.0076799336820840836\n",
      "Epoch:181 Train acc:0.8265107212475633 Test acc:0.434331797235023 Train loss:0.007683482486754656\n",
      "Epoch:182 Train acc:0.8250487329434698 Test acc:0.4573732718894009 Train loss:0.0074440413154661655\n",
      "Epoch:183 Train acc:0.8294346978557505 Test acc:0.43894009216589863 Train loss:0.007470936980098486\n",
      "Epoch:184 Train acc:0.8240740740740741 Test acc:0.4804147465437788 Train loss:0.008166208863258362\n",
      "Epoch:185 Train acc:0.8099415204678363 Test acc:0.4735023041474654 Train loss:0.008030029945075512\n",
      "Epoch:186 Train acc:0.8318713450292398 Test acc:0.4631336405529954 Train loss:0.007375509012490511\n",
      "Epoch:187 Train acc:0.8386939571150097 Test acc:0.48847926267281105 Train loss:0.007521276827901602\n",
      "Epoch:188 Train acc:0.8318713450292398 Test acc:0.4769585253456221 Train loss:0.007268229499459267\n",
      "Epoch:189 Train acc:0.8421052631578947 Test acc:0.46889400921658986 Train loss:0.006949239410459995\n",
      "Epoch:190 Train acc:0.854775828460039 Test acc:0.4619815668202765 Train loss:0.006877052132040262\n",
      "Epoch:191 Train acc:0.8226120857699805 Test acc:0.4631336405529954 Train loss:0.007564274128526449\n",
      "Epoch:192 Train acc:0.8411306042884991 Test acc:0.4596774193548387 Train loss:0.007073099724948406\n",
      "Epoch:193 Train acc:0.8450292397660819 Test acc:0.4804147465437788 Train loss:0.00694281654432416\n",
      "Epoch:194 Train acc:0.8489278752436648 Test acc:0.4792626728110599 Train loss:0.006828823126852512\n",
      "Epoch:195 Train acc:0.8503898635477583 Test acc:0.4735023041474654 Train loss:0.006772845052182674\n",
      "Epoch:196 Train acc:0.8484405458089669 Test acc:0.4930875576036866 Train loss:0.006970516871660948\n",
      "Epoch:197 Train acc:0.8576998050682261 Test acc:0.45276497695852536 Train loss:0.006651617586612701\n",
      "Epoch:198 Train acc:0.8474658869395711 Test acc:0.5034562211981567 Train loss:0.006820001173764467\n",
      "Epoch:199 Train acc:0.8347953216374269 Test acc:0.4608294930875576 Train loss:0.0076950532384216785\n",
      "Epoch:200 Train acc:0.7631578947368421 Test acc:0.4965437788018433 Train loss:0.010194236412644386\n",
      "Epoch:201 Train acc:0.7919103313840156 Test acc:0.4435483870967742 Train loss:0.009045582264661789\n",
      "Epoch:202 Train acc:0.8216374269005848 Test acc:0.45622119815668205 Train loss:0.007530788891017437\n",
      "Epoch:203 Train acc:0.8450292397660819 Test acc:0.44815668202764974 Train loss:0.006925845518708229\n",
      "Epoch:204 Train acc:0.8328460038986355 Test acc:0.4735023041474654 Train loss:0.007102185860276222\n",
      "Epoch:205 Train acc:0.8518518518518519 Test acc:0.4274193548387097 Train loss:0.006703447084873915\n",
      "Epoch:206 Train acc:0.8606237816764133 Test acc:0.48502304147465436 Train loss:0.006501998286694288\n",
      "Epoch:207 Train acc:0.8601364522417154 Test acc:0.5034562211981567 Train loss:0.006258740555495024\n",
      "Epoch:208 Train acc:0.8489278752436648 Test acc:0.47465437788018433 Train loss:0.0065795728005468845\n",
      "Epoch:209 Train acc:0.8596491228070176 Test acc:0.45622119815668205 Train loss:0.00650067301467061\n",
      "Epoch:210 Train acc:0.8669590643274854 Test acc:0.48963133640552997 Train loss:0.006090837996453047\n",
      "Epoch:211 Train acc:0.8679337231968811 Test acc:0.5 Train loss:0.0062099359929561615\n",
      "Epoch:212 Train acc:0.8250487329434698 Test acc:0.49078341013824883 Train loss:0.008456571027636528\n",
      "Epoch:213 Train acc:0.8508771929824561 Test acc:0.47465437788018433 Train loss:0.006546535529196262\n",
      "Epoch:214 Train acc:0.8776803118908382 Test acc:0.4642857142857143 Train loss:0.006139054894447327\n",
      "Epoch:215 Train acc:0.8669590643274854 Test acc:0.5011520737327189 Train loss:0.006482837256044149\n",
      "Epoch:216 Train acc:0.8484405458089669 Test acc:0.4873271889400922 Train loss:0.006733637303113937\n",
      "Epoch:217 Train acc:0.8757309941520468 Test acc:0.4608294930875576 Train loss:0.005806585308164358\n",
      "Epoch:218 Train acc:0.8698830409356725 Test acc:0.4423963133640553 Train loss:0.006073434371501207\n",
      "Epoch:219 Train acc:0.865009746588694 Test acc:0.44930875576036866 Train loss:0.006116162519901991\n",
      "Epoch:220 Train acc:0.8742690058479532 Test acc:0.4435483870967742 Train loss:0.005940435919910669\n",
      "Epoch:221 Train acc:0.8723196881091618 Test acc:0.4700460829493088 Train loss:0.006157776806503534\n",
      "Epoch:222 Train acc:0.8669590643274854 Test acc:0.44585253456221197 Train loss:0.0059433490969240665\n",
      "Epoch:223 Train acc:0.8845029239766082 Test acc:0.4792626728110599 Train loss:0.005563466344028711\n",
      "Epoch:224 Train acc:0.8752436647173489 Test acc:0.4573732718894009 Train loss:0.005655498243868351\n",
      "Epoch:225 Train acc:0.8830409356725146 Test acc:0.4827188940092166 Train loss:0.005467765498906374\n",
      "Epoch:226 Train acc:0.8567251461988304 Test acc:0.4470046082949309 Train loss:0.005937837529927492\n",
      "Epoch:227 Train acc:0.8884015594541911 Test acc:0.4976958525345622 Train loss:0.005401680711656809\n",
      "Epoch:228 Train acc:0.8942495126705653 Test acc:0.4608294930875576 Train loss:0.005296717397868633\n",
      "Epoch:229 Train acc:0.8932748538011696 Test acc:0.5011520737327189 Train loss:0.005272458307445049\n",
      "Epoch:230 Train acc:0.8888888888888888 Test acc:0.4642857142857143 Train loss:0.00546096870675683\n",
      "Epoch:231 Train acc:0.8820662768031189 Test acc:0.45276497695852536 Train loss:0.005717227701097727\n",
      "Epoch:232 Train acc:0.8606237816764133 Test acc:0.4827188940092166 Train loss:0.006298000458627939\n",
      "Epoch:233 Train acc:0.8435672514619883 Test acc:0.4735023041474654 Train loss:0.006658103317022324\n",
      "Epoch:234 Train acc:0.8776803118908382 Test acc:0.4700460829493088 Train loss:0.00575694115832448\n",
      "Epoch:235 Train acc:0.8635477582846004 Test acc:0.4792626728110599 Train loss:0.006214286666363478\n",
      "Epoch:236 Train acc:0.8845029239766082 Test acc:0.46543778801843316 Train loss:0.0054843914695084095\n",
      "Epoch:237 Train acc:0.8971734892787524 Test acc:0.4573732718894009 Train loss:0.005178578197956085\n",
      "Epoch:238 Train acc:0.902046783625731 Test acc:0.4700460829493088 Train loss:0.004783795680850744\n",
      "Epoch:239 Train acc:0.9074074074074074 Test acc:0.4700460829493088 Train loss:0.005054027773439884\n",
      "Epoch:240 Train acc:0.881578947368421 Test acc:0.44815668202764974 Train loss:0.005689430516213179\n",
      "Epoch:241 Train acc:0.8957115009746589 Test acc:0.4573732718894009 Train loss:0.005078477784991264\n",
      "Epoch:242 Train acc:0.8898635477582846 Test acc:0.4619815668202765 Train loss:0.00541019719094038\n",
      "Epoch:243 Train acc:0.8923001949317739 Test acc:0.4988479262672811 Train loss:0.0051613738760352135\n",
      "Epoch:244 Train acc:0.8693957115009746 Test acc:0.4619815668202765 Train loss:0.006239098031073809\n",
      "Epoch:245 Train acc:0.8879142300194932 Test acc:0.4861751152073733 Train loss:0.005135510582476854\n",
      "Epoch:246 Train acc:0.9049707602339181 Test acc:0.4988479262672811 Train loss:0.005094332154840231\n",
      "Epoch:247 Train acc:0.871832358674464 Test acc:0.478110599078341 Train loss:0.005813864059746265\n",
      "Epoch:248 Train acc:0.9010721247563352 Test acc:0.4573732718894009 Train loss:0.0049689533188939095\n",
      "Epoch:249 Train acc:0.8981481481481481 Test acc:0.46543778801843316 Train loss:0.005078085698187351\n",
      "Epoch:250 Train acc:0.9030214424951267 Test acc:0.44930875576036866 Train loss:0.004712407011538744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:251 Train acc:0.8966861598440545 Test acc:0.4700460829493088 Train loss:0.004721750505268574\n",
      "Epoch:252 Train acc:0.922514619883041 Test acc:0.47465437788018433 Train loss:0.004250993020832539\n",
      "Epoch:253 Train acc:0.9064327485380117 Test acc:0.48502304147465436 Train loss:0.004585830494761467\n",
      "Epoch:254 Train acc:0.8942495126705653 Test acc:0.45622119815668205 Train loss:0.005072996485978365\n",
      "Epoch:255 Train acc:0.9078947368421053 Test acc:0.4596774193548387 Train loss:0.004597974009811878\n",
      "Epoch:256 Train acc:0.9117933723196882 Test acc:0.48847926267281105 Train loss:0.004388578236103058\n",
      "Epoch:257 Train acc:0.9137426900584795 Test acc:0.4504608294930876 Train loss:0.004338076803833246\n",
      "Epoch:258 Train acc:0.9093567251461988 Test acc:0.4735023041474654 Train loss:0.005096870474517345\n",
      "Epoch:259 Train acc:0.8762183235867447 Test acc:0.45852534562211983 Train loss:0.006569830700755119\n",
      "Epoch:260 Train acc:0.8771929824561403 Test acc:0.48847926267281105 Train loss:0.005446721334010363\n",
      "Epoch:261 Train acc:0.9259259259259259 Test acc:0.46774193548387094 Train loss:0.0041539366357028484\n",
      "Epoch:262 Train acc:0.9210526315789473 Test acc:0.47580645161290325 Train loss:0.004008755553513765\n",
      "Epoch:263 Train acc:0.9264132553606238 Test acc:0.4377880184331797 Train loss:0.003889713902026415\n",
      "Epoch:264 Train acc:0.9234892787524367 Test acc:0.4631336405529954 Train loss:0.003890949534252286\n",
      "Epoch:265 Train acc:0.9293372319688109 Test acc:0.46889400921658986 Train loss:0.0036701506469398737\n",
      "Epoch:266 Train acc:0.922514619883041 Test acc:0.49193548387096775 Train loss:0.0038533234037458897\n",
      "Epoch:267 Train acc:0.9230019493177388 Test acc:0.5 Train loss:0.003741023363545537\n",
      "Epoch:268 Train acc:0.9303118908382066 Test acc:0.45506912442396313 Train loss:0.0036000118125230074\n",
      "Epoch:269 Train acc:0.928849902534113 Test acc:0.4631336405529954 Train loss:0.003643491305410862\n",
      "Epoch:270 Train acc:0.9332358674463938 Test acc:0.4642857142857143 Train loss:0.0036225684452801943\n",
      "Epoch:271 Train acc:0.9220272904483431 Test acc:0.49193548387096775 Train loss:0.0038773026317358017\n",
      "Epoch:272 Train acc:0.9317738791423001 Test acc:0.4815668202764977 Train loss:0.0035741731990128756\n",
      "Epoch:273 Train acc:0.9259259259259259 Test acc:0.49193548387096775 Train loss:0.00393250398337841\n",
      "Epoch:274 Train acc:0.9278752436647173 Test acc:0.4804147465437788 Train loss:0.004123961552977562\n",
      "Epoch:275 Train acc:0.8747563352826511 Test acc:0.4930875576036866 Train loss:0.005957894492894411\n",
      "Epoch:276 Train acc:0.9035087719298246 Test acc:0.46543778801843316 Train loss:0.004578877240419388\n",
      "Epoch:277 Train acc:0.9186159844054581 Test acc:0.45852534562211983 Train loss:0.003850481705740094\n",
      "Epoch:278 Train acc:0.9278752436647173 Test acc:0.47465437788018433 Train loss:0.0035430225543677807\n",
      "Epoch:279 Train acc:0.9366471734892787 Test acc:0.4642857142857143 Train loss:0.003408034797757864\n",
      "Epoch:280 Train acc:0.9346978557504874 Test acc:0.45276497695852536 Train loss:0.003226631786674261\n",
      "Epoch:281 Train acc:0.9371345029239766 Test acc:0.4539170506912442 Train loss:0.0034889932721853256\n",
      "Epoch:282 Train acc:0.9293372319688109 Test acc:0.47119815668202764 Train loss:0.0035323009360581636\n",
      "Epoch:283 Train acc:0.932261208576998 Test acc:0.5046082949308756 Train loss:0.0036258262116461992\n",
      "Epoch:284 Train acc:0.9415204678362573 Test acc:0.4769585253456221 Train loss:0.003241932485252619\n",
      "Epoch:285 Train acc:0.9429824561403509 Test acc:0.4400921658986175 Train loss:0.0030860237311571836\n",
      "Epoch:286 Train acc:0.952729044834308 Test acc:0.4827188940092166 Train loss:0.0029361448250710964\n",
      "Epoch:287 Train acc:0.9415204678362573 Test acc:0.5288018433179723 Train loss:0.003247537650167942\n",
      "Epoch:288 Train acc:0.9342105263157895 Test acc:0.4792626728110599 Train loss:0.0036268592812120914\n",
      "Epoch:289 Train acc:0.9195906432748538 Test acc:0.4804147465437788 Train loss:0.004116827622056007\n",
      "Epoch:290 Train acc:0.9239766081871345 Test acc:0.49539170506912444 Train loss:0.0037339734844863415\n",
      "Epoch:291 Train acc:0.949317738791423 Test acc:0.48502304147465436 Train loss:0.0031464339699596167\n",
      "Epoch:292 Train acc:0.9317738791423001 Test acc:0.4861751152073733 Train loss:0.003475264413282275\n",
      "Epoch:293 Train acc:0.9342105263157895 Test acc:0.4861751152073733 Train loss:0.0034108427353203297\n",
      "Epoch:294 Train acc:0.9434697855750487 Test acc:0.48502304147465436 Train loss:0.002979972632601857\n",
      "Epoch:295 Train acc:0.9429824561403509 Test acc:0.47465437788018433 Train loss:0.0029949478339403868\n",
      "Epoch:296 Train acc:0.9488304093567251 Test acc:0.45622119815668205 Train loss:0.0027768309228122234\n",
      "Epoch:297 Train acc:0.9537037037037037 Test acc:0.4930875576036866 Train loss:0.0026005490217357874\n",
      "Epoch:298 Train acc:0.9390838206627681 Test acc:0.45852534562211983 Train loss:0.003125737654045224\n",
      "Epoch:299 Train acc:0.9410331384015594 Test acc:0.4861751152073733 Train loss:0.0031155196484178305\n",
      "Epoch:300 Train acc:0.9137426900584795 Test acc:0.5023041474654378 Train loss:0.004427141975611448\n",
      "Epoch:301 Train acc:0.9483430799220273 Test acc:0.47465437788018433 Train loss:0.0027883672155439854\n",
      "Epoch:302 Train acc:0.9502923976608187 Test acc:0.478110599078341 Train loss:0.002686154330149293\n",
      "Epoch:303 Train acc:0.9395711500974658 Test acc:0.4827188940092166 Train loss:0.0030631201807409525\n",
      "Epoch:304 Train acc:0.9541910331384016 Test acc:0.4504608294930876 Train loss:0.002560820896178484\n",
      "Epoch:305 Train acc:0.956140350877193 Test acc:0.4827188940092166 Train loss:0.002585639012977481\n",
      "Epoch:306 Train acc:0.9576023391812866 Test acc:0.48963133640552997 Train loss:0.0025899410247802734\n",
      "Epoch:307 Train acc:0.952729044834308 Test acc:0.4573732718894009 Train loss:0.002504104981198907\n",
      "Epoch:308 Train acc:0.9615009746588694 Test acc:0.4700460829493088 Train loss:0.0023243112955242395\n",
      "Epoch:309 Train acc:0.9639376218323586 Test acc:0.4665898617511521 Train loss:0.002207972342148423\n",
      "Epoch:310 Train acc:0.9585769980506823 Test acc:0.4769585253456221 Train loss:0.00224096211604774\n",
      "Epoch:311 Train acc:0.9629629629629629 Test acc:0.43663594470046085 Train loss:0.002330211689695716\n",
      "Epoch:312 Train acc:0.9351851851851852 Test acc:0.45276497695852536 Train loss:0.003654178697615862\n",
      "Epoch:313 Train acc:0.945906432748538 Test acc:0.48502304147465436 Train loss:0.003084770869463682\n",
      "Epoch:314 Train acc:0.8927875243664717 Test acc:0.478110599078341 Train loss:0.005096422042697668\n",
      "Epoch:315 Train acc:0.9264132553606238 Test acc:0.47465437788018433 Train loss:0.003854194888845086\n",
      "Epoch:316 Train acc:0.9551656920077972 Test acc:0.4988479262672811 Train loss:0.00245564803481102\n",
      "Epoch:317 Train acc:0.9532163742690059 Test acc:0.46543778801843316 Train loss:0.0027672273572534323\n",
      "Epoch:318 Train acc:0.949317738791423 Test acc:0.5046082949308756 Train loss:0.00275099603459239\n",
      "Epoch:319 Train acc:0.9551656920077972 Test acc:0.4700460829493088 Train loss:0.0025901233311742544\n",
      "Epoch:320 Train acc:0.9576023391812866 Test acc:0.49539170506912444 Train loss:0.002343050902709365\n",
      "Epoch:321 Train acc:0.9668615984405458 Test acc:0.4861751152073733 Train loss:0.0021311501041054726\n",
      "Epoch:322 Train acc:0.9546783625730995 Test acc:0.4631336405529954 Train loss:0.0023845017421990633\n",
      "Epoch:323 Train acc:0.9658869395711501 Test acc:0.5023041474654378 Train loss:0.001956482883542776\n",
      "Epoch:324 Train acc:0.9649122807017544 Test acc:0.4504608294930876 Train loss:0.0021039750427007675\n",
      "Epoch:325 Train acc:0.9517543859649122 Test acc:0.46889400921658986 Train loss:0.002433606656268239\n",
      "Epoch:326 Train acc:0.9585769980506823 Test acc:0.4804147465437788 Train loss:0.0022175691556185484\n",
      "Epoch:327 Train acc:0.9692982456140351 Test acc:0.4827188940092166 Train loss:0.0021064328029751778\n",
      "Epoch:328 Train acc:0.9434697855750487 Test acc:0.4608294930875576 Train loss:0.002850927412509918\n",
      "Epoch:329 Train acc:0.9673489278752436 Test acc:0.47580645161290325 Train loss:0.002230235608294606\n",
      "Epoch:330 Train acc:0.9610136452241715 Test acc:0.4665898617511521 Train loss:0.002088701818138361\n",
      "Epoch:331 Train acc:0.9756335282651072 Test acc:0.46774193548387094 Train loss:0.0016701736021786928\n",
      "Epoch:332 Train acc:0.9639376218323586 Test acc:0.47580645161290325 Train loss:0.0018871966749429703\n",
      "Epoch:333 Train acc:0.9727095516569201 Test acc:0.47119815668202764 Train loss:0.0018313248874619603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:334 Train acc:0.9615009746588694 Test acc:0.4769585253456221 Train loss:0.0019236438674852252\n",
      "Epoch:335 Train acc:0.9712475633528265 Test acc:0.4815668202764977 Train loss:0.0019237229134887457\n",
      "Epoch:336 Train acc:0.9751461988304093 Test acc:0.4608294930875576 Train loss:0.001654534018598497\n",
      "Epoch:337 Train acc:0.9702729044834308 Test acc:0.4642857142857143 Train loss:0.0017411510925740004\n",
      "Epoch:338 Train acc:0.9780701754385965 Test acc:0.48847926267281105 Train loss:0.001587913022376597\n",
      "Epoch:339 Train acc:0.982943469785575 Test acc:0.4815668202764977 Train loss:0.0013652225024998188\n",
      "Epoch:340 Train acc:0.9751461988304093 Test acc:0.45622119815668205 Train loss:0.0015946993371471763\n",
      "Epoch:341 Train acc:0.9702729044834308 Test acc:0.478110599078341 Train loss:0.0016276683891192079\n",
      "Epoch:342 Train acc:0.9824561403508771 Test acc:0.4861751152073733 Train loss:0.0013597295619547367\n",
      "Epoch:343 Train acc:0.9766081871345029 Test acc:0.4769585253456221 Train loss:0.0015141029143705964\n",
      "Epoch:344 Train acc:0.9668615984405458 Test acc:0.5011520737327189 Train loss:0.002145726466551423\n",
      "Epoch:345 Train acc:0.9342105263157895 Test acc:0.47580645161290325 Train loss:0.0033557405695319176\n",
      "Epoch:346 Train acc:0.9361598440545809 Test acc:0.45161290322580644 Train loss:0.0035634771920740604\n",
      "Epoch:347 Train acc:0.9580896686159844 Test acc:0.47465437788018433 Train loss:0.0025894753634929657\n",
      "Epoch:348 Train acc:0.9044834307992202 Test acc:0.5023041474654378 Train loss:0.007329440210014582\n",
      "Epoch:349 Train acc:0.9161793372319688 Test acc:0.5011520737327189 Train loss:0.004163842182606459\n",
      "Epoch:350 Train acc:0.9692982456140351 Test acc:0.4769585253456221 Train loss:0.0019775712862610817\n",
      "Epoch:351 Train acc:0.9546783625730995 Test acc:0.5011520737327189 Train loss:0.0024149511009454727\n",
      "Epoch:352 Train acc:0.9551656920077972 Test acc:0.47580645161290325 Train loss:0.0026959830429404974\n",
      "Epoch:353 Train acc:0.9727095516569201 Test acc:0.4815668202764977 Train loss:0.001825999584980309\n",
      "Epoch:354 Train acc:0.9775828460038987 Test acc:0.4815668202764977 Train loss:0.0015288465656340122\n",
      "Epoch:355 Train acc:0.9819688109161794 Test acc:0.48847926267281105 Train loss:0.0013263195287436247\n",
      "Epoch:356 Train acc:0.9848927875243665 Test acc:0.4470046082949309 Train loss:0.0013848679373040795\n",
      "Epoch:357 Train acc:0.9449317738791423 Test acc:0.4815668202764977 Train loss:0.00298193097114563\n",
      "Epoch:358 Train acc:0.969785575048733 Test acc:0.4976958525345622 Train loss:0.0017788029508665204\n",
      "Epoch:359 Train acc:0.9785575048732943 Test acc:0.4792626728110599 Train loss:0.00142053107265383\n",
      "Epoch:360 Train acc:0.9834307992202729 Test acc:0.48963133640552997 Train loss:0.0011726496741175652\n",
      "Epoch:361 Train acc:0.9853801169590644 Test acc:0.47465437788018433 Train loss:0.0011349288979545236\n",
      "Epoch:362 Train acc:0.9878167641325536 Test acc:0.4619815668202765 Train loss:0.0010392401600256562\n",
      "Epoch:363 Train acc:0.9858674463937622 Test acc:0.48847926267281105 Train loss:0.0011105589801445603\n",
      "Epoch:364 Train acc:0.9853801169590644 Test acc:0.4665898617511521 Train loss:0.0010922440560534596\n",
      "Epoch:365 Train acc:0.98635477582846 Test acc:0.4792626728110599 Train loss:0.0012924749171361327\n",
      "Epoch:366 Train acc:0.9658869395711501 Test acc:0.47119815668202764 Train loss:0.002057256642729044\n",
      "Epoch:367 Train acc:0.9595516569200779 Test acc:0.478110599078341 Train loss:0.002249820390716195\n",
      "Epoch:368 Train acc:0.9839181286549707 Test acc:0.47235023041474655 Train loss:0.0012895975960418582\n",
      "Epoch:369 Train acc:0.9873294346978557 Test acc:0.4608294930875576 Train loss:0.0010983196552842855\n",
      "Epoch:370 Train acc:0.9844054580896686 Test acc:0.47235023041474655 Train loss:0.0011187911732122302\n",
      "Epoch:371 Train acc:0.9902534113060428 Test acc:0.4861751152073733 Train loss:0.0008416403434239328\n",
      "Epoch:372 Train acc:0.9887914230019493 Test acc:0.47465437788018433 Train loss:0.0009419401176273823\n",
      "Epoch:373 Train acc:0.973196881091618 Test acc:0.4642857142857143 Train loss:0.0016480643535032868\n",
      "Epoch:374 Train acc:0.9834307992202729 Test acc:0.47580645161290325 Train loss:0.0012469810899347067\n",
      "Epoch:375 Train acc:0.9839181286549707 Test acc:0.4470046082949309 Train loss:0.001105033908970654\n",
      "Epoch:376 Train acc:0.9853801169590644 Test acc:0.4573732718894009 Train loss:0.0010704875458031893\n",
      "Epoch:377 Train acc:0.9848927875243665 Test acc:0.45852534562211983 Train loss:0.0010071158176288009\n",
      "Epoch:378 Train acc:0.98635477582846 Test acc:0.4861751152073733 Train loss:0.0009781798580661416\n",
      "Epoch:379 Train acc:0.9848927875243665 Test acc:0.4815668202764977 Train loss:0.0010673888027668\n",
      "Epoch:380 Train acc:0.99317738791423 Test acc:0.47119815668202764 Train loss:0.0008353102602995932\n",
      "Epoch:381 Train acc:0.9887914230019493 Test acc:0.4827188940092166 Train loss:0.0008715809672139585\n",
      "Epoch:382 Train acc:0.98635477582846 Test acc:0.4573732718894009 Train loss:0.0008728624088689685\n",
      "Epoch:383 Train acc:0.9853801169590644 Test acc:0.46774193548387094 Train loss:0.00100704375654459\n",
      "Epoch:384 Train acc:0.9805068226120858 Test acc:0.4873271889400922 Train loss:0.001235114294104278\n",
      "Epoch:385 Train acc:0.9873294346978557 Test acc:0.47580645161290325 Train loss:0.0009980731410905719\n",
      "Epoch:386 Train acc:0.98635477582846 Test acc:0.47580645161290325 Train loss:0.0011004074476659298\n",
      "Epoch:387 Train acc:0.9883040935672515 Test acc:0.4861751152073733 Train loss:0.0008336013415828347\n",
      "Epoch:388 Train acc:0.9775828460038987 Test acc:0.49078341013824883 Train loss:0.0012160527985543013\n",
      "Epoch:389 Train acc:0.9429824561403509 Test acc:0.49078341013824883 Train loss:0.003578183241188526\n",
      "Epoch:390 Train acc:0.949317738791423 Test acc:0.4861751152073733 Train loss:0.0027849720790982246\n",
      "Epoch:391 Train acc:0.9629629629629629 Test acc:0.5 Train loss:0.002210079925134778\n",
      "Epoch:392 Train acc:0.973196881091618 Test acc:0.48502304147465436 Train loss:0.0017460661474615335\n",
      "Epoch:393 Train acc:0.9897660818713451 Test acc:0.4827188940092166 Train loss:0.0008547190809622407\n",
      "Epoch:394 Train acc:0.9941520467836257 Test acc:0.4861751152073733 Train loss:0.0006360044935718179\n",
      "Epoch:395 Train acc:0.9961013645224172 Test acc:0.4804147465437788 Train loss:0.0006127359811216593\n",
      "Epoch:396 Train acc:0.9946393762183235 Test acc:0.47465437788018433 Train loss:0.0006344577413983643\n",
      "Epoch:397 Train acc:0.9941520467836257 Test acc:0.48502304147465436 Train loss:0.0007073361775837839\n",
      "Epoch:398 Train acc:0.9961013645224172 Test acc:0.478110599078341 Train loss:0.0005335429450497031\n",
      "Epoch:399 Train acc:0.996588693957115 Test acc:0.47235023041474655 Train loss:0.0005096941604278982\n",
      "Epoch:400 Train acc:0.9951267056530214 Test acc:0.47119815668202764 Train loss:0.0007485487149097025\n",
      "Epoch:401 Train acc:0.919103313840156 Test acc:0.47119815668202764 Train loss:0.004405921790748835\n",
      "Epoch:402 Train acc:0.9376218323586745 Test acc:0.5195852534562212 Train loss:0.0036844657734036446\n",
      "Epoch:403 Train acc:0.9678362573099415 Test acc:0.4861751152073733 Train loss:0.0018862419528886676\n",
      "Epoch:404 Train acc:0.9844054580896686 Test acc:0.478110599078341 Train loss:0.0011792113073170185\n",
      "Epoch:405 Train acc:0.9907407407407407 Test acc:0.47119815668202764 Train loss:0.0008118855184875429\n",
      "Epoch:406 Train acc:0.9926900584795322 Test acc:0.47580645161290325 Train loss:0.0006969202659092844\n",
      "Epoch:407 Train acc:0.9970760233918129 Test acc:0.4861751152073733 Train loss:0.0005601588054560125\n",
      "Epoch:408 Train acc:0.996588693957115 Test acc:0.47465437788018433 Train loss:0.0005159262800589204\n",
      "Epoch:409 Train acc:0.9980506822612085 Test acc:0.4827188940092166 Train loss:0.00046819442650303245\n",
      "Epoch:410 Train acc:0.9951267056530214 Test acc:0.4838709677419355 Train loss:0.0004879608750343323\n",
      "Epoch:411 Train acc:0.9985380116959064 Test acc:0.4804147465437788 Train loss:0.00042651232797652483\n",
      "Epoch:412 Train acc:0.9970760233918129 Test acc:0.47235023041474655 Train loss:0.00047610534238629043\n",
      "Epoch:413 Train acc:0.9809941520467836 Test acc:0.4792626728110599 Train loss:0.0013645432190969586\n",
      "Epoch:414 Train acc:0.9824561403508771 Test acc:0.47580645161290325 Train loss:0.0012511668028309941\n",
      "Epoch:415 Train acc:0.8752436647173489 Test acc:0.48847926267281105 Train loss:0.008335358463227749\n",
      "Epoch:416 Train acc:0.9132553606237817 Test acc:0.47580645161290325 Train loss:0.004606006667017937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:417 Train acc:0.8961988304093568 Test acc:0.4665898617511521 Train loss:0.006633682642132044\n",
      "Epoch:418 Train acc:0.9405458089668616 Test acc:0.49078341013824883 Train loss:0.0028197967913001776\n",
      "Epoch:419 Train acc:0.9785575048732943 Test acc:0.4873271889400922 Train loss:0.0015456335386261344\n",
      "Epoch:420 Train acc:0.9853801169590644 Test acc:0.5092165898617511 Train loss:0.001115648658014834\n",
      "Epoch:421 Train acc:0.9897660818713451 Test acc:0.49193548387096775 Train loss:0.0011312095448374748\n",
      "Epoch:422 Train acc:0.9775828460038987 Test acc:0.4942396313364055 Train loss:0.0015220020432025194\n",
      "Epoch:423 Train acc:0.9907407407407407 Test acc:0.48963133640552997 Train loss:0.000937771750614047\n",
      "Epoch:424 Train acc:0.9727095516569201 Test acc:0.4804147465437788 Train loss:0.0019488069228827953\n",
      "Epoch:425 Train acc:0.9848927875243665 Test acc:0.47235023041474655 Train loss:0.0010453700087964535\n",
      "Epoch:426 Train acc:0.99317738791423 Test acc:0.4827188940092166 Train loss:0.00071114202728495\n",
      "Epoch:427 Train acc:0.9985380116959064 Test acc:0.5 Train loss:0.0005699220346286893\n",
      "Epoch:428 Train acc:0.9980506822612085 Test acc:0.4861751152073733 Train loss:0.000486910023028031\n",
      "Epoch:429 Train acc:0.9980506822612085 Test acc:0.4942396313364055 Train loss:0.0004956932389177382\n",
      "Epoch:430 Train acc:0.9980506822612085 Test acc:0.4827188940092166 Train loss:0.0004797505389433354\n",
      "Epoch:431 Train acc:0.9848927875243665 Test acc:0.4861751152073733 Train loss:0.0009931878885254264\n",
      "Epoch:432 Train acc:0.9941520467836257 Test acc:0.46543778801843316 Train loss:0.0006177904433570802\n",
      "Epoch:433 Train acc:0.9985380116959064 Test acc:0.4861751152073733 Train loss:0.00045920832781121135\n",
      "Epoch:434 Train acc:0.9985380116959064 Test acc:0.46774193548387094 Train loss:0.0005342006916180253\n",
      "Epoch:435 Train acc:0.9405458089668616 Test acc:0.5195852534562212 Train loss:0.0032861887011677027\n",
      "Epoch:436 Train acc:0.9702729044834308 Test acc:0.46889400921658986 Train loss:0.0017482650000602007\n",
      "Epoch:437 Train acc:0.9917153996101364 Test acc:0.4838709677419355 Train loss:0.000785964191891253\n",
      "Epoch:438 Train acc:0.9951267056530214 Test acc:0.4792626728110599 Train loss:0.0006230794242583215\n",
      "Epoch:439 Train acc:0.9995126705653021 Test acc:0.4815668202764977 Train loss:0.00040849114884622395\n",
      "Epoch:440 Train acc:0.9980506822612085 Test acc:0.4861751152073733 Train loss:0.0003875453257933259\n",
      "Epoch:441 Train acc:0.9970760233918129 Test acc:0.4769585253456221 Train loss:0.0004148221923969686\n",
      "Epoch:442 Train acc:0.9985380116959064 Test acc:0.4873271889400922 Train loss:0.00036778789944946766\n",
      "Epoch:443 Train acc:0.9951267056530214 Test acc:0.46774193548387094 Train loss:0.000490860256832093\n",
      "Epoch:444 Train acc:0.9975633528265108 Test acc:0.4942396313364055 Train loss:0.00037659925874322653\n",
      "Epoch:445 Train acc:1.0 Test acc:0.49539170506912444 Train loss:0.0003151496348436922\n",
      "Epoch:446 Train acc:1.0 Test acc:0.47119815668202764 Train loss:0.0002954527735710144\n",
      "Epoch:447 Train acc:0.9990253411306043 Test acc:0.49539170506912444 Train loss:0.00031829922227188945\n",
      "Epoch:448 Train acc:0.9980506822612085 Test acc:0.4769585253456221 Train loss:0.0002900550898630172\n",
      "Epoch:449 Train acc:0.9995126705653021 Test acc:0.5011520737327189 Train loss:0.00026316416915506124\n",
      "Epoch:450 Train acc:0.9995126705653021 Test acc:0.4861751152073733 Train loss:0.0003145057416986674\n",
      "Epoch:451 Train acc:0.9995126705653021 Test acc:0.4804147465437788 Train loss:0.000250130717176944\n",
      "Epoch:452 Train acc:0.9995126705653021 Test acc:0.4827188940092166 Train loss:0.0002524266019463539\n",
      "Epoch:453 Train acc:1.0 Test acc:0.4838709677419355 Train loss:0.0002624961780384183\n",
      "Epoch:454 Train acc:0.9985380116959064 Test acc:0.4861751152073733 Train loss:0.0002789194695651531\n",
      "Epoch:455 Train acc:0.9951267056530214 Test acc:0.4838709677419355 Train loss:0.0005113392253406346\n",
      "Epoch:456 Train acc:0.9990253411306043 Test acc:0.4700460829493088 Train loss:0.0003586096572689712\n",
      "Epoch:457 Train acc:0.9980506822612085 Test acc:0.47580645161290325 Train loss:0.0003057728463318199\n",
      "Epoch:458 Train acc:1.0 Test acc:0.49193548387096775 Train loss:0.00022925413213670254\n",
      "Epoch:459 Train acc:1.0 Test acc:0.4873271889400922 Train loss:0.00023045945272315294\n",
      "Epoch:460 Train acc:0.9985380116959064 Test acc:0.4873271889400922 Train loss:0.000265532813500613\n",
      "Epoch:461 Train acc:1.0 Test acc:0.47580645161290325 Train loss:0.00024016547831706703\n",
      "Epoch:462 Train acc:1.0 Test acc:0.4827188940092166 Train loss:0.000203296760446392\n",
      "Epoch:463 Train acc:1.0 Test acc:0.4735023041474654 Train loss:0.00020347669487819076\n",
      "Epoch:464 Train acc:0.9990253411306043 Test acc:0.4735023041474654 Train loss:0.00024067905906122178\n",
      "Epoch:465 Train acc:0.9995126705653021 Test acc:0.4815668202764977 Train loss:0.0002534513478167355\n",
      "Epoch:466 Train acc:1.0 Test acc:0.4838709677419355 Train loss:0.0002154743269784376\n",
      "Epoch:467 Train acc:1.0 Test acc:0.49193548387096775 Train loss:0.00019014047575183213\n",
      "Epoch:468 Train acc:1.0 Test acc:0.47119815668202764 Train loss:0.00018546498904470354\n",
      "Epoch:469 Train acc:0.9990253411306043 Test acc:0.46774193548387094 Train loss:0.00022067521058488637\n",
      "Epoch:470 Train acc:0.9956140350877193 Test acc:0.47465437788018433 Train loss:0.000415440445067361\n",
      "Epoch:471 Train acc:0.9619883040935673 Test acc:0.4792626728110599 Train loss:0.002105000661686063\n",
      "Epoch:472 Train acc:0.9220272904483431 Test acc:0.5391705069124424 Train loss:0.0042707109823822975\n",
      "Epoch:473 Train acc:0.8791423001949318 Test acc:0.47119815668202764 Train loss:0.007671825587749481\n",
      "Epoch:474 Train acc:0.9269005847953217 Test acc:0.45852534562211983 Train loss:0.004074299708008766\n",
      "Epoch:475 Train acc:0.9736842105263158 Test acc:0.48847926267281105 Train loss:0.0017852826276794076\n",
      "Epoch:476 Train acc:0.9926900584795322 Test acc:0.4792626728110599 Train loss:0.0007367733051069081\n",
      "Epoch:477 Train acc:0.9995126705653021 Test acc:0.4861751152073733 Train loss:0.000438183662481606\n",
      "Epoch:478 Train acc:0.9990253411306043 Test acc:0.4861751152073733 Train loss:0.00036059136618860066\n",
      "Epoch:479 Train acc:1.0 Test acc:0.4769585253456221 Train loss:0.00029314690618775785\n",
      "Epoch:480 Train acc:1.0 Test acc:0.4838709677419355 Train loss:0.00027319733635522425\n",
      "Epoch:481 Train acc:1.0 Test acc:0.4792626728110599 Train loss:0.0002579193387646228\n",
      "Epoch:482 Train acc:1.0 Test acc:0.4827188940092166 Train loss:0.000231235331739299\n",
      "Epoch:483 Train acc:0.9990253411306043 Test acc:0.478110599078341 Train loss:0.00031549439881928265\n",
      "Epoch:484 Train acc:0.9995126705653021 Test acc:0.4838709677419355 Train loss:0.0002758782065939158\n",
      "Epoch:485 Train acc:1.0 Test acc:0.4827188940092166 Train loss:0.0002444290730636567\n",
      "Epoch:486 Train acc:0.9926900584795322 Test acc:0.47119815668202764 Train loss:0.0005260211764834821\n",
      "Epoch:487 Train acc:0.9990253411306043 Test acc:0.49193548387096775 Train loss:0.00026322132907807827\n",
      "Epoch:488 Train acc:1.0 Test acc:0.4804147465437788 Train loss:0.0002554715902078897\n",
      "Epoch:489 Train acc:0.9917153996101364 Test acc:0.4700460829493088 Train loss:0.0005918199312873185\n",
      "Epoch:490 Train acc:0.9995126705653021 Test acc:0.4815668202764977 Train loss:0.00022146459377836436\n",
      "Epoch:491 Train acc:1.0 Test acc:0.4827188940092166 Train loss:0.00015985089703463018\n",
      "Epoch:492 Train acc:1.0 Test acc:0.4804147465437788 Train loss:0.00014706117508467287\n",
      "Epoch:493 Train acc:1.0 Test acc:0.4804147465437788 Train loss:0.00014420540537685156\n",
      "Epoch:494 Train acc:1.0 Test acc:0.478110599078341 Train loss:0.00013413125998340547\n",
      "Epoch:495 Train acc:1.0 Test acc:0.478110599078341 Train loss:0.00012559858441818506\n",
      "Epoch:496 Train acc:1.0 Test acc:0.478110599078341 Train loss:0.00012859830167144537\n",
      "Epoch:497 Train acc:1.0 Test acc:0.478110599078341 Train loss:0.00012420144048519433\n",
      "Epoch:498 Train acc:1.0 Test acc:0.4815668202764977 Train loss:0.0001200748811243102\n",
      "Epoch:499 Train acc:1.0 Test acc:0.4815668202764977 Train loss:0.00011669281957438216\n",
      "Epoch:500 Train acc:1.0 Test acc:0.4804147465437788 Train loss:0.00011232055112486705\n",
      "Epoch:501 Train acc:1.0 Test acc:0.48847926267281105 Train loss:0.00010569926234893501\n",
      "Epoch:502 Train acc:1.0 Test acc:0.4873271889400922 Train loss:0.00010358190775150433\n",
      "Epoch:503 Train acc:1.0 Test acc:0.4942396313364055 Train loss:0.0001054113163263537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:504 Train acc:0.9990253411306043 Test acc:0.48502304147465436 Train loss:0.00016449153190478683\n",
      "Epoch:505 Train acc:1.0 Test acc:0.47465437788018433 Train loss:0.00011814207391580567\n",
      "Epoch:506 Train acc:1.0 Test acc:0.4873271889400922 Train loss:9.756950748851523e-05\n",
      "Epoch:507 Train acc:0.9990253411306043 Test acc:0.478110599078341 Train loss:0.0001360599126201123\n",
      "Epoch:508 Train acc:0.9995126705653021 Test acc:0.47580645161290325 Train loss:0.00017112036584876478\n",
      "Epoch:509 Train acc:0.9936647173489279 Test acc:0.4423963133640553 Train loss:0.0004843349743168801\n",
      "Epoch:510 Train acc:0.9736842105263158 Test acc:0.4435483870967742 Train loss:0.001493989140726626\n",
      "Epoch:511 Train acc:0.9429824561403509 Test acc:0.4976958525345622 Train loss:0.003587373998016119\n",
      "Epoch:512 Train acc:0.9259259259259259 Test acc:0.5149769585253456 Train loss:0.004227769561111927\n",
      "Epoch:513 Train acc:0.9273879142300195 Test acc:0.49539170506912444 Train loss:0.004162126686424017\n",
      "Epoch:514 Train acc:0.9683235867446394 Test acc:0.43548387096774194 Train loss:0.0016579354414716363\n",
      "Epoch:515 Train acc:0.9926900584795322 Test acc:0.4619815668202765 Train loss:0.0006786939920857549\n",
      "Epoch:516 Train acc:0.9990253411306043 Test acc:0.46543778801843316 Train loss:0.0003354617510922253\n",
      "Epoch:517 Train acc:1.0 Test acc:0.47235023041474655 Train loss:0.0002265255170641467\n",
      "Epoch:518 Train acc:1.0 Test acc:0.46774193548387094 Train loss:0.00017665838822722435\n",
      "Epoch:519 Train acc:1.0 Test acc:0.4504608294930876 Train loss:0.00018472986994311213\n",
      "Epoch:520 Train acc:0.9975633528265108 Test acc:0.4700460829493088 Train loss:0.0002697253366932273\n",
      "Epoch:521 Train acc:1.0 Test acc:0.46774193548387094 Train loss:0.00015617540339007974\n",
      "Epoch:522 Train acc:1.0 Test acc:0.46889400921658986 Train loss:0.00015266249829437584\n",
      "Epoch:523 Train acc:1.0 Test acc:0.47580645161290325 Train loss:0.00012003822484984994\n",
      "Epoch:524 Train acc:1.0 Test acc:0.478110599078341 Train loss:0.00011876368807861581\n",
      "Epoch:525 Train acc:1.0 Test acc:0.46543778801843316 Train loss:0.00010302606096956879\n",
      "Epoch:526 Train acc:1.0 Test acc:0.4735023041474654 Train loss:9.974088607123122e-05\n",
      "Epoch:527 Train acc:1.0 Test acc:0.4792626728110599 Train loss:9.403745934832841e-05\n",
      "Epoch:528 Train acc:1.0 Test acc:0.4642857142857143 Train loss:9.536280413158238e-05\n",
      "Epoch:529 Train acc:1.0 Test acc:0.4735023041474654 Train loss:9.210761345457286e-05\n",
      "Epoch:530 Train acc:1.0 Test acc:0.46889400921658986 Train loss:8.196615090128034e-05\n",
      "Epoch:531 Train acc:1.0 Test acc:0.478110599078341 Train loss:8.012279431568459e-05\n",
      "Epoch:532 Train acc:1.0 Test acc:0.47119815668202764 Train loss:8.812310989014804e-05\n",
      "Epoch:533 Train acc:1.0 Test acc:0.47119815668202764 Train loss:7.57498710299842e-05\n",
      "Epoch:534 Train acc:1.0 Test acc:0.4642857142857143 Train loss:7.514176104450598e-05\n",
      "Epoch:535 Train acc:1.0 Test acc:0.46543778801843316 Train loss:7.828039088053629e-05\n",
      "Epoch:536 Train acc:1.0 Test acc:0.47235023041474655 Train loss:7.287797052413225e-05\n",
      "Epoch:537 Train acc:1.0 Test acc:0.47465437788018433 Train loss:7.689791527809575e-05\n",
      "Epoch:538 Train acc:1.0 Test acc:0.47465437788018433 Train loss:8.11997233540751e-05\n",
      "Epoch:539 Train acc:0.9975633528265108 Test acc:0.4769585253456221 Train loss:0.00029220967553555965\n",
      "Epoch:540 Train acc:0.9975633528265108 Test acc:0.47580645161290325 Train loss:0.00023873911413829774\n",
      "Epoch:541 Train acc:0.9975633528265108 Test acc:0.4769585253456221 Train loss:0.000194457737961784\n",
      "Epoch:542 Train acc:1.0 Test acc:0.4735023041474654 Train loss:0.00013405922800302505\n",
      "Epoch:543 Train acc:1.0 Test acc:0.47235023041474655 Train loss:9.188673720927909e-05\n",
      "Epoch:544 Train acc:1.0 Test acc:0.47235023041474655 Train loss:7.852082489989698e-05\n",
      "Epoch:545 Train acc:1.0 Test acc:0.47580645161290325 Train loss:6.453558307839558e-05\n",
      "Epoch:546 Train acc:1.0 Test acc:0.4792626728110599 Train loss:6.297948129940778e-05\n",
      "Epoch:547 Train acc:1.0 Test acc:0.4735023041474654 Train loss:8.25567840365693e-05\n",
      "Epoch:548 Train acc:1.0 Test acc:0.47465437788018433 Train loss:6.610863783862442e-05\n",
      "Epoch:549 Train acc:1.0 Test acc:0.47119815668202764 Train loss:4.8331261496059597e-05\n",
      "Epoch:550 Train acc:1.0 Test acc:0.4815668202764977 Train loss:4.649348193197511e-05\n",
      "Epoch:551 Train acc:1.0 Test acc:0.4735023041474654 Train loss:4.640854604076594e-05\n",
      "Epoch:552 Train acc:1.0 Test acc:0.47235023041474655 Train loss:4.753569737658836e-05\n",
      "Epoch:553 Train acc:1.0 Test acc:0.4792626728110599 Train loss:4.355684723122977e-05\n",
      "Epoch:554 Train acc:1.0 Test acc:0.48502304147465436 Train loss:4.2353047319920734e-05\n",
      "Epoch:555 Train acc:1.0 Test acc:0.47580645161290325 Train loss:4.7823712520767e-05\n"
     ]
    }
   ],
   "source": [
    "loops = [True, False]\n",
    "weighted = ['weighted', 'unweighted_dynamic_threshold', 'unweighted_static_threshold']\n",
    "data = ['power_and_entropy', 'only_entropy', 'only_powerbands']\n",
    "\n",
    "lr = 0.001\n",
    "wd = 0.001\n",
    "batch_size = 50\n",
    "hidden_channels = 128 \n",
    "epoches = 1000\n",
    "\n",
    "\n",
    "# train_dataset = GNNDataset('./', 'gnn_prepared_data/power_and_entropy_gnn_train.npy', \n",
    "#                            0, \n",
    "#                            'all_features',\n",
    "#                            allow_loops = True,\n",
    "#                            weighted = 'weighted')\n",
    "# test_dataset = GNNDataset('./', 'gnn_prepared_data/power_and_entropy_gnn_test.npy', \n",
    "#                           1, \n",
    "#                           'all_features',\n",
    "#                           allow_loops = True,\n",
    "#                           weighted = 'weighted') \n",
    "\n",
    "# model = GCN(num_node_features = train_dataset.num_node_features, \n",
    "#                             hidden_channels = hidden_channels)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr = lr, weight_decay = wd)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)\n",
    "    \n",
    "# train(model = model,\n",
    "#       epoches = epoches, \n",
    "#       criterion = criterion, \n",
    "#       train_dataloader = train_loader, \n",
    "#       test_dataloader = test_loader,\n",
    "#       expirement_name = 'debug')\n",
    "\n",
    "buff = []\n",
    "for loops_config in loops:\n",
    "    for weight_config in weighted:\n",
    "        for dataset in data:\n",
    "            train_dataset = GNNDataset(root = './', \n",
    "                                       data_dict = f'gnn_prepared_data/{dataset}_gnn_train.npy',\n",
    "                                       idx = 0, \n",
    "                                       feature_names = dataset, \n",
    "                                       allow_loops = loops_config, \n",
    "                                       weighted = weight_config) # idx = 0 - train\n",
    "            test_dataset = GNNDataset(root = './', \n",
    "                                      data_dict = f'gnn_prepared_data/{dataset}_gnn_test.npy', \n",
    "                                      idx = 1, \n",
    "                                      feature_names = dataset,\n",
    "                                      allow_loops = loops_config, \n",
    "                                      weighted = weight_config) # idx = 1 - train\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "            test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "            model = GCN(num_node_features = train_dataset.num_node_features, \n",
    "                        hidden_channels = hidden_channels)\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr = lr, weight_decay = wd)\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "            config = {\n",
    "                        'learning_rate': lr,\n",
    "                        'weight_decay': wd,\n",
    "                        'epochs': epoches,\n",
    "                        'training_batch_size' : batch_size,\n",
    "                        'validation_batch_size' : batch_size,\n",
    "                        'loops_config': loops_config,\n",
    "                        'weight_config': weight_config,\n",
    "                        'dataset': dataset,\n",
    "                        'criterion': criterion,    \n",
    "                        'node_representation_size': train_dataset.num_node_features, \n",
    "                        'model': {\n",
    "                                    'num_graph_conv_blocks': 4,\n",
    "                                    'hidden_channels' : hidden_channels,\n",
    "                                    'activation' : 'ReLU',\n",
    "                                    'readout': 'global_mean_pool'}}\n",
    "#             buff.append(config)\n",
    "#             print(config)\n",
    "            experiment_name = f'self_loops={loops_config}_weighted={weight_config}_data={dataset}'\n",
    "\n",
    "            wandb.init(project = 'neuroimaging_gnn_eeg_final_project', \n",
    "                       entity = 'dmasny',\n",
    "                       name = experiment_name, \n",
    "                       config = config)\n",
    "\n",
    "            train(model = model,\n",
    "                  epoches = epoches, \n",
    "                  criterion = criterion, \n",
    "                  train_dataloader = train_loader, \n",
    "                  test_dataloader = test_loader,\n",
    "                  expirement_name = experiment_name)\n",
    "            print(experiment_name)\n",
    "            del model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95e101ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mprocessed/\u001b[00m\r\n",
      "├── gnn_dataset_test_all_features.pt\r\n",
      "├── gnn_dataset_train_all_features.pt\r\n",
      "├── pre_filter.pt\r\n",
      "└── pre_transform.pt\r\n",
      "\r\n",
      "0 directories, 4 files\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    }
   ],
   "source": [
    "!tree processed/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba8ab01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429bce30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e44c052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f19479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
