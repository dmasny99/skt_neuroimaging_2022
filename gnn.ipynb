{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5ff702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "import os\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import InMemoryDataset, download_url\n",
    "\n",
    "from IPython.display import Javascript\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv,SAGEConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf261b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_patients = set(['F117', 'F130', 'F128', 'F133', 'F135', 'F138', 'F139', 'F136',\n",
    "                 'F131', 'F344', 'F504', 'F510', 'M512', 'F341', 'F337', 'F324',\n",
    "                 'F308', 'F317'])\n",
    "elecs = ['Fp1', 'Fp2', 'Fpz', 'F3', 'F4', 'Fz', 'C3', 'C4',\n",
    "         'Cz', 'P3', 'P4', 'Pz', '01', '02', '0z', 'F7', \n",
    "         'F8', 'T3', 'T4','T5', 'T6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ad2b321",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdmasny\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/notebooks/gnn_eeg/wandb/run-20221027_193856-3gcrw76y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/dmasny/neuroimaging_gnn_eeg_final_project/runs/3gcrw76y\" target=\"_blank\">graceful-dream-22</a></strong> to <a href=\"https://wandb.ai/dmasny/neuroimaging_gnn_eeg_final_project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/dmasny/neuroimaging_gnn_eeg_final_project/runs/3gcrw76y?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f15e2f74640>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project = \"neuroimaging_gnn_eeg_final_project\", entity = \"dmasny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97eeba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNDataset(InMemoryDataset):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 root, \n",
    "                 data_dict, \n",
    "                 idx, \n",
    "                 feature_names, \n",
    "                 allow_loops, \n",
    "                 weighted, \n",
    "                 threshold = 0.65,\n",
    "                 transform = None, \n",
    "                 pre_transform = None, \n",
    "                 pre_filter = None):\n",
    "        \n",
    "        self.data = np.load(data_dict, allow_pickle = True).item()\n",
    "        self.stage = idx # idx 0 - train, idx 1 - test\n",
    "        self.feature_names = feature_names\n",
    "        self.allow_loops = allow_loops\n",
    "        self.weighted = weighted\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[self.stage])\n",
    "        \n",
    "    def vectorize_adj_mat_coo(self, matrix, allow_loops = True):\n",
    "        source_nodes = []\n",
    "        target_nodes = []\n",
    "        if allow_loops:\n",
    "            for i in range(matrix.shape[0]):\n",
    "                for j in range(i, matrix.shape[1]):\n",
    "                    source_nodes.append(i)\n",
    "                    target_nodes.append(j)\n",
    "        else:\n",
    "            for i in range(matrix.shape[0]):\n",
    "                for j in range(i + 1, matrix.shape[1]):\n",
    "                    source_nodes.append(i)\n",
    "                    target_nodes.append(j)\n",
    "        return source_nodes, target_nodes\n",
    "\n",
    "    def vectorize_adj_mat_weights(self, matrix):\n",
    "        edge_weights = []\n",
    "        if self.weighted == 'weighted':\n",
    "            if self.allow_loops:\n",
    "                for i in range(matrix.shape[0]):\n",
    "                    for j in range(i, matrix.shape[1]):\n",
    "                        edge_weights.append(matrix[i][j])\n",
    "            else:\n",
    "                for i in range(matrix.shape[0]):\n",
    "                    for j in range(i + 1, matrix.shape[1]):\n",
    "                        edge_weights.append(matrix[i][j])\n",
    "        else:\n",
    "            threshold = np.min(np.max(matrix, axis = 1)) if self.weighted == 'unweighted_dynamic_threshold' else self.threshold\n",
    "            mask = np.array((matrix > threshold), dtype = np.uint8)\n",
    "            if allow_loops:\n",
    "                for i in range(mask.shape[0]):\n",
    "                    for j in range(i, mask.shape[1]):\n",
    "                        edge_weights.append(mask[i][j])\n",
    "            else:\n",
    "                for i in range(mask.shape[0]):\n",
    "                    for j in range(i + 1, mask.shape[1]):\n",
    "                        edge_weights.append(mask[i][j])\n",
    "        return edge_weights\n",
    "\n",
    "    def upload_data(self, patch_name):\n",
    "        '''\n",
    "        input:\n",
    "            path_to_data: path to precomputed node representations\n",
    "            path_to_adj_matr: path to precomputed adj matrices\n",
    "            \n",
    "        returns:\n",
    "            Pygeometric Data object (see PyG docs)\n",
    "        '''\n",
    "        X, target, adj_matrix = self.data[patch_name] # triplet in format [X, target, A]\n",
    "        edge_index = np.array(self.vectorize_adj_mat_coo(adj_matrix))\n",
    "        edge_features = self.vectorize_adj_mat_weights(adj_matrix)\n",
    "        return Data(x = torch.tensor(X), \n",
    "                    edge_index = torch.tensor(edge_index),\n",
    "                    edge_attrs = edge_features, \n",
    "                    y = torch.tensor([target]))  \n",
    "    \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [f'gnn_dataset_train_{self.feature_names}.pt',\n",
    "                f'gnn_dataset_test_{self.feature_names}.pt']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "        \n",
    "    def process(self):\n",
    "        data_list = []\n",
    "        for elem in self.data.keys():\n",
    "            data_list.append(self.upload_data(elem))\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[self.stage])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4f61d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_channels, output_dim = 2):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(2022)\n",
    "        self.conv1 = SAGEConv(num_node_features, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.linear = Linear(hidden_channels, output_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        \n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        \n",
    "        x = self.conv4(x ,edge_index)\n",
    "        x = x.relu()\n",
    "        \n",
    "\n",
    "        # Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        # Classifier\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4297bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = GNNDataset('./', 'gnn_prepared_data/power_and_entropy_gnn_train.npy', \n",
    "#                            0, \n",
    "#                            'all_features',\n",
    "#                            allow_loops = True,\n",
    "#                            weighted = 'weighted') # idx = 0 - train\n",
    "# test_dataset = GNNDataset('./', 'gnn_prepared_data/power_and_entropy_gnn_test.npy', \n",
    "#                           1, \n",
    "#                           'all_features',\n",
    "#                           allow_loops = True,\n",
    "#                           weighted = 'weighted') # idx = 1 - train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fe494f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = GNNDataset('./', 'gnn_prepared_data/power_and_entropy_gnn_train.npy', 0, 'all_features',  weighted = False) # idx = 0 - train\n",
    "# test_dataset = GNNDataset('./', 'gnn_prepared_data/power_and_entropy_gnn_test.npy', 1, 'all_features', weighted = False) # idx = 1 - train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4195769",
   "metadata": {},
   "source": [
    "1) adj(2) + loops(2) + weighted(2) + data(3) = 2*2*2*3 = 12 exp\n",
    "2) batch size, num_layers, num_epoches, lr, scheduler\n",
    "3)\n",
    "\n",
    "object : [X, target, A]\n",
    "X - only use local info about the node\n",
    "1) acc entropy <\n",
    "2) acc power <\n",
    "3) acc power + entropy < gnn(power + entropy + adj matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84bc6770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, dataloader):\n",
    "#     model.train()\n",
    "\n",
    "#     for data in dataloader:  # Iterate in batches over the training dataset.\n",
    "#         out = model(data.x.type(dtype=torch.float), data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "#         loss = criterion(out, data.y)  # Compute the loss.\n",
    "#         loss.backward()  # Derive gradients.\n",
    "#         optimizer.step()  # Update parameters based on gradients.\n",
    "#         optimizer.zero_grad()  # Clear gradients.\n",
    "#         wandb.log()\n",
    "        \n",
    "# def test(model, dataloader):\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     for data in dataloader:  # Iterate in batches over the training/test dataset.\n",
    "#         out = model(data.x.type(dtype=torch.float), data.edge_index, data.batch)  \n",
    "#         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "#         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "#     return correct / len(loader.dataset)  # Derive ratio of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92880e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, \n",
    "          epoches, \n",
    "          criterion, \n",
    "          train_dataloader, \n",
    "          test_dataloader, \n",
    "          expirement_name, \n",
    "          path_to_save_weights = 'model_weights'):\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    for i in range(epoches):\n",
    "        model.train()\n",
    "        train_correct = 0\n",
    "        train_loss = 0\n",
    "        for data in train_dataloader:\n",
    "            out = model(data.x.type(dtype = torch.float), \n",
    "                        data.edge_index, \n",
    "                        data.batch)  \n",
    "            loss = criterion(out, data.y) \n",
    "            train_loss += loss\n",
    "            pred = out.argmax(dim = 1)\n",
    "            train_correct += int((pred == data.y).sum())\n",
    "            loss.backward() \n",
    "            optimizer.step()  \n",
    "            optimizer.zero_grad() \n",
    "        train_loss = train_loss / len(train_dataloader.dataset)\n",
    "        train_accuracy = train_correct / len(train_dataloader.dataset)\n",
    "        \n",
    "        wandb.log({'train_loss': train_loss})\n",
    "        wandb.log({'train_accuracy': train_accuracy})\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():  \n",
    "            test_correct = 0\n",
    "            for data in test_dataloader:  \n",
    "                out = model(data.x.type(dtype = torch.float), data.edge_index, data.batch)  \n",
    "                pred = out.argmax(dim = 1)  \n",
    "                test_correct += int((pred == data.y).sum())\n",
    "            test_accuracy = test_correct / len(test_dataloader.dataset)\n",
    "            \n",
    "            if test_accuracy > best_accuracy:\n",
    "                best_accuracy = test_accuracy\n",
    "                torch.save(model.state_dict(), f'{path_to_save_weights}/{expirement_name}.pth')\n",
    "                \n",
    "            print(f'Epoch:{i} Train acc:{train_accuracy} Test acc:{test_accuracy} Train loss:{train_loss}')\n",
    "            \n",
    "            wandb.log({'test_accuracy': test_accuracy})\n",
    "            wandb.log({'best_test_accuracy': best_accuracy})\n",
    "            \n",
    "    \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c9aacb5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3gcrw76y) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db806c01f39e4e86bb3b88ec9cb40f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">graceful-dream-22</strong>: <a href=\"https://wandb.ai/dmasny/neuroimaging_gnn_eeg_final_project/runs/3gcrw76y\" target=\"_blank\">https://wandb.ai/dmasny/neuroimaging_gnn_eeg_final_project/runs/3gcrw76y</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221027_193856-3gcrw76y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3gcrw76y). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8926b0bdd0fc416786e717c4999f3c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666905900153021, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/notebooks/gnn_eeg/wandb/run-20221027_193959-181g9aoy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/dmasny/neuroimaging_gnn_eeg_final_project/runs/181g9aoy\" target=\"_blank\">self_loops=True_weighted=weighted_data=power_and_entropy</a></strong> to <a href=\"https://wandb.ai/dmasny/neuroimaging_gnn_eeg_final_project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Train acc:0.5428849902534113 Test acc:0.4665898617511521 Train loss:0.014184386469423771\n",
      "Epoch:1 Train acc:0.5662768031189084 Test acc:0.4251152073732719 Train loss:0.013797205872833729\n",
      "Epoch:2 Train acc:0.601364522417154 Test acc:0.4147465437788018 Train loss:0.013724185526371002\n",
      "Epoch:3 Train acc:0.584307992202729 Test acc:0.41244239631336405 Train loss:0.013682049699127674\n",
      "Epoch:4 Train acc:0.604775828460039 Test acc:0.4216589861751152 Train loss:0.013269738294184208\n",
      "Epoch:5 Train acc:0.6174463937621832 Test acc:0.48963133640552997 Train loss:0.013063423335552216\n",
      "Epoch:6 Train acc:0.5555555555555556 Test acc:0.4205069124423963 Train loss:0.01390183623880148\n",
      "Epoch:7 Train acc:0.5896686159844055 Test acc:0.4423963133640553 Train loss:0.013311393558979034\n",
      "Epoch:8 Train acc:0.628167641325536 Test acc:0.42972350230414746 Train loss:0.01276925764977932\n",
      "Epoch:9 Train acc:0.6301169590643275 Test acc:0.4412442396313364 Train loss:0.012714765034615993\n",
      "Epoch:10 Train acc:0.6247563352826511 Test acc:0.43317972350230416 Train loss:0.012730089947581291\n",
      "Epoch:11 Train acc:0.6388888888888888 Test acc:0.478110599078341 Train loss:0.012801614589989185\n",
      "Epoch:12 Train acc:0.615009746588694 Test acc:0.4642857142857143 Train loss:0.012761340476572514\n",
      "Epoch:13 Train acc:0.6252436647173489 Test acc:0.4423963133640553 Train loss:0.012799516320228577\n",
      "Epoch:14 Train acc:0.6340155945419104 Test acc:0.4804147465437788 Train loss:0.012417959980666637\n",
      "Epoch:15 Train acc:0.6232943469785575 Test acc:0.4573732718894009 Train loss:0.012902667745947838\n",
      "Epoch:16 Train acc:0.6364522417153996 Test acc:0.44930875576036866 Train loss:0.01274982187896967\n",
      "Epoch:17 Train acc:0.6247563352826511 Test acc:0.45276497695852536 Train loss:0.01269597839564085\n",
      "Epoch:18 Train acc:0.6325536062378168 Test acc:0.4447004608294931 Train loss:0.01262302603572607\n",
      "Epoch:19 Train acc:0.6442495126705653 Test acc:0.4596774193548387 Train loss:0.01266584824770689\n",
      "Epoch:20 Train acc:0.628167641325536 Test acc:0.4792626728110599 Train loss:0.012334387749433517\n",
      "Epoch:21 Train acc:0.6340155945419104 Test acc:0.46543778801843316 Train loss:0.012497362680733204\n",
      "Epoch:22 Train acc:0.6262183235867447 Test acc:0.4827188940092166 Train loss:0.012461411766707897\n",
      "Epoch:23 Train acc:0.6442495126705653 Test acc:0.4804147465437788 Train loss:0.012235961854457855\n",
      "Epoch:24 Train acc:0.6505847953216374 Test acc:0.4470046082949309 Train loss:0.012329823337495327\n",
      "Epoch:25 Train acc:0.6535087719298246 Test acc:0.45276497695852536 Train loss:0.012349221855401993\n",
      "Epoch:26 Train acc:0.6340155945419104 Test acc:0.45161290322580644 Train loss:0.012506627477705479\n",
      "Epoch:27 Train acc:0.6539961013645225 Test acc:0.44815668202764974 Train loss:0.012474630028009415\n",
      "Epoch:28 Train acc:0.6252436647173489 Test acc:0.47119815668202764 Train loss:0.012486262246966362\n",
      "Epoch:29 Train acc:0.6179337231968811 Test acc:0.4873271889400922 Train loss:0.012849975377321243\n",
      "Epoch:30 Train acc:0.6359649122807017 Test acc:0.4976958525345622 Train loss:0.01229226402938366\n",
      "Epoch:31 Train acc:0.6364522417153996 Test acc:0.543778801843318 Train loss:0.0123420599848032\n",
      "Epoch:32 Train acc:0.6481481481481481 Test acc:0.511520737327189 Train loss:0.0122680040076375\n",
      "Epoch:33 Train acc:0.6379142300194932 Test acc:0.4804147465437788 Train loss:0.012364625930786133\n",
      "Epoch:34 Train acc:0.648635477582846 Test acc:0.5241935483870968 Train loss:0.012149185873568058\n",
      "Epoch:35 Train acc:0.6544834307992202 Test acc:0.46889400921658986 Train loss:0.012338717468082905\n",
      "Epoch:36 Train acc:0.6539961013645225 Test acc:0.5299539170506913 Train loss:0.012099338695406914\n",
      "Epoch:37 Train acc:0.6549707602339181 Test acc:0.511520737327189 Train loss:0.012106199748814106\n",
      "Epoch:38 Train acc:0.6539961013645225 Test acc:0.511520737327189 Train loss:0.012120178900659084\n",
      "Epoch:39 Train acc:0.652046783625731 Test acc:0.4735023041474654 Train loss:0.01181474793702364\n",
      "Epoch:40 Train acc:0.6574074074074074 Test acc:0.5241935483870968 Train loss:0.012156653217971325\n",
      "Epoch:41 Train acc:0.6466861598440545 Test acc:0.5069124423963134 Train loss:0.012125045992434025\n",
      "Epoch:42 Train acc:0.6613060428849903 Test acc:0.5149769585253456 Train loss:0.011961880140006542\n",
      "Epoch:43 Train acc:0.6603313840155945 Test acc:0.5138248847926268 Train loss:0.011949444189667702\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m experiment_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mself_loops=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloops_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_weighted=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_data=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     84\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(project \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneuroimaging_gnn_eeg_final_project\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     85\u001b[0m            entity \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdmasny\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     86\u001b[0m            name \u001b[38;5;241m=\u001b[39m experiment_name, \n\u001b[1;32m     87\u001b[0m            config \u001b[38;5;241m=\u001b[39m config)\n\u001b[0;32m---> 89\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m      \u001b[49m\u001b[43mepoches\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepoches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m      \u001b[49m\u001b[43mexpirement_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(experiment_name)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, epoches, criterion, train_dataloader, test_dataloader, expirement_name, path_to_save_weights)\u001b[0m\n\u001b[1;32m     21\u001b[0m     train_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m((pred \u001b[38;5;241m==\u001b[39m data\u001b[38;5;241m.\u001b[39my)\u001b[38;5;241m.\u001b[39msum())\n\u001b[1;32m     22\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward() \n\u001b[0;32m---> 23\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     24\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \n\u001b[1;32m     25\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader\u001b[38;5;241m.\u001b[39mdataset)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/optim/adamw.py:161\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m             max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    159\u001b[0m         state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 161\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m          \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m          \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m          \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m          \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m          \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m          \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m          \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m          \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m          \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/optim/adamw.py:218\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 218\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/optim/adamw.py:311\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m--> 311\u001b[0m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loops = [True, False]\n",
    "weighted = ['weighted', 'unweighted_dynamic_threshold', 'unweighted_static_threshold']\n",
    "data = ['power_and_entropy', 'only_entropy', 'only_powerbands']\n",
    "\n",
    "lr = 0.001\n",
    "wd = 0.001\n",
    "batch_size = 50\n",
    "hidden_channels = 128 \n",
    "epoches = 1000\n",
    "\n",
    "\n",
    "# train_dataset = GNNDataset('./', 'gnn_prepared_data/power_and_entropy_gnn_train.npy', \n",
    "#                            0, \n",
    "#                            'all_features',\n",
    "#                            allow_loops = True,\n",
    "#                            weighted = 'weighted') # idx = 0 - train\n",
    "# test_dataset = GNNDataset('./', 'gnn_prepared_data/power_and_entropy_gnn_test.npy', \n",
    "#                           1, \n",
    "#                           'all_features',\n",
    "#                           allow_loops = True,\n",
    "#                           weighted = 'weighted') # \n",
    "\n",
    "# model = GCN(num_node_features = train_dataset.num_node_features, \n",
    "#                             hidden_channels = hidden_channels)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr = lr, weight_decay = wd)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)\n",
    "    \n",
    "# train(model = model,\n",
    "#       epoches = epoches, \n",
    "#       criterion = criterion, \n",
    "#       train_dataloader = train_loader, \n",
    "#       test_dataloader = test_loader,\n",
    "#       expirement_name = 'debug')\n",
    "buff = []\n",
    "for loops_config in loops:\n",
    "    for weight_config in weighted:\n",
    "        for dataset in data:\n",
    "            train_dataset = GNNDataset(root = './', \n",
    "                                       data_dict = f'gnn_prepared_data/{dataset}_gnn_train.npy',\n",
    "                                       idx = 0, \n",
    "                                       feature_names = data, \n",
    "                                       allow_loops = loops_config, \n",
    "                                       weighted = weight_config) # idx = 0 - train\n",
    "            test_dataset = GNNDataset(root = './', \n",
    "                                      data_dict = f'gnn_prepared_data/{dataset}_gnn_test.npy', \n",
    "                                      idx = 1, \n",
    "                                      feature_names = data,\n",
    "                                      allow_loops = loops_config, \n",
    "                                      weighted = weight_config) # idx = 1 - train\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "            test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "            model = GCN(num_node_features = train_dataset.num_node_features, \n",
    "                        hidden_channels = hidden_channels)\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr = lr, weight_decay = wd)\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "            config = {\n",
    "                        'learning_rate': lr,\n",
    "                        'weight_decay': wd,\n",
    "                        'epochs': epoches,\n",
    "                        'training_batch_size' : batch_size,\n",
    "                        'validation_batch_size' : batch_size,\n",
    "                        'loops_config': loops_config,\n",
    "                        'weight_config': weight_config,\n",
    "                        'dataset': dataset,\n",
    "                        'criterion': criterion,    \n",
    "                        'node_representation_size': train_dataset.num_node_features, \n",
    "                        'model': {\n",
    "                                    'num_graph_conv_blocks': 4,\n",
    "                                    'hidden_channels' : hidden_channels,\n",
    "                                    'activation' : 'ReLU',\n",
    "                                    'readout': 'global_mean_pool'}}\n",
    "#             buff.append(config)\n",
    "#             print(config)\n",
    "            experiment_name = f'self_loops={loops_config}_weighted={weight_config}_data={dataset}'\n",
    "\n",
    "            wandb.init(project = 'neuroimaging_gnn_eeg_final_project', \n",
    "                       entity = 'dmasny',\n",
    "                       name = experiment_name, \n",
    "                       config = config)\n",
    "\n",
    "            train(model = model,\n",
    "                  epoches = epoches, \n",
    "                  criterion = criterion, \n",
    "                  train_dataloader = train_loader, \n",
    "                  test_dataloader = test_loader,\n",
    "                  expirement_name = experiment_name)\n",
    "            print(experiment_name)\n",
    "            del model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1d8e3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mgnn_prepared_data/\u001b[00m\r\n",
      "├── only_entropy_gnn_test.npy\r\n",
      "├── only_entropy_gnn_train.npy\r\n",
      "├── only_powerbands_gnn_test.npy\r\n",
      "├── only_powerbands_gnn_train.npy\r\n",
      "├── power_and_entropy_gnn_test.npy\r\n",
      "└── power_and_entropy_gnn_train.npy\r\n",
      "\r\n",
      "0 directories, 6 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree gnn_prepared_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3025d54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.001,\n",
       " 'weight_decay': 0.001,\n",
       " 'epochs': 1000,\n",
       " 'training_batch_size': 50,\n",
       " 'validation_batch_size': 50,\n",
       " 'loops_config': True,\n",
       " 'weight_config': 'weighted',\n",
       " 'dataset': 'power_and_entropy',\n",
       " 'criterion': CrossEntropyLoss(),\n",
       " 'node_representation_size': 4,\n",
       " 'model': {'num_graph_conv_blocks': 4,\n",
       "  'hidden_channels': 128,\n",
       "  'activation': 'ReLU',\n",
       "  'readout': 'global_mean_pool'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buff[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa432868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e465991c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d084e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bf300b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
