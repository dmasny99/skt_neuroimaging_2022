{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1078987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "import networkx as nx\n",
    "import os\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import InMemoryDataset, download_url\n",
    "\n",
    "from IPython.display import Javascript\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv,SAGEConv\n",
    "from torch_geometric.nn import global_mean_pool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bf762737",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_patients = set(['F117', 'F130', 'F128', 'F133', 'F135', 'F138', 'F139', 'F136',\n",
    "                 'F131', 'F344', 'F504', 'F510', 'M512', 'F341', 'F337', 'F324',\n",
    "                 'F308', 'F317'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4438b1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unq_set = set()\n",
    "# for elem in os.listdir('clean_wcoh/kz_clean/health/'):\n",
    "#     if 'log' in elem:\n",
    "#         continue\n",
    "#     name = elem.split('_')[1]\n",
    "#     if name in test_patients:\n",
    "#         if name not in unq_set:\n",
    "#             unq_set.add(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dad31085",
   "metadata": {},
   "outputs": [],
   "source": [
    "elecs = ['Fp1', 'Fp2', 'Fpz', 'F3', 'F4', 'Fz', 'C3', 'C4',\n",
    "         'Cz', 'P3', 'P4', 'Pz', '01', '02', '0z', 'F7', \n",
    "         'F8', 'T3', 'T4','T5', 'T6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a98f9614",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNDataset(InMemoryDataset):\n",
    "    \n",
    "    def __init__(self, root, data_dict, idx, feature_names, allow_loops = True, weighted = True, threshold = 0.6,\n",
    "                 transform = None, pre_transform = None, pre_filter = None):\n",
    "        self.data = np.load(data_dict, allow_pickle = True).item()\n",
    "        self.stage = idx # idx 0 - train, idx 1 - test\n",
    "        self.feature_names = feature_names\n",
    "        self.allow_loops = allow_loops\n",
    "        self.weighted = weighted\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        super().__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[self.stage])\n",
    "        \n",
    "    def vectorize_adj_mat_coo(self, matrix, allow_loops = True):\n",
    "        source_nodes = []\n",
    "        target_nodes = []\n",
    "        if allow_loops:\n",
    "            for i in range(matrix.shape[0]):\n",
    "                for j in range(i, matrix.shape[1]):\n",
    "                    source_nodes.append(i)\n",
    "                    target_nodes.append(j)\n",
    "        else:\n",
    "            for i in range(matrix.shape[0]):\n",
    "                for j in range(i + 1, matrix.shape[1]):\n",
    "                    source_nodes.append(i)\n",
    "                    target_nodes.append(j)\n",
    "        return source_nodes, target_nodes\n",
    "    \n",
    "    def vectorize_adj_mat_weights(self, matrix, allow_loops = True, weighted = True, threshold = 0.5):\n",
    "        edge_weights = []\n",
    "        if weighted:\n",
    "            if allow_loops:\n",
    "                for i in range(matrix.shape[0]):\n",
    "                    for j in range(i, matrix.shape[1]):\n",
    "                        edge_weights.append(matrix[i][j])\n",
    "            else:\n",
    "                for i in range(matrix.shape[0]):\n",
    "                    for j in range(i + 1, matrix.shape[1]):\n",
    "                        edge_weights.append(matrix[i][j])\n",
    "        else:\n",
    "            mask = np.array((matrix > threshold), dtype = np.uint8)\n",
    "            if allow_loops:\n",
    "                for i in range(mask.shape[0]):\n",
    "                    for j in range(i, mask.shape[1]):\n",
    "                        edge_weights.append(mask[i][j])\n",
    "            else:\n",
    "                for i in range(mask.shape[0]):\n",
    "                    for j in range(i + 1, mask.shape[1]):\n",
    "                        edge_weights.append(mask[i][j])\n",
    "        return edge_weights\n",
    "\n",
    "    def upload_data(self, patch_name):\n",
    "        '''\n",
    "        input:\n",
    "            path_to_data: path to precomputed node representations\n",
    "            path_to_adj_matr: path to precomputed adj matrices\n",
    "            \n",
    "        returns:\n",
    "            Pygeometric Data object (see PyG docs)\n",
    "        '''\n",
    "        X, target, adj_matrix = self.data[patch_name] # triplet in format [X, target, A]\n",
    "        edge_index = np.array(self.vectorize_adj_mat_coo(adj_matrix, \n",
    "                                                         allow_loops = self.allow_loops))\n",
    "        edge_features = self.vectorize_adj_mat_weights(adj_matrix, \n",
    "                                                       allow_loops = self.allow_loops,\n",
    "                                                       weighted = self.weighted, \n",
    "                                                       threshold = self.threshold)\n",
    "        return Data(x = torch.tensor(X), \n",
    "                    edge_index = torch.tensor(edge_index),\n",
    "                    edge_attrs = edge_features, \n",
    "                    y = torch.tensor([target]))  \n",
    "    \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [f'gnn_dataset_train_{self.feature_names}.pt',\n",
    "                f'gnn_dataset_test_{self.feature_names}.pt']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "        \n",
    "    def process(self):\n",
    "        data_list = []\n",
    "        for elem in self.data.keys():\n",
    "            data_list.append(self.upload_data(elem))\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[self.stage])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b069467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GNNDataset('./', 'train_full_data.npy', 0, 'all_features') # idx = 0 - train\n",
    "test_dataset = GNNDataset('./', 'test_full_data.npy', 1, 'all_features') # idx = 1 - train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bb3b5bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "# for step, data in enumerate(train_loader):\n",
    "#     print(f'Step {step + 1}:')\n",
    "#     print('=======')\n",
    "#     print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "#     print(data)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0dd7865d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = SAGEConv(train_dataset.num_node_features, hidden_channels)\n",
    "        self.batchnorm = torch.nn.BatchNorm1d(hidden_channels) #Vovan added\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, train_dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "#         x = self.batchnorm(x)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "#         x = self.batchnorm(x)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        #added\n",
    "        x = x.relu()\n",
    "        x = self.conv4(x ,edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        #x = F.dropout(x, p=0.1, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9d7cd6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = GCN(hidden_channels = 128)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001, weight_decay = 0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a247ee43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (conv1): SAGEConv(8, 128, aggr=mean)\n",
       "  (batchnorm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): SAGEConv(128, 128, aggr=mean)\n",
       "  (conv3): SAGEConv(128, 128, aggr=mean)\n",
       "  (conv4): SAGEConv(128, 128, aggr=mean)\n",
       "  (lin): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "63eb7a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        out = model(data.x.type(dtype=torch.float), data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        out = model(data.x.type(dtype=torch.float), data.edge_index, data.batch)  \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "    return correct / len(loader.dataset)  # Derive ratio of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bbf9b5ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.4990, Test Acc: 0.5012\n",
      "Epoch: 002, Train Acc: 0.4990, Test Acc: 0.5012\n",
      "Epoch: 003, Train Acc: 0.4990, Test Acc: 0.5012\n",
      "Epoch: 004, Train Acc: 0.5317, Test Acc: 0.5588\n",
      "Epoch: 005, Train Acc: 0.5385, Test Acc: 0.5012\n",
      "Epoch: 006, Train Acc: 0.5400, Test Acc: 0.5081\n",
      "Epoch: 007, Train Acc: 0.5346, Test Acc: 0.5012\n",
      "Epoch: 008, Train Acc: 0.5521, Test Acc: 0.5230\n",
      "Epoch: 009, Train Acc: 0.5911, Test Acc: 0.5161\n",
      "Epoch: 010, Train Acc: 0.5941, Test Acc: 0.5726\n",
      "Epoch: 011, Train Acc: 0.5941, Test Acc: 0.5541\n",
      "Epoch: 012, Train Acc: 0.5892, Test Acc: 0.5288\n",
      "Epoch: 013, Train Acc: 0.5234, Test Acc: 0.5334\n",
      "Epoch: 014, Train Acc: 0.5911, Test Acc: 0.5380\n",
      "Epoch: 015, Train Acc: 0.5463, Test Acc: 0.5588\n",
      "Epoch: 016, Train Acc: 0.5833, Test Acc: 0.5841\n",
      "Epoch: 017, Train Acc: 0.5916, Test Acc: 0.5380\n",
      "Epoch: 018, Train Acc: 0.5180, Test Acc: 0.4988\n",
      "Epoch: 019, Train Acc: 0.5770, Test Acc: 0.5726\n",
      "Epoch: 020, Train Acc: 0.5814, Test Acc: 0.5818\n",
      "Epoch: 021, Train Acc: 0.5975, Test Acc: 0.5737\n",
      "Epoch: 022, Train Acc: 0.5970, Test Acc: 0.5956\n",
      "Epoch: 023, Train Acc: 0.5668, Test Acc: 0.5380\n",
      "Epoch: 024, Train Acc: 0.5999, Test Acc: 0.5853\n",
      "Epoch: 025, Train Acc: 0.5853, Test Acc: 0.5760\n",
      "Epoch: 026, Train Acc: 0.5687, Test Acc: 0.5599\n",
      "Epoch: 027, Train Acc: 0.6004, Test Acc: 0.6060\n",
      "Epoch: 028, Train Acc: 0.5034, Test Acc: 0.5081\n",
      "Epoch: 029, Train Acc: 0.5984, Test Acc: 0.5899\n",
      "Epoch: 030, Train Acc: 0.5814, Test Acc: 0.5472\n",
      "Epoch: 031, Train Acc: 0.5609, Test Acc: 0.5392\n",
      "Epoch: 032, Train Acc: 0.6038, Test Acc: 0.5876\n",
      "Epoch: 033, Train Acc: 0.5453, Test Acc: 0.5265\n",
      "Epoch: 034, Train Acc: 0.6043, Test Acc: 0.6014\n",
      "Epoch: 035, Train Acc: 0.6101, Test Acc: 0.6037\n",
      "Epoch: 036, Train Acc: 0.5902, Test Acc: 0.5864\n",
      "Epoch: 037, Train Acc: 0.6077, Test Acc: 0.5853\n",
      "Epoch: 038, Train Acc: 0.5955, Test Acc: 0.5968\n",
      "Epoch: 039, Train Acc: 0.6033, Test Acc: 0.6325\n",
      "Epoch: 040, Train Acc: 0.6111, Test Acc: 0.6336\n",
      "Epoch: 041, Train Acc: 0.5404, Test Acc: 0.5276\n",
      "Epoch: 042, Train Acc: 0.6087, Test Acc: 0.6290\n",
      "Epoch: 043, Train Acc: 0.6092, Test Acc: 0.6060\n",
      "Epoch: 044, Train Acc: 0.5960, Test Acc: 0.6060\n",
      "Epoch: 045, Train Acc: 0.5512, Test Acc: 0.5369\n",
      "Epoch: 046, Train Acc: 0.5404, Test Acc: 0.5265\n",
      "Epoch: 047, Train Acc: 0.6014, Test Acc: 0.5703\n",
      "Epoch: 048, Train Acc: 0.6053, Test Acc: 0.5806\n",
      "Epoch: 049, Train Acc: 0.5439, Test Acc: 0.5253\n",
      "Epoch: 050, Train Acc: 0.6019, Test Acc: 0.5772\n",
      "Epoch: 051, Train Acc: 0.5994, Test Acc: 0.5760\n",
      "Epoch: 052, Train Acc: 0.6053, Test Acc: 0.5979\n",
      "Epoch: 053, Train Acc: 0.5999, Test Acc: 0.5945\n",
      "Epoch: 054, Train Acc: 0.5984, Test Acc: 0.5864\n",
      "Epoch: 055, Train Acc: 0.6077, Test Acc: 0.6279\n",
      "Epoch: 056, Train Acc: 0.6053, Test Acc: 0.5853\n",
      "Epoch: 057, Train Acc: 0.6101, Test Acc: 0.6152\n",
      "Epoch: 058, Train Acc: 0.5999, Test Acc: 0.5910\n",
      "Epoch: 059, Train Acc: 0.5877, Test Acc: 0.5714\n",
      "Epoch: 060, Train Acc: 0.5902, Test Acc: 0.5691\n",
      "Epoch: 061, Train Acc: 0.6087, Test Acc: 0.5968\n",
      "Epoch: 062, Train Acc: 0.5828, Test Acc: 0.5703\n",
      "Epoch: 063, Train Acc: 0.6019, Test Acc: 0.6210\n",
      "Epoch: 064, Train Acc: 0.6009, Test Acc: 0.5818\n",
      "Epoch: 065, Train Acc: 0.5955, Test Acc: 0.5853\n",
      "Epoch: 066, Train Acc: 0.6072, Test Acc: 0.5829\n",
      "Epoch: 067, Train Acc: 0.6135, Test Acc: 0.6048\n",
      "Epoch: 068, Train Acc: 0.5867, Test Acc: 0.5841\n",
      "Epoch: 069, Train Acc: 0.5517, Test Acc: 0.5300\n",
      "Epoch: 070, Train Acc: 0.5853, Test Acc: 0.5611\n",
      "Epoch: 071, Train Acc: 0.6072, Test Acc: 0.6233\n",
      "Epoch: 072, Train Acc: 0.6019, Test Acc: 0.5853\n",
      "Epoch: 073, Train Acc: 0.5736, Test Acc: 0.5495\n",
      "Epoch: 074, Train Acc: 0.6101, Test Acc: 0.6060\n",
      "Epoch: 075, Train Acc: 0.5989, Test Acc: 0.5772\n",
      "Epoch: 076, Train Acc: 0.5843, Test Acc: 0.5622\n",
      "Epoch: 077, Train Acc: 0.6155, Test Acc: 0.5887\n",
      "Epoch: 078, Train Acc: 0.6155, Test Acc: 0.5795\n",
      "Epoch: 079, Train Acc: 0.6096, Test Acc: 0.6198\n",
      "Epoch: 080, Train Acc: 0.5858, Test Acc: 0.5622\n",
      "Epoch: 081, Train Acc: 0.6179, Test Acc: 0.6267\n",
      "Epoch: 082, Train Acc: 0.6199, Test Acc: 0.6313\n",
      "Epoch: 083, Train Acc: 0.5712, Test Acc: 0.5323\n",
      "Epoch: 084, Train Acc: 0.6028, Test Acc: 0.6141\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [78]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m train()\n\u001b[1;32m      4\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m test(train_loader)\n\u001b[0;32m----> 5\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m best_train, cur_epoch, best_val \u001b[38;5;241m=\u001b[39m (train_acc, epoch, test_acc) \u001b[38;5;28;01mif\u001b[39;00m test_acc \u001b[38;5;241m>\u001b[39m best_val \\\n\u001b[1;32m      7\u001b[0m                                 \u001b[38;5;28;01melse\u001b[39;00m (best_train, cur_epoch, best_val)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [77]\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m     13\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m loader:  \u001b[38;5;66;03m# Iterate in batches over the training/test dataset.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     16\u001b[0m     pred \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Use the class with highest probability.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m((pred \u001b[38;5;241m==\u001b[39m data\u001b[38;5;241m.\u001b[39my)\u001b[38;5;241m.\u001b[39msum())  \u001b[38;5;66;03m# Check against ground-truth labels.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [74]\u001b[0m, in \u001b[0;36mGCN.forward\u001b[0;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#         x = self.batchnorm(x)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mrelu()\n\u001b[0;32m---> 20\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;66;03m#added\u001b[39;00m\n\u001b[1;32m     22\u001b[0m         x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mrelu()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch_geometric/nn/conv/sage_conv.py:136\u001b[0m, in \u001b[0;36mSAGEConv.forward\u001b[0;34m(self, x, edge_index, size)\u001b[0m\n\u001b[1;32m    134\u001b[0m x_r \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_weight \u001b[38;5;129;01mand\u001b[39;00m x_r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin_r\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_r\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize:\n\u001b[1;32m    139\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(out, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch_geometric/nn/dense/linear.py:118\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m        x (Tensor): The features.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_train, cur_epoch, best_val = -1, -1, -1 \n",
    "for epoch in range(1, 500):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    best_train, cur_epoch, best_val = (train_acc, epoch, test_acc) if test_acc > best_val \\\n",
    "                                    else (best_train, cur_epoch, best_val)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "print(f'Best test accuracy - {best_val} on epoch {cur_epoch} with train accuracy - {best_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331619d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.6509"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a90dbae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.load('test_full_data.npy', allow_pickle = True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b20b3552",
   "metadata": {},
   "outputs": [],
   "source": [
    "health = 0\n",
    "mdd = 0\n",
    "for elem in t.keys():\n",
    "    if t[elem][1] == 0:\n",
    "        health += 1\n",
    "    elif t[elem][1] == 1:\n",
    "        mdd += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c25abb32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "433"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b22b3fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "435"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f6b43c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "590.9200000000001"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(433 + 436) * 0.68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcb2d93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
